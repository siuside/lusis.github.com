<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[blog dot lusis]]></title>
  <link href="http://lusis.github.com/atom.xml" rel="self"/>
  <link href="http://lusis.github.com/"/>
  <updated>2012-02-06T17:08:55-05:00</updated>
  <id>http://lusis.github.com/</id>
  <author>
    <name><![CDATA[John E. Vincent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ZeroMQ and Logstash - Part 1]]></title>
    <link href="http://lusis.github.com/blog/2012/02/06/zeromq-and-logstash-part-1/"/>
    <updated>2012-02-06T01:07:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/02/06/zeromq-and-logstash-part-1</id>
    <content type="html"><![CDATA[<p>Every once in a while, a software project comes along that makes you rethink how you&#8217;ve done things up until that point. I&#8217;ve often said that ElasticSearch was the first of those projects for me. The other is ZeroMQ.</p>

<!-- more -->


<h1>Edit and update</h1>

<p>Evidently my testing missed a pretty critical usecase - pubsub. It doesn&#8217;t work right now. Due to the way we&#8217;re doing sockopts works for setting topics. However we don&#8217;t have a commensurate setting on the PUB side. I&#8217;ve created <a href="https://logstash.jira.com/browse/LOGSTASH-399">LOGSTASH-399</a> and <a href="https://logstash.jira.com/browse/LOGSTASH-400">LOGSTASH-400</a> to deal with these issues. I am so sorry about that however it doesn&#8217;t change the overall tone and content of this message as <code>pair</code> and <code>pushpull</code> still work.</p>

<h1>A little history</h1>

<p>In January of this year, <a href="https://twitter.com/jordansissel">Jordan</a> merged the first iteration of ZeroMQ support for Logstash. Several people had been asking for it and I had it on my plate to do as well. Funny side note, the pull request for the ZeroMQ plugin was my inspiration for adding <a href="http://logstash.net/docs/1.1.0/plugin-status">plugin_status</a> to Logstash.</p>

<p>The reason for wanting to mark it experimental is that there was concern over the best approach to using ZeroMQ with Logstash. Did we create a single context per agent? Did we do a context per thread? How well would the multiple layers of indirection work (jvm + ruby + ffi)?</p>

<p><a href="https://twitter.com/_masterzen_">Brice&#8217;s</a> original pull request only hadnled one part of the total ZeroMQ package (PUBSUB) but it was an awesome start. We actually had two other pull requests around the same time but his was first.</p>

<p>A week or so ago, I started a series of posts around doing load balanced filter pipelines with Logstash. The first was <a href="http://goo.gl/vWyCH">AMQP</a> and then <a href="http://goo.gl/6W8Lv">Redis</a>. The next logical step was ZeroMQ (and something of a &#8220;Oh..and one more thing..&#8221; post). Sadly, the current version of the plugin was not amenable to doing the same flow. Since it only supported PUBSUB, I needed to do some work on the plugin to get the other socket types supported. I made this my weekend project.</p>

<h1>Something different</h1>

<p>One thing that ZeroMQ does amazingly well is make something complex very easy. It exposes common communication patterns over socket types and makes it easy to use them. It really is just plug and play communication.</p>

<p>However it also makes some really powerful flows available to you if you dig deep enough. Look at this example from the <a href="http://zguide.zeromq.org">zguide</a></p>

<p><img src="https://github.com/imatix/zguide/raw/master/images/fig14.png" alt="complex-flow" /></p>

<p>Mind you the code for that is pretty simple (<a href="http://zguide.zeromq.org/rb:taskwork2">ruby example</a>) but we need to enable that level of flexibility and power behind the Logstash config language. We also wanted to avoid the confusion that we faced with the AMQP plugin around exchange vs. queue.</p>

<p>Jordan came up with the idea of removing the socket type confusion and just exposing the patterns. And that&#8217;s what we&#8217;ve done.</p>

<h1>Configuration</h1>

<p>In the configuration language, Logstash exposes the ZeroMQ socket type pairs in the using the same syntax on both inputs and outputs. We call these a &#8220;topology&#8221;. In fact, out of the box, Logstash ZeroMQ support will work out of the box with two agents on the same machine:</p>

<h2>Output</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>  stdin { type =&gt; "stdin-input" }
</span><span class='line'>}
</span><span class='line'>output {
</span><span class='line'>  zeromq { topology =&gt; "pushpull" }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Input</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>  zeromq { topology =&gt; "pushpull" type =&gt; "zeromq-input" }
</span><span class='line'>}
</span><span class='line'>output {
</span><span class='line'>  stdout { debug =&gt; true }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Opinionated</h2>

<p>Because any side of a socket type in ZeroMQ can be the connecting or binding side (the underlying message flow is disconnected from how the connection is established), Logstash follows the recommendation of the zguide. The more &#8220;stable&#8221; parts of your infrastructure should be the side that binds/listens while they ephemeral side should be the one that initiates connections.</p>

<p>Following this, we have some sane defaults around the plugins:</p>

<ul>
<li>Logstash inputs will, by default, be the <code>bind</code> side and bind to all interfaces on port 2120</li>
<li>Logstash outputs will, by default, be the <code>connect</code> side</li>
<li>Logstash inputs will be the consumer side of a flow</li>
<li>Logstash outputs will be the producing side of a flow</li>
</ul>


<p>The last two are obviously pretty &#8220;duh&#8221; but worth mentioning. Right now Logstash exposes three socket types that make sense for Logstash:</p>

<ul>
<li>PUSHPULL (Output is PUSH. Input is PULL)</li>
<li>PUBSUB (Output is PUB. Input is SUB)</li>
<li>PAIR</li>
</ul>


<p>It&#8217;s worth reading up on ALL <a href="http://api.zeromq.org/2-1:zmq-socket">the socket types in ZeroMQ</a>.</p>

<p>By default, because of how ZeroMQ will most commonly be slotted into your pipeline, it sets the default message format to the Logstash native <em>json_event</em>.</p>

<p>You can still get to the low-level tuning of the sockets via the <code>sockopts</code> configuration setting. This is a Logstash config hash. For example, if you wanted to tune the high water mark of a socket (<code>ZMQ_HWM</code>), you would do so with this option:</p>

<p><code>zeromq { topology =&gt; "pushpull" sockopts =&gt; ["ZMQ::HWM", 20] }</code></p>

<p>These options are passed directly to the <code>ffi-rzmq</code> library we use (hence the syntax on the option name). If a new option is added in a later release, it&#8217;s already available that way.</p>

<h1>Usage of each topology</h1>

<p>While I have a few more blog posts in the hopper around ZeroMQ (and various patterns with Logstash), I&#8217;ll briefly cover where each type might fit.</p>

<h2>PUBSUB</h2>

<p>This is exactly what it sounds like. Each output (PUB) broadcasts to all connected inputs (SUB).</p>

<h2>PUSHPULL</h2>

<p>This most closely mimics the examples in my previous posts on AMQP and Redis. Each output (PUSH) load-balances across all connected inputs (PULL).</p>

<h2>PAIR</h2>

<p>This is essentially a one-to-one streaming socket. While messages CAN flow both directions, Logstash does not support (nor need) that. Outputs stream events to the input.</p>

<p>ZeroMQ has other topologies (like REQREP - request response and ROUTER/DEALER) but they don&#8217;t really make sense for Logstash right now. For the type of messaging that Logstash does between peers, PAIR is a much better fit. We have plans to expose these in a future release.</p>

<h1>Future</h1>

<p>As I said, I&#8217;ve got quite a few ideas for posts around this plugin. It opens up so many avenues for users and makes doing complex pipelines much easier. Here&#8217;s a sample of some things you&#8217;ll be able to do:</p>

<ul>
<li>Writing your own &#8220;broker&#8221; to sit between edges and indexers in whatever language works best (8 lines of Ruby)</li>
<li>Log directly from your application (e.g. log4j ZMQ appender) to logstash with minimal fuss</li>
<li>Tune ZeroMQ sockopts for durability</li>
</ul>


<p>Current ZeroMQ support only exists in master right now. However building from source is very easy. Simply clone the repo and type <code>make</code>. You don&#8217;t even need to have Ruby installed. This will leave your very own jar file in the <code>build</code> directory.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Load balancing Logstash with Redis]]></title>
    <link href="http://lusis.github.com/blog/2012/01/31/load-balancing-logstash-with-redis/"/>
    <updated>2012-01-31T23:24:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/01/31/load-balancing-logstash-with-redis</id>
    <content type="html"><![CDATA[<p>After yesterday&#8217;s post about load balancing logstash with AMQP and RabbitMQ, I got to thinking that it might be useful to show a smilar pattern with other inputs and outputs.
To me this, is the crux of what makes Logstash so awesome. Someone asked me to describe Logstash in one sentence. The best I could come up with was:</p>

<blockquote><p>Logstash is a unix pipe on steroids</p></blockquote>


<p>I hope this post helps you understand what I meant by that</p>

<!-- more -->


<h1>Revisiting our requirements and pattern</h1>

<p>If you recall from the post <a href="http://goo.gl/vWyCH">yesterday</a>, we had the following &#8216;requirements&#8217;:</p>

<ul>
<li>No lost messages in transit/due to inputs or outputs.</li>
<li>Shipper only configuration on the source</li>
<li>Worker based filtering model</li>
<li>No duplicate messages due to transit mediums (i.e. fanout is inappropriate as all indexers would see the same message)</li>
</ul>


<h2>EDIT</h2>

<p>Originally our list stated the requirements as <em>No lost messages</em> and <em>No duplicate messages</em>. I&#8217;ve amended those with a slight modification to closer reflect the original intent. Please see <a href="http://blog.lusis.org/blog/2012/01/31/load-balancing-logstash-with-amqp/#comment-426175086">comment from Jelle Smet here</a> for details. Thanks Jelle!</p>

<p>Our design looked something like this:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/gliffy-overview.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/gliffy-overview.png" alt="gliffy-overview.png" /></a></p>

<p>One of the reasons that post was so long was that AMQP is a complicated beast. There was quite a bit of dense frontloading I had to do to cover AMQP before we got to the meat.
We&#8217;re going to take that same example, and swap out RabbitMQ for something a bit simpler and achieve the same results.</p>

<h1>Quick background on Redis</h1>

<p><a href="http://redis.io">Redis</a> is commonly lumped in with a group of data storage technologies called NoSQL. Its name is short for &#8220;REmoteDIctionaryServer&#8221;. It typically falls into the &#8220;key/value&#8221; family of NoSQL.
Several things set Redis apart from most key/value systems however:</p>

<ul>
<li>&#8220;data types&#8221; as values</li>
<li>native operations on those data types</li>
<li>atomic operations</li>
<li>built-in PUB/SUB subsystem</li>
<li>No external dependencies</li>
</ul>


<h2>Data types</h2>

<p>I&#8217;m not going to go into too much detail about the data types except to list them and highlight the one we&#8217;ll be leveraging. You can read more about them <a href="http://redis.io/topics/data-types">here</a></p>

<ul>
<li>Strings</li>
<li>Lists*</li>
<li>Sets</li>
<li>Hashes</li>
<li>Sorted Sets</li>
</ul>


<h3>How Logstash uses Redis</h3>

<p>Looking back at our AMQP example, we note three distinct exchange types. These are mapped to the following functionality in Redis (and Logstash <code>data_type</code> config for reference):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/mapping-table.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/mapping-table.png" alt="mapping-table.png" /></a></p>

<p>This is a somewhat over simplified list. In the case of a message producer, mimicing <code>direct</code> exchanges is done by writing to a Redis <code>list</code> while consumption of that is done via the Redis command <code>BLPOP</code><a href="http://redis.io/commands/blpop">*</a>. However mimicing the <code>fanout</code> and <code>topic</code> functionality is done strictly with the commands <code>PUBLISH</code><a href="http://redis.io/commands/publish">*</a>, <code>SUBSCRIBE</code><a href="http://redis.io/commands/subscribe">*</a> and <code>PSUBSCRIBE</code><a href="http://redis.io/commands/psubscribe">*</a>. It&#8217;s worth reading each of those for a better understanding.</p>

<p>Oddly enough, the use of Redis as a messaging bus is something of a side effect. Redis supported lists that are auto-sorted by insert order. The <code>POP</code> command variants allowed single transaction get and remove of the data. It just fit the use case.</p>

<h1>The configs</h1>

<p>As with our previous example, we&#8217;re going to show the configs needed on each side and explain them a little bit.</p>

<h2>Client-side/Producer</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input { stdin { type =&gt; "producer"} }
</span><span class='line'>output {
</span><span class='line'>redis {
</span><span class='line'> host =&gt; 'localhost'
</span><span class='line'> data_type =&gt; 'list'
</span><span class='line'> key =&gt; 'logstash:redis'
</span><span class='line'>}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>data_type</h3>

<p>This is where we tell Logstash how to send the data to Redis. In the case, again, we&#8217;re storing it in a list data type.</p>

<h3>key</h3>

<p>Unfortunately, key means different things (though with the same effect) depending on the <code>data_type</code> being used. In the case of a <code>list</code> this maps cleanly to the understanding of a <code>key</code> in a key/value system. It&#8217;s common in Redis to namespace keys with a <code>:</code> though it&#8217;s entirely unneccesary.</p>

<p>As an aside, when using <code>key</code> on <code>channel</code> data type, this behaves like the routing key in AMQP parlance with the exception of being able to use any separator you like (in other words, you can namespace with <code>.</code>,<code>:</code>,<code>::</code> whatever).</p>

<h2>Indexer-side/Consumer</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>redis {
</span><span class='line'>  host =&gt; 'localhost'
</span><span class='line'>  data_type =&gt; 'list'
</span><span class='line'>  key =&gt; 'logstash:redis'
</span><span class='line'>  type =&gt; 'redis-input'
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>output {stdout {debug =&gt; true} }</span></code></pre></td></tr></table></div></figure>


<h3>data_type</h3>

<p>This needs to match up with the value from the output plugin. Again, in this example <code>list</code>.</p>

<h3>key</h3>

<p>In the case of a <code>list</code> this needs to map EXACTLY to the output plugin. Following on to our previous aside, for <code>data_type</code> values of <code>channel</code> input, the key must match exactly while <code>pattern_channel</code> can support wildcards. Redis PSUBSCRIBE wildcards actually much simpler than AMQP ones. You can use <code>*</code> at any point in the key name.</p>

<h1>Starting it all up</h1>

<p>We&#8217;re going to simplify our original tests a little bit in the interest of brevity. Showing 2 producers and 2 consumers gives us the same benefit as showing four of each. Since we don&#8217;t have the benefit of a pretty management interface, we&#8217;re going to use the redis server debug information and the <code>redis-cli</code> application to allow us to see certain management information.</p>

<h2>redis-server</h2>

<p>Start the server with the command <code>redis-server</code> I&#8217;m running this from homebrew but you literally build Redis on any machine that has <code>make</code> and a compiler. That&#8217;s all you need. You can even run it straight from the source directory:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/redis-server.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/redis-server.png" alt="redis-server.png" /></a></p>

<p>You&#8217;ll notice that the redis server is periodically dumping some stats - number of connected clients and the amount of memory in use.</p>

<h2>Starting the logstash agents</h2>

<p>We&#8217;re going to start two producers (redis output) and two consumers (redis input):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/agents.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/agents.png" alt="agents.png" /></a></p>

<p>Back in our redis-server window, you should now see two connected clients in the periodic status messages. Why not four? Because the producers don&#8217;t have a persistent connection to Redis. Only the consumers do (via BLPOP):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/two-clients.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/two-clients.png" alt="two-clients.png" /></a></p>

<h1>Testing message flow</h1>

<p>As with our previous post, we&#8217;re going to alternate messages between the two producers. In the first producer, we&#8217;ll type <code>window 1</code> and in the second <code>window 2</code>. You&#8217;ll see the consumers pick up the messages:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/delivery.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/delivery.png" alt="delivery.png" /></a></p>

<p>If you look over in the redis-server window, you&#8217;ll also see that our client count went up to four. If we were to leave these clients alone, eventually it would drop back down to two.</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/new-connections.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/new-connections.png" alt="new-connections.png" /></a></p>

<p>Feel free to run the tests a few times and get a feel for message flow.</p>

<h2>Offline consumers</h2>

<p>This is all well and good but as with the previous example, we want to test how this configuration handles the case of consumers going offline. Shut down the two indexer configs and let&#8217;s verify. To do this, we&#8217;re going to also open up a new window and run the <code>redis-cli</code> app. Technically, you don&#8217;t even need that. You can telnet to the redis port and just run these commands yourself. We&#8217;re going to use the <code>LLEN</code> command to get the size of our &#8220;backlog&#8221;.</p>

<p>In the producer windows, type a few messages. Alternate between producers for maximum effect. Then go over to the <code>redis-cli</code> window and type <code>LLEN logstash:redis</code>. You should see something like the following (obviously varied by how many messages you sent):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/llen.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/llen.png" alt="llen.png" /></a></p>

<p>You&#8217;ll also notice in the redis server window that the amount of memory in use went up slightly.</p>

<p>Now let&#8217;s start our consumers back up and ensure they drain (and in insert order):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/drain.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/drain.png" alt="drain.png" /></a></p>

<p>Looks good to me!</p>

<h1>Persistence</h1>

<p>You might have noticed I didn&#8217;t address disk-based persistence at all. This was intentional. Redis is primarily a memory-based store. However it does have support for a few different ways of persisting to disk - RDB and AOF. I&#8217;m not going to go into too much detail on those. The Redis documentation does a good job of explaining the pros and cons of each. You can read that <a href="http://redis.io/topics/persistence">here</a>.</p>

<h1>Wrap up</h1>

<p>One thing that&#8217;s important to note is that Redis is pretty damn fast. The limitation for Redis is essentially memory. However if speed isn&#8217;t your primary concern, there&#8217;s an interesting alpha project called <a href="http://inaka.github.com/edis">edis</a> worth investigating. It is a port of Redis to Erlang. Its primary goal is better persistence for Redis. For this post I also tested Logstash against edis and I&#8217;m happy to say it works:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/edis.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/edis.png" alt="edis.png" /></a></p>

<p>I hope to do further testing with it in the future in a multinode setup.</p>

<h2>Part three</h2>

<p>I&#8217;m also working on a part three in this &#8220;series&#8221;. The last configuration I&#8217;d like to show is doing this same setup but using <a href="http://zeromq.org">0mq</a> as the bus. This is going to be especially challenging since our 0mq support is curretly &#8216;alpha&#8217;-ish quality. Beyond that, I plan on doing a similar series using pub/sub patterns. If you&#8217;re enjoying these posts, please comment and let me know!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Load balancing Logstash with AMQP]]></title>
    <link href="http://lusis.github.com/blog/2012/01/31/load-balancing-logstash-with-amqp/"/>
    <updated>2012-01-31T01:12:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/01/31/load-balancing-logstash-with-amqp</id>
    <content type="html"><![CDATA[<p>AMQP in Logstash is one of the most complicated parts of the workflow. I&#8217;ve taken it on myself, as the person with the most AMQP experience (both RabbitMQ and Qpid) to try and explain as much as need for logstash users.</p>

<p><a href="https://twitter.com/patrickdebois">Patrick DeBois</a> hit me up with a common logstash design pattern that I felt warranted a full detailed post.</p>

<p><em>Warning: This is an image heavy post. Terminal screenshots are linked to larger versions</em></p>

<h2>Requirements</h2>

<ul>
<li>No lost messages in transit/due to inputs or outputs.</li>
<li>Shipper-only configuration on the source</li>
<li>Worker-based filtering model</li>
<li>No duplicate messages due to transit mediums (i.e. fanout is inappropriate as all indexers would see the same message)</li>
<li>External ElasticSearch cluster as final destination</li>
</ul>


<!-- more -->


<h2>EDIT</h2>

<p>Originally our list stated the requirements as <em>No lost messages</em> and <em>No duplicate messages</em>. I&#8217;ve amended those with a slight modification to closer reflect the original intent. Please see <a href="http://blog.lusis.org/blog/2012/01/31/load-balancing-logstash-with-amqp/#comment-426175086">comment from Jelle Smet here</a> for details. Thanks Jelle!</p>

<h2>Notes</h2>

<p>We&#8217;re going to leave the details of filtering and client-side input up to the imagination.
For this use case we&#8217;ll simply use <code>stdin</code> as our starting point. You can modify this as you see fit.
The same goes for filtering. The assumption is that your filters will be correct and not be the source of any messages NOT making it into ElasticSearch.</p>

<p>Each configuration will be explained so don&#8217;t stress over it at first glance. We&#8217;re also going to explicitly set some options for the sake of easier comprehension.</p>

<h1>Client-side agent config</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  input {
</span><span class='line'>    stdin { debug =&gt; true type =&gt; "host-agent-input" }
</span><span class='line'>  }
</span><span class='line'>  output {
</span><span class='line'>    amqp {
</span><span class='line'>      name =&gt; "logstash-exchange"
</span><span class='line'>      exchange_type =&gt; "direct"
</span><span class='line'>      host =&gt; "rabbitmq-server"
</span><span class='line'>      key =&gt; "logstash-routing-key"
</span><span class='line'>      durable =&gt; true
</span><span class='line'>      persistent =&gt; true
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Config Explained</h2>

<p>The amqp output:</p>

<h3>name</h3>

<p>This is the name that will be provided to RabbitMQ for the exchange. By default, the Bunny driver will auto-generate a name. This won&#8217;t work in this usecase because the consumers will need a known name. Remember exchanges are for producers. Queues are for consumers. When we wire up the indexer side, we&#8217;ll need to know the name of the exchange to perform the binding.</p>

<h3>exchange_type</h3>

<p>For this particular design, we want to use a direct exchange. It&#8217;s the only way we can guarantee that only one copy of a log message will be processed.</p>

<h3>key</h3>

<p>We&#8217;re going to explicitly set the routing key as direct exchanges do not support wildcard routing key bindings. Again, we&#8217;ll need this on the consumer side to ensure we get the right messages.</p>

<h3>durable</h3>

<p>This setting controls if the exchange should survive RabbitMQ restarts or not.</p>

<h3>persistent</h3>

<p>This is for the messages. Should they be persisted to disk or not?</p>

<p>Note that for a fully &#8220;no lost messages scenario&#8221; to work in RabbitMQ, you have to jump through some hoops. This is explain more below.</p>

<h2>Running the agent</h2>

<p>This same configuration should be used on ALL host agents where logs are being read. You can have variation in the inputs. You can have additional outputs however the amqp output stanza above will ensure that all messages will be sent to RabbitMQ.</p>

<h1>Indexer agent config</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>  amqp {
</span><span class='line'>    host =&gt; "rabbitmq-server"
</span><span class='line'>    name =&gt; "indexer-queue"
</span><span class='line'>    exchange =&gt; "logstash-exchange"
</span><span class='line'>    key =&gt; "logstash-routing-key"
</span><span class='line'>    exclusive =&gt; false
</span><span class='line'>    durable =&gt; true
</span><span class='line'>    auto_delete =&gt; false
</span><span class='line'>    type =&gt; "logstash-indexer-input"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'>  # your filters here
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch {
</span><span class='line'>    # your elasticsearch settings here
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Config explained</h2>

<p>The amqp input:</p>

<h3>name</h3>

<p>This is the name that will be provided to RabbitMQ for the queue. Again, as with exchange, we need a known name. The reason for this is that all of our indexers are going to share a common queue. This will make sense in a moment.</p>

<h3>exchange</h3>

<p>This should match exactly with the name of the exchange that was created before in the host-side config.</p>

<h3>key</h3>

<p>This should, again, match the routing key provided in the host-side configuration exactly. <code>direct</code> exchanges do NOT support wildcard routing keys. By providing a routing key, you are creating a <code>binding</code> in RabbitMQ terms. This <code>binding</code> says &#8220;I want all messages sent to the <code>logstash-exchange</code> with a routing key of <code>logstash-routing-key</code> to be sent to the queue named <code>indexer-queue</code>.</p>

<h3>exclusive</h3>

<p>As with the exchange in the host-side config, we&#8217;re going to have multiple workers using this queue. This is another AMQP detail. When you bind a queue to an exchange, a <code>channel</code> is created for the messages to flow across. A single queue can have multiple channels. This is how our worker pool is going to operate.</p>

<p><strong>You do not want a different queue name for each worker despite how weird that sounds</strong></p>

<p>If you give each worker its own queue, then you <strong>WILL</strong> get duplicate messages. It&#8217;s counterintuitive, I know. Just trust me. The way to ensure that multiple consumers don&#8217;t see the same message is to use mutliple channels on the same queue.</p>

<h3>durable</h3>

<p>Same as the exchange declarition, this ensures that the queue will stick around if the broker (the RabbitMQ server) restarts.</p>

<h3>auto_delete</h3>

<p>This is the setting most people miss when trying to ensure no lost messages. By default, RabbitMQ will throw away even durable queues once the last user of the queue disconnects.</p>

<h3>type</h3>

<p>This is the standard logstash requirement for inputs. They must have a <code>type</code> defined. Arbitrary string.</p>

<h1>Sidebar on RabbitMQ message reliability</h1>

<p>Simply put, RabbitMQ makes you jump through hoops to ensure that no message is lost. There&#8217;s a trifecta of settings that you have to have for it to work:</p>

<ul>
<li>Your exchange must be durable with persistent messages</li>
<li>Your queue must be durable</li>
<li>Auto-delete must not be disabled</li>
</ul>


<p><strong>EVEN IF YOU DO ALL THESE THINGS, YOU CAN STILL LOSE MESSAGES!</strong></p>

<h2>Order matters</h2>

<p>I know &#8230; you&#8217;re thinking &#8220;What the F&#8212;?&#8221;. There is still a scenario where you can lose messages. It has to do with how you start things up.</p>

<ul>
<li>If you start the exchange side but never start the queue side, messages are dropped on the floor</li>
<li>You can&#8217;t start the queue side without first starting the exchange side</li>
</ul>


<p>While RabbitMQ let&#8217;s you predeclare exchanges and queues from the command-line, it normally only creates things when someone asks for it. Since exchanges know nothing about the consumption side of the messages (the queues), creating an exchange with all the right settings does NOT create the queue and thus no binding is ever created.</p>

<p>Conversely, you can&#8217;t declare a totally durable queue when there is no exchange in place to bind against.</p>

<p>Follow these rules and you&#8217;ll be okay. You only need to do it once:</p>

<ul>
<li>Start a producer (the host-side logstash agent)</li>
<li>Ensure via <code>rabbitmqctl</code> or the management web interface that the exchange exists</li>
<li>Start one of the consumers (the indexer config)</li>
</ul>


<p>Once the indexer agent has started, you will be good to go. You can shutdown the indexers and messages will start piling up. You can shut everything down - rabbitmq (with backlogged messages), the indexer agent and the host-side agent. When you start RabbitMQ, the queues, exchanges and messages will all still be there. If you start an indexer agent, it will drain the outstanding messages.</p>

<p>However, if you screw the configuration up you&#8217;ll have to delete the exchange and the queue via <code>rabbitmqctl</code> or the management web interface and start over.</p>

<h1>How it looks visually</h1>

<p>There are two plugins you should install with RabbitMQ:</p>

<ul>
<li>rabbitmq_management</li>
<li>rabbitmq_management_visualizer</li>
</ul>


<p>The first will provide a web interface (and HTTP API!) listening on port 55672 of your RabbitMQ server. It provides a really easy way to see messages backlogged, declared exchanges/queue and pretty much everything else. Seeing as it also provides a very nice REST api to everything inside the RabbitMQ server, you&#8217;ll want it anyway if for nothing but monitoring hooks.</p>

<p>The visualizer is an ad-hoc addon that helps you see the flows through the system. It&#8217;s not as pretty as the management web interface proper but it gets the job done.</p>

<h1>Starting it all up</h1>

<p>Now we can start things up</p>

<h2>Producers</h2>

<p>We&#8217;re going to start up our four client side agents. These will create the exchange (or alternately connect to the existing one). If you look at the management interface, you&#8217;ll see four channels established:</p>

<p>Management view:
<img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/amqp-four-channels.png" alt="amqp-four-channels.png" /></p>

<p>Visualizer view:
<img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/amqp-four-producers.png" alt="amqp-four-producers.png" /></p>

<p>Remember that until we connect with a consumer configuration (the indexer) messages sent to these exchanges WILL be lost.</p>

<h2>Consumers</h2>

<p>Now we start our indexer configurations - all four of them</p>

<p>Now if we take a peek around the management interface and the visualizer, we start to see some cool stuff.</p>

<p>In the managment interface, you&#8217;ll see eight total channels - four for the queue and four for the exchange</p>

<p><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/amqp-eight-channels.png" alt="amqp-eight-channels.png" /></p>

<p>If you click on &#8220;Queues&#8221; at the top and then on the entry for our <code>indexer-queue</code>, you&#8217;ll see more details:</p>

<p><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/amqp-indexer-queue-details.png" alt="amqp-indexer-queue-details.png" /></p>

<p>But the real visual is in the visualizer tab. Click on it and then click on the <code>indexer-queue</code> on the far right</p>

<p><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/amqp-visualizer-detail.png" alt="amqp-visualizer-detail.png" /></p>

<p>You can see the lines showing the flow of messages.</p>

<p>One thing to make note of about RabbitMQ load balancing. Messages are load balanced across CONSUMERS not QUEUES. There&#8217;s a subtle distinction there from RabbitMQ&#8217;s semantic point of view.</p>

<h2>Testing the message flow</h2>

<p>Over in your terminal window, let&#8217;s send some test messages. For this test, again, I&#8217;m using <code>stdin</code> for my origination and <code>stdout</code> to mimic the ElasticSearch destination.</p>

<p>In my first input window, I&#8217;m going just type 1 through 4 with a newline after each. This should result in each consumer getting a message round-robin style:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/load-balance-test-1.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/load-balance-test-1.png" alt="load-balance-test-1.png" /></a></p>

<p>Now I&#8217;m going to cycle through the input windows and send a single message from each:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/load-balance-test-4.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/load-balance-test-4.png" alt="load-balance-test-4.png" /></a></p>

<p>You can see that messages 4-7 were sent round-robin style.</p>

<h2>Testing persistence</h2>

<p>All of this is for naught if we lose messages because our workers are offline. Let&#8217;s shutdown all of our workers and send a bunch of messages from each input window:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/workers-offline-terminal.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/workers-offline-terminal.png" alt="workers-offline-terminal.png" /></a></p>

<p>We sent two lines of text per window. This amounts to eight log messages that should be queued up for us. Let&#8217;s check the management interface:</p>

<p><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/eight-messages-waiting.png" alt="eight-messages-waiting.png" /></p>

<p>Now if we stop rabbitmq entirely and restart it, those messages should still be there (along with the queue and exchanges we created).</p>

<p>Once you&#8217;ve verified that, start one of the workers back up. When it comes fully online, it should drain all of the messages from the exchange:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/drained-messages.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/drained-messages.png" alt="drained-messages.png" /></a></p>

<p>Yep, there they went. The last two messages you get should be the ones from window 4. This is another basic functionality of message queue software in general. Messages should be delivered in the order in which they were recieved.</p>

<h1>One last diagram</h1>

<p>Here&#8217;s a flowchart I created with Gliffy to show what the high-level overview of our setup would look like. Hope it helps and feel free to hit me up on freenode irc in the <code>#logstash</code> channel or on <a href="https://twitter.com/lusis">twitter</a>.</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/gliffy-overview.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/gliffy-overview.png" alt="gliffy-overview.png" /></a></p>

<p><em>This post will eventually make its way into the <a href="http://cookbook.logstash.net">Logstash Cookbook Site</a>.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lowtech monitoring with Jenkins]]></title>
    <link href="http://lusis.github.com/blog/2012/01/23/lowtech-monitoring-with-jenkins/"/>
    <updated>2012-01-23T00:07:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/01/23/lowtech-monitoring-with-jenkins</id>
    <content type="html"><![CDATA[<p>I mentioned briefly in my previous post that I got quite a few people coming up to me after the panel and asking me for advice on monitoring.</p>

<!-- more -->


<p>I tweeted about this scenario not long after it happened but here&#8217;s the gist:</p>

<blockquote><p>I just need something simple to check on the status of a few jobs and run some SQL statements. I&#8217;m a DBA and I can&#8217;t get any help from my ops team.</p></blockquote>


<p>The person who asked this was very friendly and I could sense the frustration in her voice. It frustrates me to no end to hear stories of my tribe being this way to customers.</p>

<p>I thought for a minute because I really wanted to help and the best thing I could think of was Jenkins. Yes, Jenkins.</p>

<h1>Reasoning</h1>

<p>Let&#8217;s look for a minute at what we need from a simple health check system:</p>

<ul>
<li>Performing some task on a given schedule</li>
<li>Ability to run a given command</li>
<li>Reporting on the output of the given command (success/failure)</li>
</ul>


<p>Now you might think to your self &#8220;Self, this sure does sound a lot like cron&#8221;. You&#8217;d be right. And that&#8217;s EXACTLY what took me down the Jenkins path. There have been numerous posts about people replacing individual cron jobs with a centralized model based on Jenkins. This makes perfect sense and is something of a holy grail. I clearly remember researching and evaluating batch scheduling products many years ago to essentially do just this. If only Jenkins had been around then.</p>

<h2>Small disclaimer</h2>

<p>While Jenkins is a great low friction way to accomplish this task, it may or may not be scalable in the long run. While Jenkins jobs are defined as XML files and can be managed via an API, it&#8217;s still a bit cumbersome to automate.</p>

<h1>Getting started</h1>

<p>First thing to do is grab the <a href="http://mirrors.jenkins-ci.org/war/latest/jenkins.war">latest Jenkins war</a>. The nice thing about Jenkins is that it ships in such an easy to use format - a self-contained executable war. You can start it very simply with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java -jar jenkins.war</span></code></pre></td></tr></table></div></figure>


<p>You should probably click around to get comfortable with the interface. It&#8217;s pretty difficult to screw something up but if you do, just shutdown jenkins, <code>rm -rf ~/.jenkins</code> and start it back up.</p>

<p>Since this post is geared primarily at someone who probably isn&#8217;t familiar with Jenkins, I&#8217;m going to go over a few quick basics and key areas we&#8217;ll be working with:</p>

<h2>Menus</h2>

<p>The menu is the section on the left handside. It will change based on your location in the application. If you don&#8217;t always see something you&#8217;re expecting, you can use the breadcrumb navigation to work your way back. Alternately, you can click on the Jenkins logo to get to the main page.</p>

<h3>Main menu</h3>

<p>This is the menu you see from the top-level of the Jenkins interface</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/menu.png" alt="Main interface" /></p>

<h3>Job menu</h3>

<p>This is the menu you see when you are viewing the main page of a job</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/job-menu.png" alt="Job Menu" /></p>

<p>Note the &#8220;Build History&#8221; section at the bottom. This is a list of all builds that have been performed for this job. You can click on a given build to see details about it.</p>

<h3>Build menu</h3>

<p>This menu is visible when you select a specific build from the &#8220;Build History&#8221; menu of a Job page</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/build-menu.png" alt="Build Menu" /></p>

<p>Notice the &#8220;Console Output&#8221; menu option. This will show you the log of what Jenkins did during a build. If you ever have problems with a build, you should come here and look at what happened.</p>

<h3>Auto Refresh</h3>

<p>In the interest of eliminating any confusion, we&#8217;re going to enable &#8220;Auto Refresh&#8221; from the link on the top right:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/enable-auto-refresh.png" alt="Enable Auto Refresh" /></p>

<h2>Configuration</h2>

<p>For the purposes of this exercise, we won&#8217;t do too much configuration. We&#8217;re going to take the perspective of the person above. We&#8217;ll make few assumptions though in the interest of expidiency:</p>

<ul>
<li>The user has no passphrase on the SSH key. While this is probably not true, it makes this demo easier.</li>
<li>The DB test will be executed locally.</li>
<li>The local environment is some unix-y/linux-y one. The environment for this post was OS X communicating with Linux VMs</li>
</ul>


<p>The key to success here is something called a &#8220;free-style software project&#8221;. This is essentially a blank canvas with very few requirements. I&#8217;m aware that the &#8220;Monitor an external job&#8221; type has been recently added but the steps were a bit too invasive for this particular case.</p>

<h1>Our test case</h1>

<p>I don&#8217;t obviously have the specifics of what the user wanted checked so I&#8217;m going to extrapolate from her original statement:</p>

<ul>
<li>Run a SQL statement to see if some record is found</li>
<li>Check for a running process</li>
<li>Check a log file for some given string</li>
</ul>


<h2>The test database</h2>

<p>The test database will be MySQL running on a Linux VM. Getting this going is an exercise for the reader, however here is the DDL and test data we&#8217;re using:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="c1">--- This is just a sample, folks. Yes I know it&#39;s insecure.</span>
</span><span class='line'><span class="k">create</span> <span class="k">database</span> <span class="n">foo_db</span><span class="p">;</span>
</span><span class='line'><span class="n">use</span> <span class="n">foo_db</span><span class="p">;</span>
</span><span class='line'><span class="k">create</span> <span class="k">table</span> <span class="n">jobs</span> <span class="p">(</span> <span class="n">id</span> <span class="nb">int</span> <span class="k">not</span> <span class="k">null</span> <span class="n">auto_increment</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span> <span class="n">name</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">depth</span> <span class="nb">int</span><span class="p">);</span>
</span><span class='line'><span class="k">insert</span> <span class="k">into</span> <span class="n">jobs</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="ss">&quot;job_a&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">);</span>
</span><span class='line'><span class="k">insert</span> <span class="k">into</span> <span class="n">jobs</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="ss">&quot;job_b&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span><span class='line'><span class="k">insert</span> <span class="k">into</span> <span class="n">jobs</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="ss">&quot;job_c&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span>
</span><span class='line'><span class="k">grant</span> <span class="k">select</span> <span class="k">on</span> <span class="n">jobs</span> <span class="k">to</span> <span class="s1">&#39;jenkins&#39;</span><span class="o">@</span><span class="s1">&#39;%&#39;</span> <span class="n">IDENTIFIED</span> <span class="k">BY</span> <span class="s1">&#39;password&#39;</span><span class="p">;</span>
</span><span class='line'><span class="n">flush</span> <span class="k">privileges</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>First Job</h2>

<p>So we&#8217;ll create a new freestyle job called &#8220;Check FooDB Backlog&#8221;. Click on &#8220;New Job&#8221; from the main menu.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/create-db-job.png" alt="Create Job" /></p>

<p>Once you&#8217;ve created the job, the screen gets a bit more hectic. We&#8217;re only going to concern ourselves with a few key areas:</p>

<ul>
<li><p>Build Triggers <img class="right" src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/build-triggers.png"></p></li>
<li><p>Build Steps <img class="right" src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/build-step.png"></p></li>
<li><p>I&#8217;ll frequently refer to the <code>?</code> icon. <img class="right" src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/help-icon.png"></p></li>
</ul>


<h3>Scheduling</h3>

<p>Under build triggers, we want to use the &#8220;Build Periodically&#8221; option. The syntax is akin to cron and there are some additional macros for known intervals. As with any Jenkins option, you can click on the <code>?</code> icon to the right of the option for inline help.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/build-triggers.png" alt="Build Triggers" /></p>

<p>So we&#8217;re going to set up our health check to run every 15 minutes:
<img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/build-periodically.png" alt="Build Periodically" /></p>

<h3>Defining the Build Step</h3>

<p>Through Jenkins plugins, you can get an insane amount of additional build steps. However, the shipped experience has the stuff we need for now. We&#8217;re going to be using the &#8220;Execute Shell&#8221; option. If you are running Jenkins on Windows, you&#8217;ll want to use the &#8220;Execute Windows Batch command&#8221; instead. You will, of course, need to modify the commands appropriately yourself.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/build-step.png" alt="Build Step Options" /></p>

<p>Here&#8217;s the body of our build step:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">#!/</span><span class="n">bin</span><span class="o">/</span><span class="n">bash</span> <span class="o">-</span><span class="n">l</span>
</span><span class='line'><span class="k">CHECK</span><span class="o">=`</span><span class="n">mysql</span> <span class="o">-</span><span class="n">u</span> <span class="n">jenkins</span> <span class="o">-</span><span class="n">ppassword</span> <span class="o">-</span><span class="n">h</span> <span class="mi">192</span><span class="p">.</span><span class="mi">168</span><span class="p">.</span><span class="mi">56</span><span class="p">.</span><span class="mi">101</span> <span class="o">-</span><span class="n">BNe</span> <span class="s1">&#39;SELECT COUNT(*) FROM foo_db.jobs WHERE depth &gt;= 100&#39;</span><span class="o">`</span>
</span><span class='line'><span class="n">exit</span> <span class="err">${</span><span class="k">CHECK</span><span class="err">}</span>
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/db-build-step.png" alt="DB Build Step" /></p>

<h3>Running the job</h3>

<p>Once you click save, you can click &#8220;Build Now&#8221; on the job menu to give it a test. It should fail:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/db-failed.png" alt="Failed build" /></p>

<p>Let&#8217;s modify the job so we can see what success looks like. Click on the &#8220;Configure&#8221; link in the build menu and modify your build step. Set the threshold in the query to <code>101</code>. The build should now be blue:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/db-good.png" alt="Good build" /></p>

<p>This all works very well if you just want to manually inspect the status however let&#8217;s take it a step further. Click on the &#8220;Configure&#8221; link from the Job menu. Notice at the bottom of the following screen, there&#8217;s a section called &#8220;Post-build Actions&#8221;. The very last option is &#8220;E-mail Notification&#8221;. You can click the <code>?</code> to see the default behaviour. Check the box and add your email address:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/email-notification.png" alt="Email Notification" /></p>

<h3>Getting Notified</h3>

<p>Sadly, this isn&#8217;t enough to enable email notifications. You&#8217;ll need to tell Jenkins an SMTP server it can use. Go back to the main menu and click &#8220;Manage Jenkins&#8221;.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/manage-jenkins.png" alt="Manage Jenkins" /></p>

<p>From here, we&#8217;re going to click &#8220;Configure System&#8221;</p>

<p>Another busy screen! The settings in this section can get you in trouble if you aren&#8217;t careful. The most common problem is people attempting to enable security and inadvertently locking themselves out.
We&#8217;re not worried about that for now. Scroll to the bottom and configure your SMTP server. The settings shown are for gmail and you&#8217;ll need to click the &#8220;Advanced&#8221; button to enable additional settings.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/email-settings.png" alt="Configure Email Servers" /></p>

<p>You can select the last checkbox to test that your settings work.</p>

<p>Once that&#8217;s done, click save. Now we&#8217;re going to rerun the job (Go back to the main menu then click your job from the list to see the &#8220;Build Now&#8221; option in the job menu.
You most likely won&#8217;t get an email because the job is passing. Let&#8217;s configure our job again and set the threshold back to 100. Save the job and click &#8220;Build Now&#8221; again from the job menu.</p>

<p>You should get an email that looks something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="n">Subject</span><span class="p">:</span> <span class="n">Build</span> <span class="n">failed</span> <span class="k">in</span> <span class="n">Jenkins</span><span class="p">:</span> <span class="k">Check</span> <span class="n">FooDB</span> <span class="n">backlog</span>
</span><span class='line'><span class="n">See</span> <span class="o">&lt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8080</span><span class="o">/</span><span class="n">job</span><span class="o">/</span><span class="k">Check</span><span class="o">%</span><span class="mi">20</span><span class="n">FooDB</span><span class="o">%</span><span class="mi">20</span><span class="n">backlog</span><span class="o">/</span><span class="mi">8</span><span class="o">/&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="c1">------------------------------------------</span>
</span><span class='line'><span class="n">Started</span> <span class="k">by</span> <span class="k">user</span> <span class="n">anonymous</span>
</span><span class='line'><span class="n">Building</span> <span class="k">in</span> <span class="n">workspace</span> <span class="o">&lt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8080</span><span class="o">/</span><span class="n">job</span><span class="o">/</span><span class="k">Check</span><span class="o">%</span><span class="mi">20</span><span class="n">FooDB</span><span class="o">%</span><span class="mi">20</span><span class="n">backlog</span><span class="o">/</span><span class="n">ws</span><span class="o">/&gt;</span>
</span><span class='line'><span class="p">[</span><span class="n">workspace</span><span class="p">]</span> <span class="err">$</span> <span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">bash</span> <span class="o">-</span><span class="n">l</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">d6</span><span class="o">/</span><span class="n">h7dxb_zj49s8xlj91zd3z6fr0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">hudson1906255485094144268</span><span class="p">.</span><span class="n">sh</span>
</span><span class='line'><span class="n">Build</span> <span class="n">step</span> <span class="s1">&#39;Execute shell&#39;</span> <span class="n">marked</span> <span class="n">build</span> <span class="k">as</span> <span class="n">failure</span>
</span></code></pre></td></tr></table></div></figure>


<p>There&#8217;s not much information in there since our job is swallowing the mysql output and using it as the exit code. You can spice the output however you like it by adding <code>echo</code> statements to the build step. Any output from the job will be included in the email. If you change the thresholds back to a value that you know will pass, you&#8217;ll get at least one email when the build recovers. Unless the build starts failing again, you won&#8217;t get any emails.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="n">Subject</span><span class="p">:</span> <span class="n">Jenkins</span> <span class="n">build</span> <span class="k">is</span> <span class="n">back</span> <span class="k">to</span> <span class="n">normal</span> <span class="p">:</span> <span class="k">Check</span> <span class="n">FooDB</span> <span class="n">backlog</span>
</span><span class='line'><span class="n">See</span> <span class="o">&lt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8080</span><span class="o">/</span><span class="n">job</span><span class="o">/</span><span class="k">Check</span><span class="o">%</span><span class="mi">20</span><span class="n">FooDB</span><span class="o">%</span><span class="mi">20</span><span class="n">backlog</span><span class="o">/</span><span class="mi">9</span><span class="o">/&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Second Job</h2>

<p>So now we&#8217;ve got something handling our DB test. We also needed to check to see if some process was running. Let&#8217;s do a simple one to see if MySQL is running. Let&#8217;s call it &#8220;Check MySQL Running&#8221;. Follow the steps for creating a free-style job but this time we&#8217;re going to create our build step like so:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="c">#!/bin/bash -l</span>
</span><span class='line'>ssh 192.168.56.101 <span class="s1">&#39;ps -ef&#39;</span> | grep mysqld
</span></code></pre></td></tr></table></div></figure>


<p>Again, we&#8217;re going to assume that SSH keys are setup with no password. We&#8217;re keeping it simple. Just as in the case of the other job, we should get a blue build status.</p>

<h2>Third Job</h2>

<p>The third job is the most complex in that we&#8217;re going to need to install a plugin for maximum effect. This will have you jumping around a bit but hopefully you&#8217;re a bit more comfortable navigating by now.
At a high level we&#8217;re going to do the following:</p>

<ul>
<li>Install a new Jenkins plugin</li>
<li>Create a new job</li>
<li>Take note of a new build option</li>
<li>Configure the plugin globally</li>
<li>Enable the plugin in our job</li>
</ul>


<h3>Installing a Plugin</h3>

<p>We&#8217;re going to go back to &#8220;Manage Jenkins&#8221; (accessible from the main menu) but now we&#8217;re going to select &#8220;Manage Plugins&#8221;.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/plugin-main.png" alt="Plugin Page" /></p>

<p>Once on the plugin screen, click the &#8220;Available&#8221; tab. This part can be overwhelming. It&#8217;s especially confusing since plugins will be listed twice if they fall into multiple categories. However, you only need to mark it once.</p>

<p>The plugin we want is called the &#8220;Log Parser Plugin&#8221;. If you can&#8217;t easily find it, use your browser&#8217;s &#8220;find on page&#8221; (CTRL-F, APPLE-F) to find it.</p>

<p>Check the box and click &#8220;Install without Restart&#8221;. You should see a screen similar to this:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/plugin-install.png" alt="Plugin Install" /></p>

<h3>Back to the job</h3>

<p>Now let&#8217;s create our final job. Following the same steps as above, create a new job called &#8220;Check DHCP Errors&#8221;. Again, reaching for a contrived case, I&#8217;m going to check my VM&#8217;s syslog to see if it had any errors related to DHCP.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="c">#!/bin/bash -l</span>
</span><span class='line'>ssh 192.168.56.101 <span class="s1">&#39;tail -n 5 /var/log/syslog&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now we could have done this with a grep statement just like above. However I wanted to show installing plugins and the &#8220;Log Parser Plugin&#8221; actually offers some more flexible options, understands more than just pass or fail and can match multiple items without building overly complex flow into your shell step.</p>

<p>You&#8217;ll notice at the bottom we now have an ADDITIONAL option in our &#8220;Post-build Actions&#8221; - <code>Console output (build log) parsing</code>:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/post-build-log-parser.png" alt="New Option" /></p>

<p>Whenever you install a plugin, where it&#8217;s used depends on what it does. In this case, we&#8217;re doing post processing of the job run log. We can add a third state via this plugin as opposed to just &#8220;Pass&#8221; or &#8220;Fail&#8221; - &#8220;Unstable&#8221;. Before we can enable it, however, we need to give it some parsing rules.</p>

<p>For now leave the option unchecked and click &#8220;Save&#8221;</p>

<h3>Configuring the new plugin</h3>

<p>Go back to the &#8220;Manage Jenkins&#8221; screen (where you set the Email settings). At the bottom, you should now have an option for <code>Console Output Parsing</code>:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/console-output-parsing.png" alt="Console Output Parsing Configuration" /></p>

<p>Again, anything you configure in this section is GLOBAL. Luckily you can define various rule sets for parsing and apply them individualy to jobs. This plugin is a bit complex so you&#8217;ll probably want to look at the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Log+Parser+Plugin">documentation</a>.</p>

<p>We&#8217;re going to create a very basic rules file in <code>/tmp</code> on our LOCAL machine (where Jenkins is running) called <code>jenkins-dhclient-rules</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>warn /^.*dhclient: can<span class="err">&#39;</span>t create .*: No such file or directory<span class="nv">$/</span>
</span><span class='line'>info /^.*dhclient: bound to .*<span class="nv">$/</span>
</span></code></pre></td></tr></table></div></figure>


<p>This is telling the log parser that the following line is a &#8220;warning&#8221;:</p>

<p><code>Jan 23 01:49:52 ubuntu dhclient: can't create /var/lib/dhcp3/dhclient.eth1.leases: No such file or directory</code></p>

<p>and that</p>

<p><code>Jan 23 01:49:52 ubuntu dhclient: bound to 192.168.56.101 -- renewal in 1367 seconds.</code></p>

<p>is informational. These distinctions are handy for the plugin&#8217;s colorized output support.</p>

<p>Now that we&#8217;ve created that file, under the plugin settings we want to name it and give it the location to the file:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/console-parsing-plugin.png" alt="Configured Parsing Rules" /></p>

<h3>Back to our job</h3>

<p>Finally!</p>

<p>Let&#8217;s go back to our new job (Check DHCP Errors) and modify it. We want to enable the parsing plugin in the post-build steps. We&#8217;re going to check &#8220;Mark build Unstable&#8221; for warnings and select our rule. Now save the job. The reason we&#8217;re going for warning is that this error is not fatal. Our system still gets an IP address. What we want to do is draw attention to it.</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/console-parsing-plugin-enabled.png" alt="Post Build Parsing" /></p>

<p>Now if you run the job, you&#8217;ll get a yellow ball indicating that the job is unstable. If we were to change the first line of our rule to error and check the appropriate box, the job would have been marked a failure. Something additional this plugin provides is the &#8220;parsed console output&#8221;. If you click on the job detail and select &#8220;Parse Console Output&#8221; from the job menu, you&#8217;ll actually get a nicer way to see exactly what was wrong:</p>

<p><img src="http://lusis.github.com/images/posts/lowtech-monitoring-with-jenkins/parsed-output.png" alt="Parsed Console Output" /></p>

<p>Again, this is a totally contrived example. Obviously we would fail long before the parsing had the host been down.</p>

<h1>Tying it all together</h1>

<p>All of these individual jobs are neat but there&#8217;s an obvious dependency there. We need to be able to SSH to the host, we need mysql to be running and then we want to query it. We don&#8217;t want multiple emails for each failure. We only want the actual failed job to alert us. Let&#8217;s chain these jobs together to match that flow.</p>

<ul>
<li>Under the &#8220;MySQL Running&#8221; and &#8220;FooDB&#8221; jobs, disable the cron schedule. We only want it on the &#8220;DHCP&#8221; job.</li>
<li>Under the DHCP job, we&#8217;re going to select the Post-build step of &#8220;Build other projects&#8221;</li>
<li>Check &#8220;Trigger even if build is unstable&#8221; since we know it&#8217;s going to be unstable.</li>
<li>In the text area, we want to add our &#8220;Check MySQL Running&#8221; job</li>
<li>Under the &#8220;Check MySQL Running&#8221; job, we want to select &#8220;Trigger only if build succeeds&#8221; and set our text area to the &#8220;Check FooDB Backlog&#8221; job.</li>
</ul>


<p>Now if you run the top-level job (Check DHCP Errors), all of the jobs will run. If any fail, the run will stop there and alert you! Since this is now scheduled, every 15 minutes this entire workflow will be checked.</p>

<h1>Additional plugins and tips</h1>

<p>Jenkins has a boatload of plugins. It&#8217;s worth investigating them to see if they make some given task (like output parsing) easier. Some provide additional notification paths like jabber or irc. Others provide additional build step types in specific languages like Groovy or Powershell. You can also do things like create a &#8220;Parameterized Build&#8221;. This is especially handy for thresholds. There&#8217;s also a very handy SSH plugin that let&#8217;s you define hosts globally and keys per host. This helps clean up your build steps too.</p>

<p>One plugin that was recommended is the &#8220;Email-ext&#8221; plugin. This allows you to REALLY spice up and configure your email notifications.</p>

<p>There&#8217;s a plugin for checking a web site for some criteria and plugins for starting virtual machines. There are also plugins for creating a radiator view so you can get a nice big dashboard for just checking the state of jobs at a glance.</p>

<p>The key to remember is that Jenkins is an unopinionated build tool. This flexiblity lends itself to doing off-the-wall stuff (like being a monitoring system or a cron replacement). The trick is translating the concepts and terminology of building software to something that fits your use case.</p>

<h1>Additional Credits</h1>

<p>I&#8217;d like to thank <a href="https://twitter.com/miller_joe">Joe Miller</a>, <a href="https://twitter.com/ches">Ches Martin</a> and <a href="https://twitter.com/agentdero">R. Tyler Croy</a> for reviewing this post and offering up corrections, tips and advice.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scale10x Recap]]></title>
    <link href="http://lusis.github.com/blog/2012/01/22/scale10x-recap/"/>
    <updated>2012-01-22T07:02:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/01/22/scale10x-recap</id>
    <content type="html"><![CDATA[<p>This past week I had the awesome pleasure of participating in my first <a href="http://www.socallinuxexpo.org/">SoCal Linux Expo</a>. As I later discovered, this was the 10th installment of this awesome event (hence the 10x).</p>

<!-- more -->


<h1>The email and enStratus</h1>

<p>I got an email from <a href="https://twitter.com/irabinovitch">Ilan Rabinovitch</a> just as things were going down with me headed to <a href="http://enstratus.com">enStratus</a>. Since the event was going to be right around the time I started, I pretty much put it out of my mind.
Then I realized that <a href="https://twitter.com/botchagalupe">my boss</a> was going to be attending. I figured it wouldn&#8217;t hurt to ask and it was decided that I would shadow John on his trip to enStratus HQ for my mandatory cultural immersion (translation: does the fat redneck own a winter coat?) and on to SCaLE. While I didn&#8217;t make it to Minnesota (diverted to San Jose on business), I did still make it to the conference.</p>

<p>I had an awesome time in San Francisco and San Jose. I got to meet a great bunch of folks and geek out hardcore.</p>

<h1>Monitoring sucks</h1>

<p>Ilan asked me if I would be willing to be on a panel about the whole <a href="https://github.com/monitoringsucks">#monitoringsucks</a> thing. We were able to score a great panel of folks:</p>

<ul>
<li>Simon Jakesch from Zenoss</li>
<li>James Litton from PagerDuty</li>
<li>Jody Mulkey from Shopzilla</li>
</ul>


<p>The event was awesome. I did some minor introductions, got the ball rolling with some questions and then we let the audience take it from there.
The participation was AWESOME. We had great questions from the audience and the feedback I got AFTER the fact was mindblowing. One particular post-panel question is worth a blog post in its own right.</p>

<p>One thing that really stood out was this: People just don&#8217;t know where to start. The landscape is pretty &#8220;cluttered&#8221;. <a href="https://twitter.com/cwebber">Chris Webber</a> brought up a very salient point that I sometimes forget; When we talk about &#8220;monitoring&#8221;, we&#8217;re really talking about multiple things - collection, alerting, visualization, trending and multitudes of other aspects.</p>

<p>I got asked several times in the hallway - &#8220;What should I use?&#8221; or &#8220;What do you think about <foo>?&#8221;. My first response was always &#8220;What are you using now?&#8221;.</p>

<p>I like to think I&#8217;m pretty pragmatic. I love the new shiny. I love pretty graphs. I&#8217;m a technologist. However, I know when to be realistic. My thought process goes something like this:</p>

<h2>Do you have something in place now?</h2>

<h3>Yes</h3>

<p>Why are you looking to switch? Is it unreliable? Is it painful to configure? Basically, if it&#8217;s getting the job done and has relatively minor overhead there&#8217;s no reason to switch.
The pain points for me with monitoring solutions usually come much later. It doesn&#8217;t scale or scaling it is difficult. It doesn&#8217;t provide the visibility I need. It&#8217;s unreliable (usually due to scaling problems).
Until then, use what you&#8217;ve got and be guard for early signs of problems like check latency going up or missed notifications.</p>

<p>If you have a configuration management solution in place, it probably has native support for configuring Nagios. When you add a new host to your environment, you only need to tell your CM tool to run on your monitoring server. If you&#8217;ve done any sort of logical grouping, you&#8217;ll have the right things monitored quickly.</p>

<h3>No</h3>

<p>If you don&#8217;t have ANYTHING in place, you need to cover two bases pretty quick:</p>

<ul>
<li>Outside-In Checks: is my site up and responding timely?</li>
<li>Stupid stuff: Are my disks filling up? Is my database slave behind?</li>
</ul>


<p>For outside in checks, use something quick and easy like Pingdom. For the inside checks, don&#8217;t underestimate the power of a cron job. If you want something a bit more packaged, look at <a href="http://mmonit.com">monit</a>. It&#8217;s dead simple and can get you to a safe place.</p>

<h2>A note on visibility</h2>

<p>Monitoring tools are great but many times they fall down when you need to diagnose a problem ex post facto. If you went the simple route, you probably don&#8217;t have any real trending data. This is where many complaints start to come from folks. You end up monitoring the same thing twice - once for alerting systems like Nagios and another time for your visualization, trending and other engines. When you reach this point, start looking at things like Sensu or all-in-one solutions that, while cumbersome and imprecise use the same collected data - Zenoss, Zabbix, Icinga (originally a fork of Nagios).</p>

<p>The event was recorded (both audio and video) but I have no timeframe on when it&#8217;s going to be available but I&#8217;ll let you know as soon as it&#8217;s up.</p>

<h1>The rest of the conference</h1>

<p>The rest of the conference was epic as well. Being that this was my first time, I didn&#8217;t know what to expect. The thing that most stood out was the number of children. This was probably the most family friendly conference I&#8217;ve ever been to. Encouraging stuff. Plenty of events and in fact an entire track dedicated to children.</p>

<p>I didn&#8217;t get to attend as many talks as I wanted to. While the facility was really nice, the building is like a faraday cage. My phone spent what little battery life it had just trying to get a signal. I spent quite a bit of time running back to my room to charge up. <a href="https://twitter.com/cwebber">Chris Webber</a> totally got me hooked on portable chargers.</p>

<h1>Juju talk</h1>

<p><em>disclaimer: I&#8217;m fully aware that Juju is undergoing heavy active development and is a very young project</em></p>

<p>One of the talks I attended was on <a href="http://juju.ubuntu.com">Juju</a>. I was probably a bit harsh on Juju when it was first announced. The original name was much better and the whole witch doctor thing just doesn&#8217;t sit well with me.</p>

<p>I also hate the tag line - &#8220;DevOps distilled&#8221;. It&#8217;s marketing pure and simple. I have very little tolerance for things that bill themselves as a &#8220;devops tool&#8221; or &#8220;for devops&#8221;.</p>

<p>But more than the name, something about Juju didn&#8217;t feel right. After the talk, something still doesn&#8217;t feel right. While I don&#8217;t like pooping all over someone else&#8217;s hard work so writing this part is tough.</p>

<h2>Where does it fit?</h2>

<p>Right now, I don&#8217;t think Juju even knows where it fits. It&#8217;s got some great ideas and on any other day, I&#8217;d be all over it. The problem is that Juju tries to do too much in some areas and not enough in others.</p>

<p>Parts of Juju are EXACTLY what I see as my primary use case for Noah. The service orchestration is great. The ideas are pretty solid. Juju even uses ZooKeeper under the hood.</p>

<h2>Services not servers</h2>

<p>Everyone knows that I preach the mantra of &#8220;services matter. hosts don&#8217;t&#8221;</p>

<p>The problem is that in an attempt to be the Nagios (unlimited flexibility) of configuration management, it can&#8217;t actually do enough in that area. Because it only concerns itself with services (and the configuration of them), it doesn&#8217;t do enough to manage the host. Just because the end state is &#8220;I&#8217;m serving a web page&#8221; doesn&#8217;t mean you should ignore the host its running on. Since Juju isn&#8217;t designed to deal with that (and actually LACKS any primitives to do it), you&#8217;re left with needing to manage a system in multiple places - once with your CM tool and then again with the charms.</p>

<p>Someone said it best when he described Juju as &#8220;apt for services&#8221;. It&#8217;s quite evident that the same broken mentality that apt takes to managing packages is applied to Juju as well. Charms have upgrade and downgrade steps. They&#8217;re just as complicated too. Not only is there no standard (since charms can be written in any language) it&#8217;s actually detrimental. The reason for a common DSL or language like the ones exposed by CM tools is not some academic mental masturbation. It&#8217;s repeatability and efficiency. I can go into a puppet shop and look at a module and know what it does. I can look at most chef recipes (outside of ones that might use a custom LWRP) and know what&#8217;s going on.</p>

<p>In the Juju world, a single charm could be written in one spot in Python and another spot in Bash. It pushes too much responsibility to the end user NOT to mess something up. I dare say that idempotence doesn&#8217;t even exist in Juju.</p>

<h2>A fair shake</h2>

<p>Again, I&#8217;m going to do some more playing around with Juju. I think it can meet a critical need for folks but I think they need to revisit what problem they&#8217;re trying to solve. I appreciate the work they&#8217;ve done and I&#8217;m totally excited that orchestration is getting the proper attention. The presenters were fantastic.</p>

<h1>Other stuff</h1>

<p>I attended a really good talk about the history of Openstack and where it&#8217;s going. It was great. As someone who is working with openstack professionally now (and had just dealt with some of its warts not 3 days before hand), I found it very valuable. Also congrats to the speaker, <a href="https://twitter.com/anotherjesse">Jesse Andrews</a> on the birth of his first child!</p>

<p>I managed to make it to Brendan Gregg&#8217;s talk as well. If you ever have the opportunity to hear him speak, you should take it. While I&#8217;m not a SmartOS user, the talk was really not about that. I walked out with some amazing insight on how smart people troubleshoot performance problems. Very well done.</p>

<h1>The hallway track</h1>

<p>Of course the real value in any conference is the hallway track. The chance to interact with your peers. I met so many smart people (some twice because I suck at remembering faces at first - sorry!). Chatting with folks like C. Flores, Jason Cook, Sean O&#8217;Meara, Chris Webber, Dave Rawks, Matt Ray, Matt Silvey and so many others that I can&#8217;t keep straight in my head. Everyone was awesome and I hope that you were able to get as much out of me as I got out of you.</p>

<p>Thanks again to Ilan for the invitation and for running such an amazing conference.</p>

<p>Also, little known made-up fact: Lusis is Tagalog for &#8220;He who eats with both hands&#8221;&#8230;..</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2011-in-review]]></title>
    <link href="http://lusis.github.com/blog/2012/01/02/2011-in-review/"/>
    <updated>2012-01-02T15:40:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/01/02/2011-in-review</id>
    <content type="html"><![CDATA[<p>The holidays are a busy time for me. I was hoping to get this written before the end of the year but it didn&#8217;t happen.</p>

<!-- more -->


<p>I can say, wihtout a doubt, that 2011 has been the most awesome year both professionally and personally in my life. And I have pretty much everyone else to thank for it.</p>

<h1>Some special shoutouts</h1>

<p>There are so many folks to thank for this year. I owe so many beers that I can&#8217;t keep count. I can&#8217;t possibly thank everyone but I want to throw a few special shoutouts to folks.</p>

<h2>My wife</h2>

<p>Why she puts up with my shit, I&#8217;ll never know. Needless to say, without her this year would have been radically different. She managed two toddlers by herself on almost every trip I took. She&#8217;s been nothing but encouraging and she also helps keep me grounded by reminding me what&#8217;s important.</p>

<h2><a href="http://twitter.com/patrickdebois">Patrick DeBois</a></h2>

<p>Thanks for giving me the opportunity to help with the DevOps Days events. Through the events I&#8217;ve met some of the most amazing people in the world. The DevOps community is a wonderful group of folks and I would never have met half of them if Patrick hadn&#8217;t given me the opportunity to participate.</p>

<h2><a href="http://verticalacuity.com">Vertical Acuity</a></h2>

<p>While I&#8217;m sad to be leaving friends behind, VA was amazing in letting me travel so much. Not only that but they trusted and valued my opinion on so many things. Whoever takes my place will be lucky to work with such an awesome group of folks.</p>

<h2><a href="http://twitter.com/botchagalupe">John Willis</a></h2>

<p>Not only for being a good friend but for giving me an opportunity to work with him at enStratus.</p>

<h2><a href="http://twitter.com/puppetmasterd">Luke Kanies</a>, <a href="http://twitter.com/kartar">James Turnbull</a>, <a href="http://twitter.com/cruzfox">Jose Palafox</a> and the Puppet Labs crew</h2>

<p>Puppet Labs gets double the thanks - for giving me the opportunity to talk about my project at PuppetConf and also for sponsoring me to travel to Goteborg and speak. The whole crew over there is amazing.</p>

<h2><a href="http://twitter.com/damonedwards">Damon Edwards</a>, <a href="http://twitter.com/alexhonor">Alex Honor</a> and <a href="http://www.dtosolutions.com">DTO Solutions</a></h2>

<p>I&#8217;m grateful to DTO for giving me the opportunity to attend Velocity and letting me be a booth babe. Damon and Alex both have been forces of awesome for the DevOps community.</p>

<h2><a href="http://twitter.com/potus98">John Christian</a></h2>

<p>John asked me early on to help with the Atlanta DevOps meetups and I&#8217;m glad he did. He stands alone in the corporate bullshit world of the financial services industry. He tought me a lot and I want to thank him for it.</p>

<h2><a href="http://twitter.com/lnxchk">Mandi Walls</a></h2>

<p>For being pretty much awesome by listening to my ranting, letting me bounce ideas off her. And for being my sister from another mother.</p>

<h2><a href="http://twitter.com/schisamo">Seth Chisamore</a></h2>

<p>For the various lunches, talks, introductions and meetup involvement. Local folks rock. Seth rocks.</p>

<h2><a href="http://twitter.com/jordansissel">Jordan Sissel</a></h2>

<p>For being awesome, down to earth and not an asshole. And for all the code. And for giving me the honor of contributing to SysAdvent.</p>

<h2><a href="http://twitter.com/kelseyhightower">Kelsey Hightower</a></h2>

<p>For helping me navigate my foray into the world of Python (technically that was a few years ago). Also for showing folks how to just get shit done.</p>

<h2>Everyone else</h2>

<p>I can&#8217;t possible fit everyone here&#8217;s an abbreviated list of folks, in no specific order, who have impacted me this year off the top of my head. If I leave you off, please don&#8217;t take offense. I&#8217;m shooting from the cuff here.</p>

<p><a href="http://twitter.com/bradleyktaylor">Bradley Taylor</a>, <a href="http://twitter.com/wfarr">Will Farrington</a>, <a href="http://twitter.com/coreyhaines">Corey Haines</a>, <a href="http://twitter.com/dysinger">Tim Dysinger</a>, <a href="http://twitter.com/miller_joe">Joe Miller</a>, <a href="http://twitter.com/roidrage">Mathias Meyer</a>, <a href="http://twitter.com/vvuksan">Vladimir Vuksan</a>, <a href="http://twitter.com/adamhjk">Adam Jacob</a>, <a href="http://twitter.com/portertech">Sean Porter</a>, <a href="http://twitter.com/bascule">Tony Arcieri</a>, <a href="http://twitter.com/ripienaar">R.I. Pienaar</a>, <a href="http://twitter.com/adamfblahblah">Adam Fletcher</a>, <a href="http://twitter.com/anthonygoddard">Anthony Goddard</a>, <a href="http://twitter.com/williamsjoe">Joe Williams</a>, <a href="http://twitter.com/boorad">Brad Anderson</a>, Cat Muecke (alas, Cat does not tweet!), <a href="http://twitter.com/harlanbarnes">Harlan Barnes</a>, <a href="http://twitter.com/geemus">Wesley Beary</a>, <a href="http://twitter.com/mitchellh">Mitchell Hashimoto</a>, <a href="http://twitter.com/wayneeseguin">Wayne Seguin</a>, <a href="http://twitter.com/kallistec">Dan DeLeo</a>, <a href="http://twitter.com/jtimberman">Josh Timberman</a>, <a href="http://twitter.com/kantrn">Noah Kantrowitz</a>, <a href="http://twitter.com/littleidea">Andrew Clay Schafer</a>, <a href="http://twitter.com/markimbriaco">Mark Imbriaco</a>, <a href="http://twitter.com/lordcope">Stephen Nelson-Smith</a>, <a href="http://twitter.com/garethr">Gareth Rushgrove</a>, <a href="http://twitter.com/ianmeyer">Ian Meyer</a>, <a href="http://twitter.com/f3ew">Devdas Bhagat</a>, <a href="http://twitter.com/actionjack">Martin Jackson</a>, <a href="http://twitter.com/mleinart">Michael Leinartas</a>, <a href="http://twitter.com/KrisBuytaert">Kris Buytaert</a>, <a href="http://twitter.com/solarce">Brandon Burton</a>, <a href="http://twitter.com/altobey">Al Tobey</a>, <a href="http://twitter.com/matthew_jones">Matthew Jones</a>, <a href="http://twitter.com/builddoctor">Julian Simpson</a>, <a href="http://twitter.com/macros">Jason Cook</a>, <a href="http://twitter.com/jiboumans">Jos Boumans</a>, <a href="http://twitter.com/susanpotter">Susan Potter</a>, <a href="http://twitter.com/thommay">Thom May</a>, <a href="http://twitter.com/kit_plummer">Kit Plummer</a>, <a href="http://twitter.com/sascha_d">Sascha Bates</a>, <a href="http://twitter.com/unclebobmartin">Bob Martin</a>, <a href="http://twitter.com/bdha">Bryan Horstmann-Allen</a>, <a href="http://twitter.com/benjaminws">Benjamin W. Smith</a>, <a href="http://twitter.com/ches">Ches Martin</a>, <a href="http://twitter.com/obfuscurity">Jason Dixon</a>, <a href="http://twitter.com/philiph">Phil Hollenback</a>, <a href="http://twitter.com/rockpapergoat">Nate St. Germain</a>, <a href="http://twitter.com/ohlol">Scott Smith</a>, <a href="http://twitter.com/seancribbs">Sean Cribbs</a>, <a href="http://twitter.com/argv0">Andy Gross</a>, <a href="http://twitter.com/benr">Ben Rockwood</a>, <a href="http://twitter.com/jamesc_000">James Casey</a>, <a href="http://twitter.com/lhazlewood">Les Hazlewood</a>, <a href="http://twitter.com/aditzel">Allan Ditzel</a>, <a href="http://twitter.com/mariusducea">Marius Ducea</a>, <a href="http://twitter.com/noahcampbell">Noah Campbell</a>, <a href="http://twitter.com/timanglade">Tim Anglade</a>, <a href="http://twitter.com/atmos">Corey Donohoe</a>, <a href="http://twitter.com/standaloneSA">Matt Simmons</a>, <a href="http://twitter.com/ernestmueller">Ernest Mueller</a>, <a href="http://twitter.com/auxesis">Lindsay Holmwood</a>, <a href="http://twitter.com/redbluemagenta">Christian Paredes</a>, <a href="http://twitter.com/_masterzen_">Brice Figureau</a>, <a href="http://twitter.com/griggheo">Grig Gheorghiu</a>, <a href="http://twitter.com/dje">Darrin Eden</a>, <a href="http://twitter.com/kimchy">Shay Banon</a>, <a href="http://twitter.com/ramonvanalteren">Ramon Van Alteren</a> and so many others.</p>

<h1>Software that changed my world</h1>

<p>I also wanted to give a shoutout to a few projects that pretty much changed how I thought about the software world around me.</p>

<h2><a href="http://elasticsearch.org">ElasticSearch</a></h2>

<p>ElasticSearch has beeen, bar none, in my top two amazing things the past year. Having first heard about it via Logstash, when I started digging in it blew my mind. The one thing that amazed me most about ES was the Zen discovery. It&#8217;s like the first time you heard about consistent hashing. It&#8217;s one of those things that makes you say &#8220;how the fuck did I not think of this first?&#8221;. The other thing that I find awesome is that ES not only makes scaling up painless but scaling DOWN (which is the hard part) is just as easy. As a sysadmin, ElasticSearch has been the most pleasant bit of infrastructure I&#8217;ve ever had the pleasure of standing up.</p>

<h2><a href="http://zeromq.org">0mq</a></h2>

<p>The other thing that amazed me this year was 0mq. 0mq essentially makes the difficult and next to impossible things possible. I am not lying when I say that every project I have floating around in my head is either built around or has a perfect spot for 0mq. Along with ElasticSearch, it has fundamentally changed how I think about software, infrastructure and more.</p>

<h2><a href="http://zookeeper.apache.org">Apache ZooKeeper</a></h2>

<p>While I&#8217;m not a fan of ZooKeeper on several levels, It would be wrong to totally ignore it. For the longest time, ZK stood alone in what it provided. People are building amazing things with it and it inspired me to write Noah.</p>

<h2><a href="http://logstash.net">Logstash</a></h2>

<p>At first I was pretty dismissive of Logstash. Mainly because I didn&#8217;t have a real need. Then I started digging in and realized that Logstash is only tangentially about logs. Logstash is kind of what you always wanted a pipe to be. Arbitrary input, arbitrary filtering, arbitrary output. The use cases for logstash are so much greater when you stop thinking about logs and start thinking about moving data.</p>

<h2><a href="http://erlang.org">Erlang</a>, <a href="http://www.erlang.org/doc/design_principles/users_guide.html">OTP</a> and <a href="http://basho.com">Riak</a></h2>

<p>While my Erlang only extends to passable reading, via Riak, I found a desire to learn more about it. The biggest thing that stuck in my head and also changed the way I think is the Actor model. For the first time that I can remember, a concept made perfect sense to me. While learning Erlang in earnest is a goal for 2012, I think about how simple and understandable the Actor model was.</p>

<h2><a href="http://celluloid.github.com">Celluloid</a></h2>

<p>Following up on the Actor model, I have mad respect for Tony Arcieri. If you look back at his projects, they all follow a similar theme: improving the concurrency story on Ruby. He&#8217;s tenacious and passionate about something that most people would have (and if they&#8217;re arrogant dickfaces) laughed at by now. Celluloid brings some amazing functionality to Ruby inspired by Erlang and OTP. I find myself defaulting to it whenever I need to even think about concurrency in Ruby. Even outside of that, it encourages good behaviour with threads and reduces the chances you&#8217;ll fuck something up. The companion project, DCell, is something I&#8217;m itching to work with as well.</p>

<h2><a href="http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/">Statsd</a> and <a href="http://graphite.wikidot.com/">Graphite</a></h2>

<p>If this past year was about anything, it was about metrics. Lots and lots of metrics. Etsy pushed out statsd and brought metrics collection to the masses. Coda Hale gave an amazing talk on metrics and released the code to back it up. Shooting in the dark sucks. You need numbers. Collect ALL the metrics.</p>

<h1>Final Thoughts</h1>

<p>The world of open source and the community around devops is amazing. I learned from and met so many people in 2011. I&#8217;m hoping that 2012 is the year that I can give back to them in some small way. Thanks to everyone for making this past year amazing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Github Trolling for Fun and Profit]]></title>
    <link href="http://lusis.github.com/blog/2011/11/22/github-trolling-for-fun-and-profit/"/>
    <updated>2011-11-22T21:43:00-05:00</updated>
    <id>http://lusis.github.com/blog/2011/11/22/github-trolling-for-fun-and-profit</id>
    <content type="html"><![CDATA[<p>Last Friday was a pretty crappy day.</p>

<!-- more -->


<p>I&#8217;m a fairly <em>active</em> Twitter user.</p>

<p>Patrick has joked that it&#8217;s as if I have Twitter wired directly to my brain. It&#8217;s not far from the truth.
I like to engage people and normally Twitter is great medium for engaging folks. Unfortunately, the message size limit makes Twitter an imperfect medium for involved discussions.</p>

<p>I know better but sometimes I forget.</p>

<p>Anyway, last friday I realized near the end of the day that I had pretty much gone off the rails. If I wasn&#8217;t bitching about Maven and Java, I was involved in random discussions about the SaltStack project. Combine that with normal inane bullshit and I somehow managed to pull off a 60+ tweets. It took a comment by <code>roidrage</code> on IRC to point out that I really needed to calm down.</p>

<p>With that, I declared communication blackout for the weekend. I decided to go to happy hour and spend the weekend just having fun with the family. It was awesome. So sorry for the sheer number of people I managed to piss off on Friday.</p>

<h1>Trolling Github</h1>

<p>One of the things I also decided to do not stress about making time to hack. I knew that if I got working on one of my projects, I would totally stay distracted thinking about it.</p>

<p>So I went trolling. On Github.</p>

<p>I was just poking around Github, when I saw in my feed that some commits were done to the <a href="https://github.com/imatix/zguide">user&#8217;s guide for ZeroMQ</a>. Because I have a serious geek woody for ZeroMQ, I got wrapped up reading the guide and looking at some of the more advanced examples. I&#8217;ve got some ideas I want to implement in Ark and Noah that involved 0mq so I figured it would be time well spent.</p>

<p>Now if anyone has bothered to read the <a href="http://zguide.zeromq.org/">zguide</a>, you&#8217;ll know that one of the BEST parts is the code samples. Seriously. They have examples for all of the architectures in almost every language. I don&#8217;t know a single goddamn person who knows <a href="http://en.wikipedia.org/wiki/HaXe">Haxe</a>, but there are examples in the guide for Haxe. You can see an example of what I&#8217;m talking about <a href="http://zguide.zeromq.org/page:all#Divide-and-Conquer">here</a>.</p>

<p>Notice at the bottom the list of examples for languages. If you mouse over the last entry, many times you&#8217;ll get multiples highlighted. This means that chunk of highlighted languages doesn&#8217;t have any examples written.</p>

<p>I noticed that quite a few of the advanced ones didn&#8217;t have Ruby versions. I started back at the beginning of the guide until I found the first one that didn&#8217;t have a Ruby example - the <code>interrupt</code> example.</p>

<h1>Challenge Accepted</h1>

<p>So here I am - resolved not to work on any of my own projects and knowing that I didn&#8217;t have time to get involved with something TOO heavy. I decided to fork the guide and start adding missing Ruby examples.</p>

<p>Now I only got two example done the entire weekend. This mainly revolved around how limited my time was but also around getting REALLY comfortable with <a href="https://github.com/chuckremes/ffi-rzmq">ffi-rzmq</a>. I wanted to make sure that the examples I wrote had the write mix of idiomatic Ruby and yet explicit enough for someone who didn&#8217;t know the specifics of <code>ffi-rzmq</code>.</p>

<p>One that I really struggled with was this one:</p>

<p><a href="https://github.com/imatix/zguide/commit/4c231d1023819152813fad09a45458bd33cb02a9">https://github.com/imatix/zguide/commit/4c231d1023819152813fad09a45458bd33cb02a9
</a></p>

<p>If you get familiar with the zguide, you&#8217;ll see a lot of references to <code>zhelpers</code>. It&#8217;s really just a bunch of boilerplate code that helps keep the actual examples to a nice consumable chunk size. There was not a <code>zhelpers</code> for the Ruby examples. I looked at the others to get an idea of what kinds of things were in there. In relation to the <code>identity</code> examples, there was a dump helper that just dumped the contents of a message. If you look at the <a href="https://github.com/imatix/zguide/blob/master/examples/Python/zhelpers.py">Python</a> and <a href="https://github.com/imatix/zguide/blob/master/examples/C/zhelpers.h">C</a> examples for <code>dump</code>, you&#8217;ll see how they pull the identity of the message out. An interesting comparision, is how the <a href="https://github.com/imatix/zguide/blob/master/examples/Scala/utils.scala">Scala</a> version of <code>dump</code> works.</p>

<p>Instead of focusing on duplicating the strategy employed by the C and Python versions, I went with something that fit how <code>ffi-rzmq</code> works a bit more. I realized that the point was not the content of the helpers so much as the end result, showing that 0mq would generate an identity for a message if one wasn&#8217;t explcitly provided.</p>

<p>I&#8217;m quite sure that at some point, ZMQ::Message objects will get an attribute accessor to simply return the identity. Right now the code base is under a bit of a refactor.</p>

<h1>Call to Action</h1>

<p>I really want to encourage others to do something like this. No pressure. Just troll Github. Look at some projects that are interesting. Look at the open issues. Fork, fix a bug or two and make some pull requests. After that, go on your merry way. No obligations. At worst, you&#8217;ve spent some time sharpening your skills. At best, however, you&#8217;ve made a lasting contribution.</p>

<p>And shit, it doesn&#8217;t even have to be a code contribution. If you can grok the project well enough, add some wiki pages.</p>

<p>I dunno. Github just has this amazingly easy flow for contribution. Do unto others and all that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy ALL the Things]]></title>
    <link href="http://lusis.github.com/blog/2011/10/18/deploy-all-the-things/"/>
    <updated>2011-10-18T06:59:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/10/18/deploy-all-the-things</id>
    <content type="html"><![CDATA[<p><em>This is part 2 in a post on deployment strategies. The previous post is located <a href="http://blog.lusis.org/blog/2011/10/18/rollbacks-and-other-deployment-myths/">here</a></em></p>

<p>My previous post covered some of the annoying excuses and complaints that people like to use when discussing deployments. The big take away should have been the following:</p>

<ul>
<li>The risk associated with deploying new code is not in the deploy itself but everything you did up to that point.</li>
<li>The way to make deploying new code less risky is to do it more often, not less.</li>
<li>Create a culture and environment that enables and encourages small, frequent releases.</li>
<li>Everything fails. Embrace failure.</li>
<li>Make deploys trivial, automated and tolerant of failure.</li>
</ul>


<p>I want to make one thing perfectly clear. I&#8217;ve said this several times before. You can get 90% of the way to a fully automated environment, never go that last 10% and still be better off than you were before. I understand that people have regulations, requirements and other things that prevent a fully automated system. You don&#8217;t ever have to flip that switch but you should strive to get as close as possible.</p>

<!--more-->


<h1>Understanding the role of operations</h1>

<p>Operations is an interesting word. Outside of the field of IT it means something completely different than everywhere else in the business world. <a href="http://en.wikipedia.org/wiki/Business_operations">According to Wikipedia</a>:</p>

<blockquote><p>Business operations encompasses three fundamental management imperatives that collectively aim to maximize value harvested from business assets</p>

<ul>
<li><p>Generate recurring income</p></li>
<li><p>Increase the value of the business assets</p></li>
<li><p>Secure the income and value of the business</p></li>
</ul>
</blockquote>

<p>IT operations traditionally does nothing in that regard. Instead IT operations has become about cock blocking and being greybeareded gatekeepers who always say &#8220;No&#8221; regardless of the question. We shunt the responsibility off to the development staff and then, in some sick game of &#8216;fuck you&#8217;, we do all we can to prevent the code from going live. This is unsustainable; counter-productive; and in a random twist of fate, self destructive.</p>

<p>One thing I&#8217;ve always tried to get my operations and sysadmin peers to understand is that we are fundamentally a cost center. Unless we are in the business of managing systems for profit, we provide no direct benefit to the company. This is one of the reasons I&#8217;m so gung-ho on automation. <a href="https://twitter.com/botchagalupe">John Willis</a> really resonated with me in the first Devops Cafe podcast when he talked about the 80/20 split. Traditionally operations staff spends 80% of its time dealing with bullshit fire-fighting muck and 20% actually providing value to the business. The idea that we can flip that and become contributing members of our respective companies is amazing.</p>

<p>Don&#8217;t worry. I&#8217;ll address development down below but I felt it was important to set my perspective down before going any further.</p>

<h1>Technical Debt and Risk Management</h1>

<p>Glancing back to my list of take-aways from the last post, I make a pretty bold (to some people) statement. When I say that deploy risk is not the deploy itself but everything up to that point, I&#8217;m talking about technical debt.</p>

<p>Technical debt takes many forms and is the result of both concious, deliberate choices as well as unintended side-effects. Some examples of that are:</p>

<ul>
<li>Lack of or insufficient testing and associated</li>
<li>Overreliance on time consuming manual processes</li>
<li>Shortcuts to meet deadlines - both artifical and real</li>
<li>Violation of the 10-minute maxim</li>
<li>Technological choices</li>
<li>Cultural choices</li>
<li>Fiscal limitations</li>
</ul>


<p>All of these things can lead to technical debt - the accumulation of dead bodies in the room as a byproduct of how we work. At best, someone at least acknowledges they exist. At worst, we stock up on clothespins, pinch our nostrils shut and hope no one notices the stench. Let&#8217;s address a couple of foundational things before we get into the fun stuff.</p>

<h2>Testing</h2>

<p>Test coverage is one of the easiest ways to manage risk in software development. One of the first things to go in a pinch is testing. Even that assumes that testing was actually a priority at some point. I&#8217;m not going to harp on things like 100% code coverage. As I said previously, humans tend to overcompensate. Test coverage is also, however, one of the easiest places to get your head above water. If you don&#8217;t have a culture of committment to testing, it&#8217;s hard but not impossible to get started. You don&#8217;t have to shutdown development for a week.</p>

<ol>
<li>Start by having a commitment to write tests for any new code going forth.</li>
<li>As bugs arise in untested code, make a test case for the bug a requirement to close the bug.</li>
<li>Find a small victory in existing code. Create test coverage for low hanging fruit.</li>
<li>Plan for a schedule to cover any remaining code</li>
</ol>


<p>The key here is baby steps. Small victories. Think Fezzik in &#8216;The Princess Bride&#8217; - <em>&#8220;I want you to feel like you&#8217;re winning&#8221;.</em></p>

<p>Testing is one of the foundations you have to have to reach deploy nirvana. System administrators have a big responsiblity here. Running tests has to be painless, unobstrusive and performant. You should absolutely stand up something like Jenkins that actually runs your test suite on check-in. As that test suite grows, you&#8217;ll need to be able to provide the capacity to grow with it. That&#8217;s where the next point can be so important.</p>

<h2>Manual processes</h2>

<p>Just as testing is a foundation on the code side, operations has a commensurate responsibility to reduce the number of human hands involved with creating systems. We humans, despite the amazing potential that our brains provide, are generally stupid. We make mistakes. Repeatability is not something we&#8217;re good at. Some sort of automated and repeatable configuration management strategy needs to be adopted. As with testing, you can make some amazing progress in baby steps by introducing some sort of proper configuration management going forward. I don&#8217;t recommend you attempt to retrofit complex automation on top of existing systems beyond some basics. Otherwise you&#8217;ll be spending too much time trying to differentiate between &#8220;legacy&#8221; and &#8220;new&#8221; servers roles. If you are using some sort of virtualization or cloud provider like EC2, this is a no brainer. It&#8217;s obviously a bit harder when you&#8217;re using physical hardware but still doable.</p>

<p>Have you ever played the little travel puzzle game where you have a grid of moving squares? The idea is the same. You need just ONE empty system that you can work with to automate. Pick your simplest server role such as an apache webserver. Using something like Puppet or Chef, write the &#8216;code&#8217; that will create that role. Don&#8217;t get bogged down in the fancy stuff the tools provide. Keep it simple at first. Once you think you&#8217;ve got it all worked out, blow the server away and apply that code from bootstrap. Red, green, refactor. Once you&#8217;re comfortable that you can reprovision that server from bare metal, move it into service. Make sure you have your own set of &#8216;test cases&#8217; that ensure the provisioned state is the correct one. This will become important later on.</p>

<p>Take whatever server it&#8217;s replacing and do the same for the next role. When I came on board with my company I spent many useless cycles trying to retrofit an automation process on top of existing systems. In the end, I opted to take a few small victories (using Chef in this case):</p>

<ol>
<li>Create a base role that is non-destructive to existing configuration and systems. In my case, this was managing yum repos and user accounts.</li>
<li>Pick the &#8216;simplest&#8217; component in our infrastructure and start creating a role for it.</li>
<li>Spin up a new EC2 instance and test the role over and over until it works.</li>
<li>Terminate the instance and apply the role on top with a fresh one.</li>
<li>Replace the old instances providing that role with the new ones and move to the next role.</li>
</ol>


<p>Using this strategy, I was able to replace all of our legacy instances for the first and second tiers of our stack in a couple of months time. We are now at the point where, assuming Amazon plays nice with instance creation, we can have any role in those tiers recreated at a moment&#8217;s notice. Again, this will directly contribute to how we mitigate risk later on.</p>

<h2>10 minute maxim</h2>

<p>I came up with this from first principles so I&#8217;m sure there&#8217;s a better name for it. The idea is simply this:</p>

<blockquote><p>Any problem that has to be solved in five minutes can be afforded 10 minutes to think about the solution.</p></blockquote>

<p>System Administrators often pride ourselves on how cleverly and quickly we can solve a problem. It&#8217;s good for our egos. It&#8217;s not, however, good for our company. Take a little extra time and consider the longer term impact of what solution you&#8217;re about to do. Step away from the desk and move. Consult peers. Many times I&#8217;ve come to the conclusion that my first instinct was the right one. However more often than not, I&#8217;ve come across another solution that would create less technical debt for us to deal with later.</p>

<p>A correlary to this is the decision to &#8216;fix it or kick it&#8217;. That is &#8216;Do we spend an unpredictable amount of time trying to solve some obscure issue or do we simply recreate the instance providing the service from our above configuration management&#8217;. If you&#8217;ve gone through the previous step, you have should have amazing code confidence in your infrastructure. This is very important to have with Amazon EC2 where you can have an instance perform worse overtime thanks to the wonders of oversubscription and noisy neighbors.</p>

<p>Fuck that. Provision a new instance and run your smoke tests (I/O test for instance). If the smoke tests fail, throw it away and start a new one. It&#8217;s amazing the freedom of movement afforded by being able to describe your infrastructure as code.</p>

<h1>Getting back to deploys</h1>

<p>I would say that without the above, most of the stuff from here on out is pretty pointless. While you <strong>CAN</strong> do automated and non-offhour deploys without the above, you&#8217;re really setting yourself up for failure. Whether it&#8217;s a system change or new code, you need to be able to ensure that that some baseline criteria can be met. Now that we&#8217;ve got the foundation though, we can build on it and finally adopt some distinct strategies for releases.</p>

<h1>Building on the foundation</h1>

<p>The next areas you need to work on are a bit harder.</p>

<h2>Metrics and monitoring</h2>

<p>Shooting in the dark sucks. Without some sort of baseline metric, you authoritatively say whether or not a deploy was &#8216;good&#8217;. If it moves, graph it. If it moves, monitor it. You need to leverage systems like <a href="https://github.com/etsy/statsd">statsd</a> (available in non-node.js flavors as well) that can accept metrics easily from your application and make them availabile in the amazing <a href="http://graphite.wikidot.com/">graphite</a>.</p>

<p>The key here is that getting those metrics be as frictionless as possible. To fully understand this, watch <a href="http://pivotallabs.com/talks/139-metrics-metrics-everywhere">this presentation from Coda Hale of Yammer</a>. Coda has also created a kick-ass metrics library for the JVM and others have duplicated his efforts in their respective languages.</p>

<h2>Backwards compatibility</h2>

<p>You need to adopt a culture of backwards compatibility between releases. This is not Microsoft levels we&#8217;re talking about. This affects interim releases. As soon as you have upgraded all the components, you clean up the cruft and move on. This is critical to getting to zero/near-zero downtime deploys.</p>

<h2>Reduce interdependencies</h2>

<p>I won&#8217;t go into the whole SOA buzzword bingo game here except to say that treating your internal systems like a third party vendor can have some benefits. You don&#8217;t need to isolate the teams but you need to stop with shit like RMI. Have an established and versioned interface between your components. If component A needs to make a REST call to component B, upgrades to the B API should be versioned. A needs version 1 of B&#8217;s api. Meanwhile new component C can use version 2 of the API.</p>

<h2>Automation as a default</h2>

<p>While this ties a lot into the testing and configuration management topics, the real goal here is that you adopt a posture of automation by default. The reason for this should be clear in <a href="http://www.startuplessonslearned.com/2009/07/how-to-conduct-five-whys-root-cause.html">Eric Ries&#8217; &#8220;Five Whys&#8221; post</a>:</p>

<blockquote><p>Five Whys will often pierce the illusion of separate departments and discover the human problems that lurk beneath the surface of supposedly technical problems.</p></blockquote>

<p>One of the best ways to eliminate human problems is to take the human out of the problem. Machines are very good at doing things repeatedly and doing them the same way every single time. Humans are not good at this. Let the machines do it.</p>

<h1>Deploy Strategies</h1>

<p>Here are some of the key strategies that I (and others) have found effective for making deploys a non issue.</p>

<h2>Dark Launches</h2>

<p>The idea here is that for any new code path you insert in the system, you actually exercise it before it goes live. Let&#8217;s face it, you can never REALLY simulate production traffic. The only way to truly test if code is performant or not is to get it out there. With a dark launch, you&#8217;re still making new database calls but using your handy dandy metrics culture above, you now know how performant it really is. When it gets to acceptable levels, make it visible to the user.</p>

<h2>Feature flags</h2>

<p>Feature flags are amazing and one of the neat tricks that people who perform frequent deploys leverage. The idea is that you make aspects of your application into a series of toggles. In the event that some feature is causing issues, you can simply disable it through some admin panel or API call. Not only does this let you degrade gracefully but it also provides for a way to A/B test new features. With a bit more thought put into the process, you can enable a new feature for a subset of users. People love to feel special. Being a part of something like a &#8220;beta&#8221; channel is an awesome way to build advocates of your system.</p>

<h2>Smoke testing at startup</h2>

<p>This is one that I really like. The idea is simply that your application has a basic set of &#8216;tests&#8217; it runs through at startup. If any of those tests fail, the code is rolled back.</p>

<p>Now this is where someone will call me a hypocrite because I said you should and can never really roll back. You&#8217;re partially right. In my mind, however, it&#8217;s not the same thing. I consider code deployed once it&#8217;s taken production traffic. Up until that point, it&#8217;s just &#8216;pre-work&#8217; essentially. Let&#8217;s take a random API service in our stack. I&#8217;m assuming you have two API servers in this case.</p>

<ul>
<li>Take one out of service</li>
<li>Deploy code</li>
<li>Smoke tests run</li>
<li>If smoke tests fail, stop new code and start old code</li>
<li>If smoke tests pass, start sending production traffic to server</li>
<li>If acceptable, push to other server</li>
<li>profit!</li>
</ul>


<p>Now you might see a bit of gotcha there. I intentionally left out a step. This is a bit different than how shops like Wealthfront do it. They actually <strong>DO</strong> roll back if production monitoring fails. My preference is to use something similar to <a href="https://github.com/igrigorik/em-proxy">em-proxy</a> to do a sort of mini-dark launch before actually turning it over to end-users. You don&#8217;t have to actually use em-proxy. You could write your own or use something like RabbitMQ or other messaging system. This doesn&#8217;t always work depending on the service the component is providing but it does provide another level of &#8216;comfort&#8217;.</p>

<p>Of course this only works if you maintain backwards compatibility.</p>

<h2>Backwards Compatibility</h2>

<p>This is probably the hardest of all to accomplish. You may be limited by your technology stack or even some poor decision made years ago. Backwards compatibility also applies to more than just your software stack. This is pretty much a critical component of managing database changes with zero downtime.</p>

<h2>Code related</h2>

<p>Your code needs to understand &#8216;versions&#8217; of what it needs. If you leverage some internal API, you need to maintain support for an older version of that API until all users are upgrade. Always be deprecating and NEVER EVER redefine what something means. Don&#8217;t change a property or setting that means &#8220;This is my database server hostname&#8221; to &#8220;This is my mail server hostname&#8221;. Instead create a new property, start using it and remove the old on in a future release. Don&#8217;t laugh, I&#8217;ve seen this done. As much as I have frustrations with Java, constructor overloading is a good example of backwards compatibility.</p>

<h3>Database related</h3>

<p>Specifically as it relates to databases, consider some of the following approaches:</p>

<ul>
<li>Never perform backwards incompatible schema changes.</li>
<li>Don&#8217;t perform ALTERs on really large tables. Create a new table that updated systems use and copy on read to the new table. Migrate older records in the background.</li>
<li>Consider isolating access to a given table via a service. Instead of giving all your applications access to the &#8216;users&#8217; table, create a users service that does that.</li>
<li>Start exercising code paths to new tables early by leveraging dark launches</li>
</ul>


<p>Some of these techniques would make Codd spin in his grave.</p>

<p>We&#8217;re undergoing a similar situation right now. We originally stored a large amount of &#8216;blob&#8217; data in Voldemort. This was a bit perplexing as we were already leveraging S3 for similar data. To migrate that data (several hundred gigs) we took the following approach:</p>

<ul>
<li>Deploy a minor release that writes and new data to both Voldemort and S3.</li>
<li>Start a &#8216;copy&#8217; job in the background to migrate older data</li>
<li>Continue to migrate data</li>
<li>When the migration is finished, we&#8217;ll deploy a new release that uses S3 exclusively</li>
<li>Profit (because we get to terminate a few m1.large EC2 instances)</li>
</ul>


<p>This worked really well in this scenario. These aren&#8217;t new techniques either. Essentially, we&#8217;re doing a variation of a two-phase commit.</p>

<p>Now you might think that all this backwards compatibility creates cruft. It does. Again, this is something that requires a cultural shift. When things are no longer needed, you need to clean up the code. This prevents bloat and makes understanding it down the road so much easier.</p>

<h1>Swinging like a boss</h1>

<p>Here&#8217;s another real world example:</p>

<p>Our code base originally used a home-rolled load balancing technique to communicate with one of our internal services. Additionally, all communication happened over RPC using Hessian. Eventually this became untenable and we decided to move to RabbitMQ and JSON. This was a pretty major change but at face value, we should have been able to manage with dual interfaces on the provider of the service. That didn&#8217;t happen.</p>

<p>You see, to be able to use the RabbitMQ libraries, we had to upgrade our version of Spring. Again, not a big deal. However our version of Hessian was so old that the version of Hessian we would have to use with the new version of Spring was backwards incompatible. This is yak shaving at its finest, folks. So basically we had to upgrade 5 different components all at once just to get to where we wanted and NEEDED to be for the long term.</p>

<p>Since I had already finished coding our chef cookbooks, we went down the path of duplicating our entire front-end stack. What made this even remotely possible was the fact that we were using configuration management in the first place. Here&#8217;s how it went down:</p>

<ul>
<li>Duplicate the various components in a new Chef environment called &#8216;prodB&#8217;</li>
<li>Push new code to these new components</li>
<li>Add the new components to the ELBs and internal load balancers for a very short 5-10 minute window. Sort of a mini-A/B test.</li>
<li>Check the logs for anything that stood out. Validated the expected behavior of the new systems. Thsi also gave us a chance to &#8216;load-test&#8217; our rabbitmq setup. We actually did catch a few small bugs this way.</li>
</ul>


<p>Once we were sure that things looked good, we swung all the traffic to the new instances and pulled the old ones out. We never even bothered to upgrade the old instances. We just shut them down.</p>

<p>Obviously this technique doesn&#8217;t work for everyone. If you&#8217;re using physical hardware, it&#8217;s much more costly and harder to pull off. Even internally, however, you can leverage virtualization to make these kinds of things possible.</p>

<h2>Bitrot</h2>

<p>What should be the real story in this is that bitrot happens. Don&#8217;t slack on keeping third-party libraries current. If a third-party library introduces a breaking change and it affects more than one part of your stack, you probably have a bit too tight of a coupling between resources.</p>

<h1>Wrap up/Take away</h1>

<p>This post came out longer than I had planned. I hope it&#8217;s provided you with some information and things to consider. Companies of all shapes, markets and sizes are doing continuous deployment, zero downtime deploys and all sorts of things that we never considered possible. Look at companies like Wealthfront, IMVU, Flickr and Etsy. Google around for phrases like &#8216;continuous deployment&#8217; and &#8216;continuous delivery&#8217;.</p>

<p>I&#8217;m also painfully aware that even with these tricks, some folks simply cannot do them. There are many segments of industry that might not even allow for this. That doesn&#8217;t mean that some of these ideas can&#8217;t be implemented on a smaller scale.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rollbacks and other deployment myths]]></title>
    <link href="http://lusis.github.com/blog/2011/10/18/rollbacks-and-other-deployment-myths/"/>
    <updated>2011-10-18T00:35:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/10/18/rollbacks-and-other-deployment-myths</id>
    <content type="html"><![CDATA[<p>I came across an interesting post today via HN. I&#8217;m surprised (only moderately) that I missed it the first time around since this is right up my alley:</p>

<p><a href="http://briancrescimanno.com/2011/09/29/why-are-you-still-deploying-overnight/">Why are you still deploying overnight?</a></p>

<p>I thought this post was particularly apropos for several reasons. I just got back from DevOpsDays EU <strong>AND</strong> I&#8217;m currently in the process of refactoring our deploy process.</p>

<p>I&#8217;m breaking this up into two parts since it&#8217;s a big topic. The first one will cover the more &#8220;theoretical&#8221; aspects of the issue while the second will provide more concrete information.</p>

<!--more-->


<h1>Myths, Lies and other bullshit</h1>

<p>Before I go any further, we should probably clear up a few things.</p>

<p>Understand, first and foremost, that I&#8217;m no spring chicken in this business. I&#8217;ve worked in what we now call web operations and I&#8217;ve worked in traditional financial environments (multiple places). If it CAN go wrong, it has gone wrong for me. Shit, I&#8217;ve been the guy who dictated that we had to deploy after hours.</p>

<p>Also, this is not a &#8220;tell you what to do&#8221; post.</p>

<p>So what are some of the myths and other crap people like to pull out when having these discussions?</p>

<ul>
<li>Change == Risk</li>
<li>Deploys are risky</li>
<li>Rollbacks</li>
<li>Nothing fails</li>
<li>SLAs</li>
</ul>


<p>There&#8217;s plenty more but these are some of the key ones that I hear.</p>

<h2>Change is change</h2>

<p>There is nothing inherent in change that makes it risky, dangerous or anything more than change. Change is neither good or bad. It&#8217;s just change.</p>

<p>The idea that change has a risk associated with it is entirely a human construct. We have this false assumption that if we don&#8217;t change something then nothing can go wrong.
At first blush that would make sense, right? If it ain&#8217;t broke, don&#8217;t fix it.</p>

<p>Why do we think this? It&#8217;s mainly because we&#8217;re captives to our own fears. We changed something once, somewhere, and everything went tango uniform. The first reaction after a bad experience is never to do whatever caused that bad experience again. This makes sense in quite a few cases. Touch fire, get burned. Don&#8217;t touch fire, don&#8217;t get burned!</p>

<p>However this pain response tends to bleed over into areas. We deployed code one time that took the site down. We changed something and bad things happened. Engage overcompensation - We should never change anything.</p>

<h2>Deploys are not risky</h2>

<p>As with change, a deploy (a change in and of itself) is not inherently risky. Is there a risk associated with a deploy? Yes but understand that the risk associated with pushing out new code is the culmination of everything you&#8217;ve done up to that point.</p>

<p>I can&#8217;t even begin to count the number of ways that a deploy or release has gone wrong for me. Configuration settings were missed. Code didn&#8217;t run properly. The wrong code was deployed. You name it, I&#8217;ve probably seen it.</p>

<p>The correct response to this is <strong>NOT</strong> to stop doing deploys, do them off-hours or do them less often. Again with the overcompensation.</p>

<p>The correct way to handle deployment problems is to do MORE deploys. Practice. Paraphrasing myself here from an HN comment:</p>

<blockquote><p>Make deploys trivial, automated and tolerant to failure because everything fails.</p></blockquote>

<p>&#8220;Release early, release often&#8221; isn&#8217;t just about time to market. The way to reduce risk is not to add more risky behavior (introducing more vectors for shit to go wrong). The way to reduce the risk associated with deploys is to break them into smaller chunks.</p>

<p>You need to stop thinking like Subversion and start thinking like Git.</p>

<p>One of the reasons people don&#8217;t feel comfortable performing deploys during the day is because deploys are such a big deal. You&#8217;ve got to make deploys a non-issue.</p>

<h2>Rollbacks are a myth</h2>

<p>Yes, it&#8217;s true. You can never roll back. You can&#8217;t go back in time. You can fake it but understand that it&#8217;s typically more risky to rollback than rolling forward. Always be rolling forward.</p>

<p>The difficulty in rolling forward is that it requires a shift in how you think. You need to create a culture and environment that enables, encourages and allows for small and frequent changes.</p>

<h2>Everything fails. Embrace failure.</h2>

<p>It amazes me that in this day and age people seem to think you can prevent failure. Not only can you not prevent it, you should embrace it. Learn to accept that failure will happen.  Often spending your effort decreasing MTTR (mean time to recovery) as opposed to increasing MTBF (mean time between failures) is a much better investment. Failure is not a question of &#8216;if&#8217; but a question of &#8216;when&#8217;.</p>

<p>Systems should be designed to be tolerant of failure. This is not easy, it&#8217;s not always cheap and it can be quite painful at first. Failure sucks. Especially as systems administrators, we tend to personalize a failure in our systems as a personal failure.</p>

<p>The best way to deal with failure is to make failure a non-issue. If it&#8217;s going to happen and you can&#8217;t prevent it, why stress over trying to prevent it? This absolutely doesn&#8217;t mean that you should do some level of due dilligence. I&#8217;m not saying that you should give up. What I&#8217;m saying is that you should design a robust system that handles failures gracefully and returns you to service as quickly as possible. It&#8217;s called fault TOLERANCE for a reason.</p>

<h2>SLAs are not about servers</h2>

<p>SLAs are in general fairly silly things. Before you get all twisted and ranty, let me clarify. SLAs have value but the majority of that value is to the provider of the SLA and not the person on the other end. SLAs are a lot like backup policies.</p>

<p>Look at it this way. I&#8217;m giving you an SLA for four nines of availability. That allows me to take around 50 minutes of downtime a year. Of course you assume that means 50 minutes spread over a year. What you fail to realize is that I can take all 50 minutes at once and still meet my SLA. Taking 50 minutes at one time is much more impacting than taking ten 5-minute outages. What&#8217;s worse is I can take that downtime not only in one chunk but I can take it at the worst possible time for you.</p>

<p>The other side of SLAs is that we tend to equate them with servers as opposed to services. The SLA is a <em>Service Level Agreement</em>. Not a <em>Server Level Agreement</em>. Services are what matters, not servers.</p>

<p>When you start to equate an SLA with a specific server, you&#8217;ve already lost.</p>

<h1>Wrap up and part 2</h1>

<p>As I said, this topic is too big to fit in one post. The next post will go into specifics about strategies and techniques that will hopefully give you ideas on how to make deploys less painful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Configuration Management Divide]]></title>
    <link href="http://lusis.github.com/blog/2011/08/22/the-configuration-management-divide/"/>
    <updated>2011-08-22T22:07:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/08/22/the-configuration-management-divide</id>
    <content type="html"><![CDATA[<p><em>wherein John admits he was wrong about something</em></p>

<p>As I was checking my Github feed tonight (as I&#8217;ve been known to do on occassion), I saw an update for a project that I was watching. This project is, for all intents and purposes, a redo of Capistrano with some additional system management stuff on top. From the description:</p>

<blockquote><p>All together mixed to make your life easier.
As mentioned before do is a fun mix of capistrano, rake, thor and brew.
With XXXXX you are be able to do easily common task on your local or remote machine. The aim of XXXXX is to become the unique and universal tool language that permit you to make your own brew, apt-get or yum package, same syntax for all your machine.</p></blockquote>

<p>Now I&#8217;ve said before I&#8217;m a tool junkie. I&#8217;m always looking for ideas and inspiration from other tools. I also love to contribute where I can.
What bothered me particularly about this project is that it felt like a &#8220;solved problem&#8221;.</p>

<!-- more -->


<h2>Open mouth, insert foot (at least partially)</h2>

<p>So at DevOps Days Mt. View this year, I made a statement. I said with quite a bit of firmness that I thought configuration management was a solved problem. I didn&#8217;t get a chance to clarify what I meant by that. At least not before <a href="http://twitter.com/littleidea">Andrew Clay Shafer</a> strongly disagreed with me. Before I go any further, I want to clarify what I meant by that.</p>

<p>I, personally, feel like there comes a point where a class of tool reaches a certain level that makes it &#8220;good enough&#8221;. So what is good enough?
By &#8220;good enough&#8221;, I mean that it solves the first in a broader series of problems. By no means am I implying that we should no longer pursue improvement. But what I am suggesting is that at a certain point, tools are refining the first stage of what they do.</p>

<p>When we get to this point in a class of tool (let&#8217;s stick with CM), we start to see gaps in what it can do. At this point, you get a rush of tools that attempt to fill that gap. What we essentially do is try and figure out if the gap should be filled by the tool or does it make sense to exist apart. So taking the discussion I was involved in at the time - orchestration, what I was attempting to say (epic fail, I might add) is that the current crop of configuration management tools have reached a usable point where they do enough (for now). What we&#8217;re seeing as questions now are &#8220;How do I think beyond the single node where this tool is running?&#8221;. What we might find is that functionality DOES belong in our CM tool. We might also find that, no, we need this to exist apart from it for whatever reasons. Basically I was attempting to say &#8220;Hey, CM is in a pretty good state right now. Let&#8217;s tackle these OTHER problems and regroup later&#8221;.</p>

<p>So where was I <a href="http://youtu.be/V3y3QoFnqZc">wrong</a>?</p>

<h2>Developers, developers, developers</h2>

<p>I made the following comment on Twitter earlier:</p>

<blockquote><p>The sheer number of what are essentially Capistrano clones makes me realize that the config. management message fails to resonate somewhere.</p></blockquote>

<p>What I find most interesting about these various &#8220;clones&#8221; is that they&#8217;re all created by developers. People apparently pine for the days of Capistrano. Come to think of it though, who blames them?</p>

<p>As I look at the current crop of configuration management tools, and follow various tweets and blog posts I realize that people either aren&#8217;t aware of tools like puppet and chef or, in the worst cases, they feel that they&#8217;re too much hassle/too complicated to deal with. What we&#8217;re seeing is the developer community starting to come to the same realizations that the sysadmin community came to a while ago. What&#8217;s also interesting is that the sysadmin community is coming to the same realizations that developers came to a while ago.</p>

<p>It&#8217;s like some crazy romantic movie where the two lovebirds, destined to be together in the end have the pivotal mix-up scene. He leaves the bar to stop her from getting on the airplane. Meanwhile she heads to the bar upon realizing that she can&#8217;t get on the plane without him.</p>

<p>People are going to great lengths, either out of ignorance* about available tools or from frustration about the complexities involved.</p>

<h2>So where&#8217;s the divide?</h2>

<p>I think the divide is in the message. While I still stand behind the statement that the &#8220;dev&#8221; vs &#8220;ops&#8221; ratios are regionally specific, I&#8217;m starting to agree that the current crop of tools <strong>ARE</strong> very operationally focused. That&#8217;s not a bad thing either. Remember that the tools were created by people who were primarily by sysadmins.</p>

<p>When you look at the tools created by people who were primarily developers, what do you see?</p>

<ul>
<li>Apache Whirr</li>
<li>Do</li>
<li>Fabric</li>
<li>Capistrano</li>
<li>Glu</li>
<li>DeployML</li>
</ul>


<p>What&#8217;s interesting about all of these tools is that they&#8217;re designed to get the code out there as easily as possible or to stand up a stack as quickly as possible. What&#8217;s frustrating for me, as primariy a sysadmin, is that I see the pain points that will come down the road. These tools don&#8217;t really scale. Yes, sometimes you need &#8220;ssh in a for loop&#8221; however inherting these types of environments can be painful. Concepts like idemopotence and repeatability don&#8217;t exist. They generally don&#8217;t take into account the non-functional requirements. They certainly don&#8217;t handle multiple operating environments or distributions very well.</p>

<p>What&#8217;s really sad is that the end goal is the same. We just want to automate away the annoying parts of our jobs so we can get on to more important shit.</p>

<h3>You arrogant prick</h3>

<p>I figured I&#8217;d get that out of the way now. It&#8217;s very easy to dismiss these concerns as some pissy sysadmin but I don&#8217;t think that&#8217;s entirely fair. Remember what I said earlier. Sysadmins are starting to come to realizations that developers had a long time ago. The same exists for developers. Both sides, regardless of primary role, still have much to learn from each other.</p>

<h2>What do we do?</h2>

<p>I&#8217;m going to share a fairly generic dirty little secret. Developers don&#8217;t want to write puppet manifests/modules. They don&#8217;t want to write chef cookbooks/recipes. In most every situation I&#8217;ve been in, there has been little interest in those tools from outside the sysadmin community. That&#8217;s changing. I think as people are getting introduced to the power they have, they come around. But there&#8217;s still a gap. Is the gap around deployment? Partially. I think the gap also exists around configuration. Obviously I&#8217;m biased in that statement.</p>

<p>A bit of a personal story. At my previous company, we were a puppet shop. We tried to get developers to contribute to the modules. It didn&#8217;t work. It just didn&#8217;t make sense. It&#8217;s not that they were stupid. It&#8217;s that it was an extra step. &#8220;Oh you mean I have to put that variable over here instead of in my configuration file? That&#8217;s annoying because I have to be able to test locally&#8221;. What ended up happening was that one of the guys on the operations side learned just enough Java to write lookup classes himself. Instead of us populating hosts using ERB templates, we were now querying Cobbler for classes using XML-RPC. There were some other factors at play, mind you. It was a really large company with developers who came from the silo&#8217;d worldview. There was also quite a bit of ass-covering attitude running around.</p>

<p>This is why I&#8217;m so bullish on bridge tools like Noah and even Vagrant. We&#8217;re all attempting to do the same thing. Bridge that last little bit between the two groups. Find a common ground. Noah is attempting to bridge that by allowing the information to be &#8220;shared&#8221; between teams. ZooKeeper let&#8217;s the developers manage it all themselves (just push this app out, we&#8217;ll find each other). Vagrant takes the route of bringing the world of production down to the developer desktop.</p>

<p>But none of these tools address the deploy issue. Do we want to use the Opscode deploy cookbook? We can but it&#8217;s fairly opinionated**. Maybe we decide to use FPM to generate native system packages and roll those out with a Puppet run? Putting on my fairly small developer hat, none of these sounds really appealing to me. Maybe as the java developer, I just want to use JRebel. That&#8217;s nice but what about the configuration elements? Do we now write code to ask Puppet or Chef for a list of nodes with a given class or role?</p>

<p>Look at what Netflix does (not that I approve really). They roll entire fucking AMIs for releases. While the sysadmin in me is choking back the bile at the inflexibility of that, stepping into another perspective says &#8220;It works and I don&#8217;t have to deal with another server just to manage my systems&#8221;.</p>

<h2>I don&#8217;t have all the answers</h2>

<p>That much is obvious. What I can say is that I see a need. I&#8217;m addressing it the way I think is best but I still see gaps. Shared configuration is still complicated. Deploy is still complicated. While I would generally agree that things like rolling back are the wrong approach, I also realize that not everyone is at the point where they can roll-forward through a bad deploy. Hell, we do all of our deploys with Jenkins and some really ugly bash scripts and knife scripts.</p>

<p>I&#8217;d love any commentary folks might have. I&#8217;ll leave you with a few comments from other&#8217;s on twitter when I made my original comment:</p>

<blockquote><p>Capistrano serves a different need. Rapid deployment with rollback isn&#8217;t implicit in the current crop config mgmt tools - <a href="https://twitter.com/altobey">@altobey</a></p>

<p>deploying code with a CM tool (eg, the chef <code>deploy</code> resource) is a grey area for me. for example: does rollback really fit? - <a href="https://twitter.com/dpiddee">@dpiddee</a></p>

<p>or that it might be missing some use case? - <a href="https://twitter.com/jordansissel">@jordansissel</a></p>

<p>I&#8217;ve played with using deployml in its local only mode kicking off deploys with mcollective, worked really nice - <a href="https://twitter.com/ripienaar">@ripienaar</a></p>

<p>Roll forward, never back. - <a href="https://twitter.com/bdha">@bdha</a></p>

<p>This is a fight I have at least two or three times at every conference I speak at and in a bucket load of other places - <a href="https://twitter.com/kartar">@kartar</a></p>

<p>further thought: I consider capistrano to be one impl of a &#8220;deployment service&#8221; on the same level as, say, a CM tool  - <a href="https://twitter.com/dpiddee">@dpiddee</a></p></blockquote>

<ul>
<li>&#8220;Ignorance&#8221;: I don&#8217;t mean ignorance in the sense of stupidity. I mean it strictly in the sense of not having knowledge of something. No judgement is implied</li>
<li>&#8220;Opinionated&#8221; - Opinionated isn&#8217;t bad. It&#8217;s just opinionated.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Migrating From Blogger]]></title>
    <link href="http://lusis.github.com/blog/2011/08/13/migrating-from-blogger/"/>
    <updated>2011-08-13T21:50:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/08/13/migrating-from-blogger</id>
    <content type="html"><![CDATA[<p>Many, many years ago I started blogging. It&#8217;s fun and I like to feel self-important by dumping my ideas. I enjoy writing so blogging is cool like that.</p>

<p>Over the past 12 years I&#8217;ve gone through various tools to help me do that (in no particular order):</p>

<ul>
<li>PHPnuke</li>
<li>Postnuke</li>
<li>Moveable Type</li>
<li>Plone</li>
<li>CakePHP</li>
<li>rst2html vim plugin</li>
<li>Blogger</li>
<li>Wordpress</li>
<li>Homegrown shit</li>
</ul>


<p>With the exception of one, all of these tools were simply a pain in the ass. They required lots of moving pieces, we&#8217;re bug ridden and security nightmares.
The one tool I enjoyed the most?</p>

<p>rst2html in Vim</p>

<p>The only reason I used those tools was because I hated writing HTML. I hated dealing with styling. About 3 years ago I finally settled on using Blogger. Now I&#8217;m changing again.</p>

<!--more-->


<h1>Blogger and Markdown</h1>

<p>While I was perfectly happy with Blogger, one thing that kept annoying me was that writing posts became tedious. Working inside a textarea widget was still painful. I used Scribefire for a while in Firefox. For the last 4 months or so, I&#8217;ve been writing my blog posts in Markdown, manually converting them to HTML with pandoc and then pasting them into blogger.</p>

<p>I tried the google CLI tools but I still had to massage the generated output so I never bothered to automate any of the posting part. Additionally, as much more of posts became technical, I found myself jumping over to gists and pasting the embedded content link into the posts. The whole workflow sucked.</p>

<h1>Octopress and Github</h1>

<p>The other day I came across <a href="https://github.com/imathis/octopress">Octopress</a>. The default style was attractive. It handled code VERY well. Best of all, I could work in Vim, run a few rake commands and my content would be published to Github.</p>

<p>WIN!</p>

<p>I find this workflow MUCH easier and sane. It&#8217;s very coder friendly.</p>

<p>What&#8217;s also nice is that, should github go away (god forbid), I have everything here self-contained to run on my own server.</p>

<h1>Old posts</h1>

<p>I&#8217;m currently in the process of redoing what blogger posts I have that weren&#8217;t already in Markdown. I&#8217;m converting the more &#8220;popular&#8221; posts first and then I&#8217;ll get the remainder. It&#8217;s a little painful but the light at the end of the tunnel is that I only have to do this once.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fun With Celluloid]]></title>
    <link href="http://lusis.github.com/blog/2011/08/13/fun-with-celluloid/"/>
    <updated>2011-08-13T00:30:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/08/13/fun-with-celluloid</id>
    <content type="html"><![CDATA[<p><em>warning! This is a really long post</em></p>

<p>In the course of rewriting the <a href="https://github.com/lusis/Noah">Noah</a> callback daemon, I started to get really frustrated with EventMachine. This is nothing against EventMachine by any stretch of the imagination. I really like it.</p>

<p>What I was having issues with is making the plugin framework as dirt simple as possible. By using EM, I had no choice but to require folks to understand how EM works. This primarily meant not blocking the reactor. Additionally, through no fault of EM, I was starting to get mired in callback spaghetti.</p>

<h1>Actors</h1>

<p>I&#8217;ve mentioned several times before that I love the actor model. It makes sense to me. The idea of mailboxes and message passing is really simple to understand. For a while, there was project that implemented actors on top of EM called Revactor but it stalled. I started following the author (Tony Arcieri) on GitHub to see if he would ever update it. He did not but I caught wind of his new project and it was pretty much exactly what I was looking for.</p>

<p>Actors have a proven track record in Erlang and the Akka framework for Scala and Java uses them as well.</p>

<h1>Celluloid</h1>

<!--more-->


<p><a href="https://github.com/tarcieri/celluloid">Celluloid</a> is an implementation of Actors on Ruby. At this point, it lacks some of the more advanced features of the Akka and Erlang implementations. However Tony is very bullish about Celluloid and is pretty awesome in general.</p>

<p>I&#8217;m not going to go over Celluloid basics in too much detail. Tony does an awesome job in the <a href="http://celluloid.github.com/">README</a> for the project. What I want to talk more about is how I want to use it for Noah and what capabilities it has/is missing for that use case.</p>

<h1>Noah callbacks</h1>

<p>I won&#8217;t bore you with a rehash of Noah. I&#8217;ve written a ton of blog posts (and plan to write more). However for this discussion, it&#8217;s important to understand what Noah callbacks need to do.</p>

<h2>Quick recap</h2>

<p>Any object in Noah can be &#8220;watched&#8221;. This is directly inspired by ZooKeeper. Because Noah is stateless, however, watches need to work a little differently. The primary difference is that Noah&#8217;s watches are asynch. As a side-effect of that, we get some really cool additional functionality. So what does a Noah watch consist of?</p>

<ul>
<li>An absolute or partial path to and endpoint in the system</li>
<li>A URI-style location for notification of changes to that path</li>
</ul>


<p>Let&#8217;s say you had a small sinatra application running on all your servers. Its only job was to be a listener for messages from Noah. This daemon will be responsible for rewriting your <code>hosts</code> file with any hosts that are created, modified or deleted on your network.</p>

<p>In this case, you might register your watch with a path of <code>/hosts/</code> and an endpoint of <code>http://machinename:port/update_hosts</code>. Any time a host object is created, updated or deleted Noah will send the JSON representation of that object state along with the operation performed to that endpoint. Let&#8217;s say you also want to know about some configuration setting that has changed which lives at <code>/configurations/my_config_file.ini</code>. Let&#8217;s put a kink in that. You want that watch to drop its message onto a RabbitMQ exchange.</p>

<p>So now we have the following information that we need to act on:</p>

<ul>
<li><code>{:endpoint =&gt; 'http://machine:port/update_hosts', :pattern =&gt; '//noah/hosts'}</code></li>
<li><code>{:endpoint =&gt; 'amqp://host:port/exchange?durable=true', :pattern =&gt; '//noah/configurations/my_config_file.ini'}</code></li>
</ul>


<p>Not so hard right? But we also have some additional moving parts. Something needs to monitor Redis for these various CRUD messages. We need to compare them against a list of endpoints that want notification about those messages. We also need to intercept any messages from Redis that are new endpoints being registered. Oh and we also need to know about failed endpoints so we can track and eventually evict them. Obviously we don&#8217;t want to stop http messages from going out because AMQP is slow. Imagine if we implemented FTP endpoint support! Essentially we need high concurrency not only on each &#8216;class&#8217; of endpoint (http, amqp, ftp whatever) but also within each class of endpoint. If any individual endpoint attempt crashes for any reason, we need to take some action (eviction for instance) and not impact anyone else.</p>

<h1>Doing it with Celluloid</h1>

<p>So thinking about how we would do this with actors, I came up with the following basic actors:</p>

<ul>
<li>RedisActor <em>watches the Redis pubsub backend</em></li>
<li>HTTPActor <em>handles HTTP endpoints - a &#8216;worker&#8217;</em></li>
<li>AMQPActor <em>handles AMQP endpoints - a &#8216;worker&#8217;</em></li>
<li>BrokerActor <em>responsible for intercepting endpoint CRUD operations and also determining which actors to send messages to for processing</em></li>
</ul>


<p>As I said previously, we also need to ensure that if any worker crashes, that it gets replaced. Otherwise we would eventually lose all of our workers.</p>

<p>With this information, we can start to build a tree that looks something like this:</p>

<pre><code>- Master process
    |_Redis
    |_Broker
    |_HTTPPool
    |    |_Worker
    |    |_Worker
    |_AMQPPool
        |_Worker
        |_Worker
</code></pre>

<p>The master process is responsible for handling the Redis, Broker and Pool actors. Each pool actor is responsible for its workers. Not really visible in the ASCII above is how messages flow:</p>

<ul>
<li>Master process spawns Redis, Broker, HTTPPool and AMQPPool as supervised processes.</li>
<li>Each pool type spins up a set of supervised workers.</li>
<li>Master process makes an HTTP request to the Noah server for all existing watches (synchronous)</li>
<li>It sends a message with those watches to the Broker so it can build its initial list.(synchronous)</li>
<li>Redis actor watches pubsub.</li>
<li>Watch messages are sent to a mailbox on the Broker. (synchronous)</li>
<li>The rest to a different mailbox on the broker.</li>
<li>The broker performs some filtering to determine if any registered watches care about the message. If so, those are sent to the appropriate pool. (async)</li>
<li>Each Pool selects a worker and tells him the endpoint and the message</li>
<li>The worker delivers the message</li>
</ul>


<p>Where this became a slight problem with Celluloid is that it lacks two bits of functionality currently:</p>

<ul>
<li>Supervision trees</li>
<li>Pool primitives</li>
</ul>


<p>Right now in Celluloid, there is no way to build &#8220;pools&#8221; of supervised processes. The supervised part is important. If a process is supervised, crashes will be trapped and the process will be restarted.</p>

<p>So how did we &#8220;fake&#8221; this with the existing functionality?</p>

<p>The generic tree was fairly easy. The main Ruby process creates supervised processes for each actor:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">class</span> <span class="nc">RedisActor</span>
</span><span class='line'>  <span class="kp">include</span> <span class="no">Celluloid</span><span class="o">::</span><span class="no">Actor</span>
</span><span class='line'>  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
</span><span class='line'>    <span class="vi">@name</span> <span class="o">=</span> <span class="nb">name</span>
</span><span class='line'>    <span class="n">log</span><span class="o">.</span><span class="n">info</span> <span class="s2">&quot;starting redis actor&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="nf">start</span>
</span><span class='line'>   <span class="c1"># start watching redis</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'><span class="k">class</span> <span class="nc">BrokerActor</span>
</span><span class='line'>  <span class="kp">include</span> <span class="no">Celluloid</span><span class="o">::</span><span class="no">Actor</span>
</span><span class='line'>  <span class="c1"># constructor</span>
</span><span class='line'>  <span class="k">def</span> <span class="nf">process_watch</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span class='line'>    <span class="c1">#...</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>  <span class="k">def</span> <span class="nf">do_work</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span class='line'>    <span class="c1">#...</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">HTTPPool</span>
</span><span class='line'>  <span class="c1"># you get the idea</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'>
</span><span class='line'><span class="vi">@http_pool</span> <span class="o">=</span> <span class="no">HTTPPool</span><span class="o">.</span><span class="n">supervise_as</span> <span class="ss">:http_pool</span><span class="p">,</span> <span class="s2">&quot;http_pool&quot;</span>
</span><span class='line'><span class="vi">@broker_actor</span> <span class="o">=</span> <span class="no">BrokerActor</span><span class="o">.</span><span class="n">supervise_as</span> <span class="ss">:broker_actor</span><span class="p">,</span> <span class="s2">&quot;broker&quot;</span>
</span><span class='line'><span class="vi">@redis_actor</span> <span class="o">=</span> <span class="no">RedisActor</span><span class="o">.</span><span class="n">supervise_as</span> <span class="ss">:redis_actor</span><span class="p">,</span> <span class="s2">&quot;redis&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The workers were a bit more complicated. What I ended up doing was something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">class</span> <span class="nc">HTTPWorker</span>
</span><span class='line'>  <span class="kp">include</span> <span class="no">Celluloid</span><span class="o">::</span><span class="no">Actor</span>
</span><span class='line'>
</span><span class='line'>  <span class="kp">attr_reader</span> <span class="ss">:name</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
</span><span class='line'>    <span class="vi">@name</span> <span class="o">=</span> <span class="nb">name</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>  <span class="k">def</span> <span class="nf">do_work</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
</span><span class='line'>    <span class="c1"># Work to send the message</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'>
</span><span class='line'><span class="k">class</span> <span class="nc">HTTPPool</span>
</span><span class='line'>  <span class="kp">include</span> <span class="no">Celluloid</span><span class="o">::</span><span class="no">Actor</span>
</span><span class='line'>  <span class="no">WORKERS</span> <span class="o">=</span> <span class="mi">10</span>
</span><span class='line'>
</span><span class='line'>  <span class="kp">attr_reader</span> <span class="ss">:workers</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
</span><span class='line'>    <span class="vi">@name</span> <span class="o">=</span> <span class="nb">name</span>
</span><span class='line'>    <span class="vi">@workers</span> <span class="o">=</span> <span class="o">[]</span>
</span><span class='line'>    <span class="no">WORKERS</span><span class="o">.</span><span class="n">times</span> <span class="k">do</span> <span class="o">|</span><span class="nb">id</span><span class="o">|</span>
</span><span class='line'>      <span class="vi">@workers</span><span class="o">[</span><span class="nb">id</span><span class="o">]</span> <span class="o">=</span> <span class="no">HTTPWorker</span><span class="o">.</span><span class="n">supervise_as</span> <span class="s2">&quot;http_worker_</span><span class="si">#{</span><span class="nb">id</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">to_sym</span><span class="p">,</span> <span class="s2">&quot;http_worker_</span><span class="si">#{</span><span class="nb">id</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>  <span class="k">def</span> <span class="nf">do_work</span>
</span><span class='line'>    <span class="vi">@workers</span><span class="o">.</span><span class="n">sample</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">do_work</span> <span class="s2">&quot;msg&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>The problem as it stands is that we can&#8217;t really have &#8220;anonymous&#8221; supervised processes. Each actor that&#8217;s created goes into Celluloid&#8217;s registry. We need a programatic way to look those up so we use <code>supervise\_as</code> to give them a name.</p>

<p>This gives us our worker pool. Now Redis can shovel messages to the broker who filters them. He sends a unit of work to a Pool which then selects a random worker to do the REAL work. Should any actor crash, he will be restarted. Because each actor is isolated, A crash in talking to redis, doesn&#8217;t cause our existing workers to stop sending messages.</p>

<p>Obviously this a fairly naive implementation. We&#8217;re missing some really important functionality here.</p>

<ul>
<li>detecting busy workers</li>
<li>detecting dead workers (yes we still need to do this)</li>
<li>alternate worker selection mechanisms (cyclical for instance)</li>
<li>crash handling</li>
<li>backlog handling</li>
</ul>


<p>You might wonder why we care if a worker is dead or not? Currently Celluloid buffers messages in each actor until the can be dealt with. In the case of our Pool, it will select a worker and buffer any messages if the worker is blocked. If our worker crashes on its current unit of work, it returns control to the pool. The pool then attempts to send the worker the next message but the worker is dead and hasn&#8217;t respawned yet.</p>

<h1>Some code to play with</h1>

<p>Yes, we&#8217;ve finally made it to the end.</p>

<p>I&#8217;ve created a fun little sinatra application to make it easier for me to test my pools. It consists of a generic Pool class that can be subclassed and take a the name of a worker class as an argument. When a worker gets a message of &#8220;die&#8221;, it will raise an exception thus simulating a crash. Additionally, the &#8220;message processing&#8221; logic includes sleep to simulate long running work.</p>

<p>The reason Sinatra is in the mix is to provide an easy way for me to fire off simulated requests to the pool so I can experiment with different approaches. Eventually, Celluloid will have a proper Pool construct. I plan on using this as the basis for a pull request. You can see it here. Please feel free to fork and experiment with me. It&#8217;s really fun.</p>

<div><script src='https://gist.github.com/1143369.js?file='></script>
<noscript><pre><code>require 'celluloid'
require 'logger'
require 'uuid'
require 'sinatra/base'

# This is just a simple demo of a possible Pool implementation for Celluloid
# The sinatra interface exists just to do some testing of crashing workers and the like

# TODO
# Create a busy worker registry of some kind
# Implement a small stats page

LOGGER = Logger.new(STDOUT)
LOGGER.progname = &quot;noah-agent&quot;
Celluloid.logger = LOGGER

class WorkerError &lt; Exception; end

class Pool
  include Celluloid::Actor
  #trap_exit :worker_exception_handler

  attr_reader :workers, :busy_workers

  def initialize(name, opts = {:num_workers =&gt; 10, :worker_class =&gt; Worker})
    @name = name
    @workers = []
    @busy_workers = []
    LOGGER.info(&quot;Pool #{name} starting up&quot;)
    opts[:num_workers].times do |worker|
      start_worker(opts[:worker_class])
    end
  end

  def start_worker(klass)
    worker_id = gen_worker_id
    LOGGER.info(&quot;Pool #{@name} is starting a #{klass.to_s} worker&quot;)
    wkr = klass.supervise_as &quot;#{@name}_worker_#{worker_id}&quot;.to_sym, &quot;#{@name}_worker_#{worker_id}&quot;
    @workers &lt;&lt; wkr
  end

  def notify_worker(msg)
    worker = self.get_worker
    @busy_workers &lt;&lt; worker.name
    worker.work msg
    @busy_workers.delete worker.name
  end

  def worker_exception_handler(actor, reason)
    LOGGER.debug(&quot;Worker #{actor.name} crashed because #{reason}. You should see a doctor about that&quot;)
  end

  
  protected
  def gen_worker_id
    Digest::SHA1.hexdigest(UUID.generate)
  end

  def get_worker
    worker = @workers.sample.actor
    LOGGER.info(&quot;Found Worker: #{worker.name} in the pool&quot;)
    if worker.alive?
      worker
    else
      LOGGER.error &quot;Worker #{worker.name} was dead. Retrying!&quot;
      self.get_worker
    end
  end

end

class MyWorker
  include Celluloid::Actor
  attr_reader :name

  def initialize(name)
    @name = name
  end

  def work(msg)
    LOGGER.info(&quot;Message for you sir! #{msg}&quot;)
    case msg
    when &quot;die&quot;
      # Simulate some long-running work that crashes
      sleep 15
      raise WorkerError, &quot;Boo got shot!&quot;
    else
      # Simulate some long-running work here
      sleep 30
      LOGGER.debug(&quot;Hey there camper! #{@name} is doing some work for you&quot;)
    end
  end

end

class TestApp &lt; Sinatra::Base
  @pool = Pool.supervise_as :my_cool_pool, &quot;MyCoolPool&quot;, {:num_workers =&gt; 30, :worker_class =&gt; MyWorker}
  configure do
    set :app_file, __FILE__
    set :logging, false
    set :dump_errors, false
    set :run, false
    set :server, &quot;thin&quot;
    set :pool, @pool
  end

  put '/scale' do
    settings.pool.actor.start_worker(MyWorker)
    &quot;Added a worker&quot;
  end

  get '/stats' do
    &quot;Worker count: #{settings.pool.actor.workers.size}\n Busy workers: #{settings.pool.actor.busy_workers.size}&quot;
  end

  put '/die' do
    settings.pool.actor.notify_worker! &quot;die&quot;
  end

  put '/send' do
    settings.pool.actor.notify_worker! request.body.read
  end
end

app = TestApp
app.run!</code></pre></noscript></div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring Sucks - Watch your language]]></title>
    <link href="http://lusis.github.com/blog/2011/07/21/monitoring-sucks-watch-your-language/"/>
    <updated>2011-07-21T18:19:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/07/21/monitoring-sucks-watch-your-language</id>
    <content type="html"><![CDATA[<p><em>The following post is a recap of what was discussed in the 07/21/11 #monitoringsucks irc meeting</em></p>

<p>Before I get any further, I just want to thank everyone who attended,
either in virtual person or in spirit. There have been so many awesome
discussions going around this topic since we started. I am truly
priviledged to interact with each and every one of you. I struggle to
count myself a peer and I can only hope that I provide something in
return.</p>

<p>I mentioned to someone today that I’m literally sick of the current
landscape. I consider the current crop of monitoring solutions to be
technologically bankrupt. The situation is fairly untenable at this
point.</p>

<p>I just installed (after having a total loss of an existing Zenoss setup)
Nagios again. I’m not joking when I say that it depressed the everliving
hell out of me. The monitoring landscape simply has not kept up with
modern developments. At first it was mildly frustrating. Then it was
annoying. Now it’s actually a detriment.</p>

<p>Now that we have that out of the way….</p>

<!--more-->


<h1>Darmok and Jalad at Tanagra</h1>

<p>Communication is important. Like Picard and Dathon, we’ve been stranded
on a planet with shitty monitoring tools and we’re unable to communicate
about the invisibile threat of suck because we aren’t even speaking the
same language. I say event, you hear trigger, I mean data point. So the
first order of business was to try and agree on a set of terms. It was
decided that we would consider these things primitives. Here they are:</p>

<p>Please read through this before jumping to any conclusions. I promise it
will all become clear (as mud).</p>

<h2>metric</h2>

<p><em>a numeric or boolean data point</em></p>

<p>The data type of a metric was something of a sticking point. People were
getting hung up on data points being various things (a log message, a
“status”, a value). We needed something to describe the origin. The
single “thing” that triggered it all. That thing is a metric.</p>

<p>So why numeric <em>OR</em> boolean? It was pretty clear that many people
considered, and rightly so I would argue, that a state change is a
metric. A good example given by <a href="http://twitter.com/cwebber">Christopher Webber</a> is that of a BGP route going away.
Why is this a less valid data point than the amount of disk space in use
or the latency from one host to another? Frankly, it’s not.</p>

<p>But here’s where it gets fuzzy. What about a log message. Surely that’s a data point and thus a metric.</p>

<p>Yes and No. The <em>presence</em> of a log message is a data point. But it’s a boolean. The log message itself?</p>

<h2>context</h2>

<p><em>metadata about a metric</em></p>

<p>Now metadata itself is a loaded term but in this scope, the “human
readable” attributes are considered context. Going back to our log
example. The presence of the log message is a metric. The log message
itself is context. Here’s the thing. You want to know if there is an
error message in a log file. The type of error, the error message text?
That’s context for the metric to use in determining a course of action.</p>

<p>Plainly speaking, metrics are for machines. Context is for humans. This
leads us to….</p>

<h2>event</h2>

<p><em>metric combined with context</em></p>

<p>This is still up in the air but the general consensus was that this was
a passable definition. The biggest problem with a group of domain
experts is that they are frequently unable to accept semantic
approximation. Take the discussion of Erlang Spawned process:</p>

<ul>
<li>It’s sort of like a VM on a VM</li>
<li>NO IT’S NOT.</li>
<li><em>headdesk</em></li>
</ul>


<p>The fact is that an Erlang spawned process has shades of a virtual
machine is irrelevant to the domain expert. We found similar discussions
around what we would call the combination of a metric and its context.
But where do events come from?</p>

<h2>resource</h2>

<p><em>the source of a metric</em></p>

<p>Again, we could get into arguments around what a resource is. One thing
that was painfully obvious is that we’re all sick and tired of being
tied to the Host and Service model. It’s irrelevant. These constructs
are part “legacy” and part “presentation”.</p>

<p>Any modern monitoring thought needs to realize that metrics no longer
come from physical hosts or are service states. In the modern world,
we’re taking a holistic view of monitoring that includes not only bare
metal but business matters. The number of sales is a metric but it’s not
tied to a server. It’s tied to the business as a whole. The source of
your metrics is a resource. So now that we have this information - a
metric, its context and who generated it - what do we do? We take….</p>

<h2>action</h2>

<p><em>a response to a given metric</em></p>

<p>What response? It doesn’t MATTER. Remember that these are primitives.
The response is determined by components of your monitoring
infrastructure. Humans note the context. Graphite graphs it. Nagios
alerts on it. ESPER correlates it with other metrics. Don’t confuse
scope here. From this point on, whatever happens has is all decided on
by a given component. It’s all about perspective and aspects.</p>

<h1>Temba, his arms wide</h1>

<p>I’m sure through much of that, you were thinking “alerting! graphing!
correlation!”. Yes, that was pretty much what happened during the
meeting as well. Everyone has pretty much agreed (I think) at this point
that any new monitoring systems should be modular in nature. As <a href="http://twitter.com/obfuscurity">Jason
Dixon</a> put it - “Voltron”. No single
system that attempts to do everything will meet everyone’s needs.
However, with a common dictionary and open APIs you should be able to
build a system that DOES meet your needs. So what are those components?
Sadly this part is not as fleshed out. We simply ran out of time.
However we did come up with a few basics:</p>

<h2>Collection</h2>

<p><em>getting the metrics</em></p>

<p>It doesn’t matter if it’s push or pull. It doesn’t matter what the
transport is - async or point-to-point. Somehow, you have to get a
metric from a resource.</p>

<h2>Event Processing</h2>

<p><em>taking action</em></p>

<p>Extract the metric and resource from an event. Do something with it.
Maybe you send the metric to another component. Maybe you “present” it
or send it somewhere to be presented. Maybe you perform a change on a
resource (restarting a service). Essentially the decision engine.</p>

<h2>Presentation</h2>

<p>While you might be thinking of graphing here, that’s a type of
presentation. You know what else is presentation? An email alert. Stick
with me. I know what’s going through your head. No..not that…the other
thing.</p>

<h2>Analytics</h2>

<p>This is pretty much correlation. We didn’t get a REAL solid defintion
here but everyone was in agreement that some sort of analytics is a
distinct component.</p>

<h2>The “other” stuff</h2>

<p>As I said, we had to kind of cut “official” things short. There was
various discussion around Storage and Configuration. Storage I
personally can see as a distinct component but Configuration not so
much. Configuration is an aspect of a component but not a component
itself.</p>

<h2>Logical groupings</h2>

<p>Remember when I said I know what you’re thinking? This is what I think
it was.</p>

<p>You can look at the above items and from different angles they look
similar. I mean sending an email feels more like event processing than
presentation. You’d probably be right. By that token, drawing a point on
a graph is technically processing an event. The fact is many components
have a bit of a genetic bond. Not so much parent/child or sibling but
more like cousins. In all honesty, if I were building an event
processing component, I’d probably handle sending the email right there.
Why send it to another component? That makes perfect sense. Graphing?
Yeah I’ll let graphite handle that but I can do service restarts and
send emails. Maybe you have an intelligent graphing component that can
do complex correlation inband. That makes sense too.</p>

<p>I’m quite sure that we’ll have someone who writes a kickass event
processor that happens to send email. I’m cool with that. I just don’t
want to be bound to ONLY being able to send emails because that’s all
your decision system supports.</p>

<h1>Shaka, when the walls fell</h1>

<p>Speaking personally, I really feel like today’s discussion was VERY
productive. I know that you might not agree with everything here. Things
are always up for debate. The only thing I ask is that at some point,
we’re all willing to say “I know that this definition isn’t EXACTLY how
I would describe something but it’s close enough to work”.</p>

<p>So what are the next steps? I think we’ve got enough information and
consensus here for people to start moving forward with some things. One
exercise, inspired by something Matt Ray said, that we agreed would be
REALLY productive is to take an existing application and map what it
does to our primitives and components. Matt plans on doing that with
Zenoss since that’s what he knows best.</p>

<p>Let me give an example:</p>

<p>Out of the box, Nagios supports Hosts and Services which map pretty
cleanly to resources. It is does not only collection but event
processing and presentation. It not only supports metrics but also
context (Host down is the boolean metric. “Response timeout” is the
context. Through something like pnp4nagios, it can support different
presentations. It has very basic set of Analytic functionality.</p>

<p>Meanwhile Graphite is, in my mind, strictly presentation and deals only
with metrics. It does support both numeric and boolean metrics. It also
has basic resource functionality but it’s not hardwired. It doesn’t
really do event handling in the strict sense. Analytics is left to the
human mind. It certainly doesn’t support context.</p>

<p>I’d love to see more of these evaluations.</p>

<p>Also, I know there are tons of “words” that we didn’t cover - thresholds
for instance. While there wasn’t a total consensus, there was some
agreement that somethings were attributes of a component but not a
primitive itself. It was also accepted that components themselves would
be primitives. You correlation engine might aggregate (another word) a
group of metrics and generate an event. At that point, your correlation
engine is now a resource with its own metrics (25 errors) and context
(“number of errors across application servers exceeded acceptable
limits”) which could be then sent to an event processor.</p>

<p>That’s the beauty of the Voltron approach and not binding a resource to
a construct like a Host.</p>

<h1>Special note to the Aussies</h1>

<p>I’m very sorry that we couldn’t get everyone together. I’ve scheduled
another meeting where we can start from scratch just like this one, or
build on what was discussed already. I’m flexible and willing to wake up
at whatever time works best for you guys</p>

<p>Thanks again to everyone who attended. If you couldn’t be there, I hope
you can make the next one.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why Monitoring Sucks]]></title>
    <link href="http://lusis.github.com/blog/2011/06/05/why-monitoring-sucks/"/>
    <updated>2011-06-05T19:30:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/06/05/why-monitoring-sucks</id>
    <content type="html"><![CDATA[<p><em>(and what we&#8217;re doing about it)</em></p>

<p>About two weeks ago someone made a tweet. At this point, I don&#8217;t
remember who said it but the gist was that &#8220;monitoring sucks&#8221;. I
happened to be knee-deep in frustrating bullshit around that topic and
was currently evaluating the same effing tools I&#8217;d evaluated at every
other company over the past 10 years or so. So I did what seems to be
S.O.P for me these days. I started something.</p>

<!--more-->


<h1>But does monitoring REALLY suck?</h1>

<p>Heck no! Monitoring is AWESOME. Metrics are AWESOME. I love it. Here&#8217;s
what I don&#8217;t love: - Having my hands tied with the model of host and
service bindings. - Having to set up &#8220;fake&#8221; hosts just to group
arbitrary metrics together - Having to either collect metrics twice -
once for alerting and another for trending - Only being able to see my
metrics in 5 minute intervals - Having to chose between shitty interface
but great monitoring or shitty monitoring but great interface - Dealing
with a monitoring system that thinks <strong>IT</strong> is the system of truth for
my environment - Perl (I kid&#8230;sort of) - Not actually having any real
choices</p>

<p>Yes, yes I know:</p>

<blockquote><p>You can just combine Nagios + collectd + graphite + cacti +
pnp4nagios and you have everything you need!</p></blockquote>

<p>Seriously? Kiss my ass. I&#8217;m a huge fan of the Unix pipeline philosophy
but, christ, have you ever heard the phrase &#8220;antipattern&#8221;?</p>

<h1>So what the hell are you going to do about it?</h1>

<p>I&#8217;m going to let smart people be smart and do smart things.</p>

<p>Step one was getting everyone who had similar complaints together on
IRC. That went pretty damn well. Step two was creating a github repo.
Seriously. Step two should ALWAYS be &#8220;create a github repo&#8221;. Step three?
Hell if I know.</p>

<p>Here&#8217;s what I do know. There are plenty of frustrated system
administrators, developers, engineers, &#8220;devops&#8221; and everything under the
sun who don&#8217;t want much. All they really want is for shit to work. When
shit breaks, they want to be notified. They want pretty graphs. They
want to see business metrics along side operational ones. They want to
have a 52-inch monitor in the office that everyone can look at and say:</p>

<blockquote><p>See that red dot? That&#8217;s bad. Here&#8217;s what was going on when we got
that red dot. Let&#8217;s fix that shit and go get beers</p></blockquote>

<h1>About the &#8220;repo&#8221;</h1>

<p>So the plan I have in place for the repository is this. We don&#8217;t really
need code. What we need is an easy way for people to contribute ideas.
The plan I have in place for this is partially underway. There&#8217;s now a
<em>monitoringsucks</em> organization on Github. Pretty much anyone who is
willing to contribute can get added to the team. The idea is that, as
smart people think of smart shit, we can create new repository under
some unifying idea and put blog posts, submodules, reviews,
ideas..whatever into that repository so people have an easy place to go
get information. I&#8217;d like to assign someone per repository to be the
owner. We&#8217;re all busy but this is something we&#8217;re all highly interested
in. If we spread the work out and allow easy contribution, then we can
get some real content up there.</p>

<p>I also want to keep the repos as light and cacheable as possible. The
organization is under the github &#8220;free&#8221; plan right now and I&#8217;d like to
keep it that way.</p>

<h2>Blog Posts Repo</h2>

<p>This repo serves as a place to collect general information about blog
posts people come across. Think of it as hyper-local delicious in a
DVCS.</p>

<p>Currently, by virtue of the first commit, Michael Conigliaro is the
&#8220;owner&#8221;. You can follow him on twitter and github as @mconigliaro</p>

<h2>IRC Logs Repo</h2>

<p>This repo is a log of any &#8220;scheduled&#8221; irc sessions. Personally, I don&#8217;t
think we need a distinct #monitoringsucks channel but people want to
keep it around. The logs in this repo are not full logs. Just those from
when someone says &#8220;Hey smart people. Let&#8217;s think of smart shit at this
date/time&#8221; on twitter.</p>

<p>Currently <strong>I</strong> appear to be the owner of this repo. I would love for
someone who can actually make the logs look good to take this over.</p>

<h2>Tools Repo</h2>

<p>This repo is really more of a &#8220;curation&#8221; repo. The plan is that each
directory is the name of some tool with two things it in:</p>

<ul>
<li>A README.md as a review of the tool</li>
<li>A submodule link to the tool&#8217;s repo (where appropriate)</li>
</ul>


<p>Again, I think I&#8217;m running point on this one. Please note that the
submodule links APPEAR to have some sort of UI issue on github. Every
submodule appears to point to Dan DeLeo&#8217;s &#8216;critical&#8217; project.</p>

<h2>Metrics Catalog Repo</h2>

<p>This is our latest member and it already has an official manager! Jason
Dixon (@obfuscurity on github/twitter - jdixon on irc) suggested it so
he get&#8217;s to run it ;) The idea here is that this will serves as a set of
best practices around what metrics you might want to collect and why.
I&#8217;m leaving the organization up to Jason but I suggested a
per-app/service/protocol directory.</p>

<h1>Wrap Up</h1>

<p>So that&#8217;s where we are. Where it goes, I have no idea. I just want to
help where ever I can. If you have any ideas, hit me up on
twitter/irc/github/email and let me know. It might help to know that if
you suggest something, you&#8217;ll probably be made the person reponsible for
it ;)</p>

<p><strong>Update!</strong></p>

<p>It was our good friend Sean Porter (@portertech on twitter), that we
have to thank for all of this ;)</p>

<p>  <a href="https://picasaweb.google.com/lh/photo/Zi1k9F_7lBKjcN8dtJlXXQ?feat=embedwebsite"><img src="https://lh5.googleusercontent.com/-O6mNvCvCPyU/TexPV1P9YaI/AAAAAAAAAWk/7ZQ8BkXUyn8/s144/monitoring-sucks.png" alt="image" /></a>
  From <a href="https://picasaweb.google.com/lusisjv/PublicPhotos?feat=embedwebsite">Public Photos</a></p>

<p><strong>Update (again)</strong></p>

<p>It was kindly pointed out that I never actually included a link to the
repositories. Here they are:</p>

<p><a href="https://github.com/monitoringsucks">https://github.com/monitoringsucks</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 4]]></title>
    <link href="http://lusis.github.com/blog/2011/05/19/on-noah-part-4/"/>
    <updated>2011-05-19T22:01:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/19/on-noah-part-4</id>
    <content type="html"><![CDATA[<p><em>This is the fourth part in a series on Noah. <a href="http://goo.gl/l3Mgt">Part 1</a>, <a href="http://goo.gl/Nj2TN">Part 2</a> and <a href="http://goo.gl/RsZtZ">Part 3</a> are available as well</em></p>

<p>In Part 1 and 2 of this series I covered background on Zookeeper and
discussed the similarities and differences between it and Noah. Part 3
was about the components underneath Noah that make it tick.</p>

<p>This post is about the &#8220;future&#8221; of Noah. Since I&#8217;m a fan of Fourcast
podcast, I thought it would be nice to do an immediate, medium and long
term set of goals.</p>

<!--more-->


<h1>Immediate Future - the road to 1.0</h1>

<p>In the most immediate future there are a few things that need to happen.
These are in no specific order.</p>

<ul>
<li><p>General</p>

<ul>
<li>Better test coverage ESPECIALLY around the watch subsystem</li>
<li>Full code comment coverage</li>
<li>Chef cookbooks/Puppet manifests for doing a full install</li>
<li>&#8220;fatty&#8221; installers for a standalone server</li>
<li>Documentation around operational best practices</li>
<li>Documentation around clustering, redundancy and hadr</li>
<li>Documentation around integration best practices</li>
<li>Performance testing</li>
</ul>
</li>
<li><p>Noah Server</p>

<ul>
<li>Expiry flags and reaping for Ephemerals</li>
<li>Convert mime-type in Configurations to make sense</li>
<li>Untag and Unlink support</li>
<li>Refactor how you specify Redis connection information</li>
<li>Integrated metrics for monitoring (failed callbacks, expired
ephemeral count, that kind of stuff)</li>
</ul>
</li>
<li><p>Watcher callback daemon</p>

<ul>
<li>Make the HTTP callback plugin more flexible</li>
<li>Finish binscript for the watcher daemon</li>
</ul>
</li>
<li><p>Other</p>

<ul>
<li>Finish <a href="http://goo.gl/B65aL">Boat</a></li>
<li>Finish NoahLite LWRP for Chef (using Boat)</li>
<li>A few more HTTP-based callback plugins (Rundeck, Jenkins)</li>
</ul>
</li>
</ul>


<p>Now that doesn&#8217;t look like a very cool list but it&#8217;s a lot of work for
one person. I don&#8217;t blame anyone for not getting excited about it. The
goal now is to get a functional and stable application out the door that
people can start using. Mind you I think it&#8217;s usable now (and I&#8217;m
already using it in &#8220;production&#8221;).</p>

<p>Obviously if anyone has something else they&#8217;d like to see on the list,
let me know.</p>

<h1>Medium Rare</h1>

<p>So beyond that 1.0 release, what&#8217;s on tap? Most of the work will
probably occur around the watcher subsystem and the callback daemon.
However there are a few key server changes I need to implement.</p>

<ul>
<li><p>Server</p>

<ul>
<li>Full ACL support on every object at every level</li>
<li>Token-based and SSH key based credentialing</li>
<li>Optional versioning on every object at every level</li>
<li>Accountability/Audit trail</li>
<li>Implement a long-polling interface for inband watchers</li>
</ul>
</li>
<li><p>Watcher callback daemon</p>

<ul>
<li>Decouple the callback daemon from the Ruby API of the server.
Instead the daemon itself needs to be a full REST client of the
Noah server</li>
<li>Break out the &#8220;official&#8221; callback daemon into a distinct package</li>
</ul>
</li>
<li><p>Clients</p>

<ul>
<li>Sinatra Helper</li>
</ul>
</li>
</ul>


<p>Also during this period, I want to spend time building up the ecosystem
as a whole. You can see a general mindmap of that
<a href="https://github.com/lusis/Noah/wiki/Ecosystem">here</a>.</p>

<p>Going into a bit more detail&#8230;</p>

<h2>Tokens and keys</h2>

<p>It&#8217;s plainly clear that something which has the ability to make runtime
environment changes needs to be secure. The first thing to roll off the
line post-1.0 will be that functionality. Full ACL support for all
entries will be enabled and in can be set at any level in the namespace
just the same as Watches.</p>

<h2>Versioning and Auditing</h2>

<p>Again for all entires and levels in the namespace, versioning and
auditing will be allowed. The intention is that the number of revisions
and audit entries are configurable as well - not just an enable/disable
bit.</p>

<h2>In-band watches</h2>

<p>While I&#8217;ve lamented the fact that watches were in-band only in
Zookeeper, there&#8217;s a real world need for that model. The idea of
long-polling functionality is something I&#8217;d actually like to have by 1.0
but likely won&#8217;t happen. The intent is simply that when you call say
<code>/some/path/watch</code>, you can pass an optional flag in the message stating
that you want to watch that endpoint for a fixed amount of time for any
changes. Optionally a way to subscribe to all changes over long-polling
for a fixed amount of time is cool too.</p>

<h2>Agent changes</h2>

<p>These two are pretty high on my list. As I said, there&#8217;s a workable
solution with minimal tech debt going into the 1.0 release but long
term, this needs to be a distinct package. A few other ideas I&#8217;m kicking
around are allowing configurable filtering on WHICH callback types an
agent will handle. The idea is that you can specify that this invocation
only handle http callbacks while this other one handles AMQP.</p>

<h2>Sinatra Helper</h2>

<p>One idea I&#8217;d REALLY like to come to fruition is the Sinatra Helper. I
envision it working something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'> <span class="nb">require</span> <span class="s1">&#39;sinatra/base&#39;</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">class</span> <span class="nc">MyApp</span> <span class="o">&lt;</span> <span class="no">Sinatra</span><span class="o">::</span><span class="no">Base</span>
</span><span class='line'>    <span class="n">register</span> <span class="no">Noah</span><span class="o">::</span><span class="no">Sinatra</span>
</span><span class='line'>  
</span><span class='line'>    <span class="n">noah_server</span> <span class="s2">&quot;http://localhost:5678&quot;</span>
</span><span class='line'>    <span class="n">noah_node_name</span> <span class="s2">&quot;myself&quot;</span>
</span><span class='line'>    <span class="n">noah_app_name</span> <span class="s2">&quot;MyApp&quot;</span>
</span><span class='line'>    <span class="n">noah_token</span> <span class="s2">&quot;somerandomlongstring&quot;</span>
</span><span class='line'>    <span class="n">dynamic_get</span> <span class="ss">:database_server</span>
</span><span class='line'>    <span class="n">dynamic_set</span> <span class="ss">:some_other_variable</span><span class="p">,</span> <span class="s2">&quot;foobar&quot;</span>
</span><span class='line'>    <span class="n">watch</span> <span class="ss">:this_other_node</span>
</span><span class='line'>  <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>The idea is that the helper allows you to register your application very
easily with Noah for other components in your environment to be know. As
a byproduct, you get the ability to get/set certain configuration
parameters entirely in Noah. The watch setting is kind of cool as well.
What will happen is if you decide to <code>watch</code> something this way, the
helper will create a random (and yes, secure) route in your application
that watch events can notify. In this way, your Sinatra application can
be notified of any changes and will automatically &#8220;reconfigure&#8221; itself.</p>

<p>Obviously I&#8217;d love to see other implementations of this idea for other
languages and frameworks.</p>

<h1>Long term changes</h1>

<p>There aren&#8217;t so much specific list items here as general themes and
ideas. While I list these as long term, I&#8217;ve already gotten an offer to
help with some of them so they might actually get out sooner.</p>

<h2>Making Noah itself distributed</h2>

<p>This is something I&#8217;m VERY keen on getting accomplished and would really
consider it the fruition of what Noah itself does. The idea is simply
that multiple Noah servers themselves are clients of other Noah servers.
I&#8217;ve got several ideas about how to accomplish this but I got an
interesting follow up from someone on Github the other day. He asked
what my plans were in this area and we had several lengthy emails back
and forth including an offer to work on this particular issue.</p>

<p>Obviously there are a whole host of issues to consider. Race conditions
in ordered delivery of Watch callbacks (getting a status &#8220;down&#8221; after a
status &#8220;up&#8221; when it&#8217;s supposed to be the other way around..) and
eventual consistency spring to mind first.</p>

<p>The general architecture idea that was offered up is to use
<a href="https://github.com/derekcollison/nats">NATS</a> as the mechanism for
accomplishing this. In the same way that there would be AMQP callback
support, there would be NATS support. Additional Noah servers would only
need to know one other member to bootstrap and everything else happens
using the natural flows within Noah.</p>

<p>The other part of that is how to handle the Redis part. The natural
inclination is to use the upcoming Redis clustering but that&#8217;s not
something I want to do. I want each Noah server to actually include its
OWN Redis instance &#8220;embedded&#8221; and not need to rely on any external
mechanism for replication of the data. Again, the biggest validation of
what Noah is designed to do is using only Noah itself to do it.</p>

<h2>Move off Redis/Swappable persistence</h2>

<p>If NATS says anything to me, it says &#8220;Why do you even need Redis?&#8221;. If
you recall, I went with Redis because it solved multiple problems. If I
can find a persistence mechanism that I can use without any external
service running, I&#8217;d love to use it.</p>

<h2>ZeroMQ</h2>

<p>If I were to end up moving off Redis, I&#8217;d need a cross platform and
cross language way to handle the pubsub component. NATS would be the
first idea but NATS is Ruby only (unless I&#8217;ve missed something). ZeroMQ
appears to have broad language and platform support so writing custom
agents in the same vein as the Redis PUBSUB method should be feasible.</p>

<h2>Nanite-style agents</h2>

<p>This is more of a command-and-control topic but a set of
high-performance specialized agents on systems that can watch the PUBSUB
backend or listen for callbacks would be awesome. This would allow you
really integrate Noah into your infrastructure beyond the application
level. Use it to trigger a puppet or chef run, reboot instances or do
whatever. This is really about bringing what I wanted to accomplish with
Vogeler into Noah.</p>

<h2>The PAXOS question</h2>

<p>A lot of people have asked me about this. I&#8217;ll state right now that I
can only make it through about 20-30% of any reading about Paxos before
my brain starts to melt. However in the interest of proving myself the
fool, I think it would be possible to implement some Paxos like
functionality on top of Noah. Remember that Noah is fundamentally about
fully disconnected nodes. What better example of a network of unreliable
processors than ones that never actually talk to each other. The problem
is that the use case for doing it in Noah is fairly limited so as not to
be worth it.</p>

<p>The grand scheme is that Noah helps enable the construction of systems
where you can say &#8220;This component is free to go off and operate in this
way secure in the knowledge that if something it needs to know changes,
someone will tell it&#8221;. I did say &#8220;grand&#8221; didn&#8217;t I? At some point, I may
hit the limit of what I can do using only Ruby. Who knows.</p>

<h1>Wrap up - Part 4</h1>

<p>Again with the recap</p>

<ul>
<li>Get to 1.0 with a stable and fixed set of functionality</li>
<li>Nurture the Noah ecosystem</li>
<li>Make it easy for people to integrate Noah into thier applications</li>
<li>Get all meta and make Noah itself distributed using Noah</li>
<li>Minimize the dependencies even more</li>
<li>Build skynet</li>
</ul>


<p><em>I&#8217;m not kidding on that last one. Ask me about Parrot AR drones and
Noah sometime</em></p>

<p>If you made it this far, I want to say thank you to anyone who read any
or all of the parts. Please don&#8217;t hesitate to contact me with any
questions about the project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 3]]></title>
    <link href="http://lusis.github.com/blog/2011/05/18/on-noah-part-3/"/>
    <updated>2011-05-18T18:14:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/18/on-noah-part-3</id>
    <content type="html"><![CDATA[<p><em>This is the third part in a series on Noah. <a href="http://goo.gl/l3Mgt">Part 1</a> and <a href="http://goo.gl/Nj2TN">Part 2</a> are available as well</em></p>

<p>In Part 1 and 2 of this series I covered background on Zookeeper and
discussed the similarities and differences between it and Noah. This
post is discussing the technology stack under Noah and the reasoning for
it.</p>

<h1>A little back story</h1>

<p>I&#8217;ve told a few people this but my original intention was to use Noah as
a way to learn Erlang. However this did not work out. I needed to get a
proof of concept out much quicker than the ramp up time it would take to
<a href="http://learnyousomeerlang.com/">learn me some Erlang</a>. I had this
grandiose idea to slap mnesia, riak_core and webmachine into a tasty
ball of Zookeeper clonage.</p>

<!--more-->


<p>I am not a developer by trade. I don&#8217;t have any formal education in
computer science (or anything for that matter). The reason I mention
this is to say that programming is hard work for me. This has two side
effects:</p>

<ul>
<li>It takes me considerably longer than a working developer to code
what&#8217;s in my head</li>
<li>I can only really learn a new language when I have an itch to
scratch. A real world problem to model.</li>
</ul>


<p>So in the interest of time, I fell back to a language I&#8217;m most
comfortable with right now, Ruby.</p>

<h1>Sinatra and Ruby</h1>

<p>Noah isn&#8217;t so much a web application as it is this &#8216;api thing&#8217;. There&#8217;s
no proper front end and honestly, you guys don&#8217;t want to see what my
design deficient mind would create. I like to joke that in the world of
MVC, I stick to the M and C. Sure, APIs have views but not in the &#8220;click
the pretty button sense&#8221;.</p>

<p>I had been doing quite a bit of glue code at the office using
<a href="http://www.sinatrarb.com">Sinatra</a> (and EventMachine) so I went with
that. Sinatra is, if you use sheer number of clones in other languages
as an example, a success for writing API-only applications. I also
figured that if I wanted to slap something proper on the front, I could
easily integrate it with <a href="http://www.padrinorb.com">Padrino</a>.</p>

<p>But now I had to address the data storage issue.</p>

<h1>Redis</h1>

<p>Previously, as a way to learn Python at another company, I wrote an
application called <a href="https://github.com/lusis/vogeler">Vogeler</a>. That
application had a lot of moving parts - CouchDB for storage and RabbitMQ
for messaging.</p>

<p>I knew from dealing with CouchDB on CentOS5 that I wasn&#8217;t going to use
THAT again. Much of it would have been overkill for Noah anyway. I
realized I really needed nothing more than a key/value store. That
really left me with either Riak or Redis. I love Riak but it wasn&#8217;t the
right fit in this case. I needed something with a smaller dependency
footprint. Mind you Riak is VERY easy to install but managing Erlang
applications is still a bit edgy for some folks. I needed something
simpler.</p>

<p>I also realized early on that I needed some sort of basic queuing
functionality. That really sealed Redis for me. Not only did it have
zero external dependencies, but it also met the needs for queuing. I
could use <code>lists</code> as dedicated direct queues and I could use the
built-in <code>pubsub</code> as a broadcast mechanism. Redis also has a fast atomic
counter that could be used to approximate the ZK sequence primitive
should I want to do that.</p>

<p>Additionally, Redis has master/slave (not my first choice) support for
limited scaling as well as redundancy. One of my original design goals
was that Noah behave like a traditional web application. This is a model
ops folks understand very well at this point.</p>

<h1>EventMachine</h1>

<p>When you think asynchronous in the Ruby world, there&#8217;s really only one
tool that comes to mind, EventMachine. Noah is designed for asynchronous
networks and is itself asynchronous in its design. The callback agent
itself uses EventMachine to process watches. As I said previously, this
is simply using an EM friendly Redis driver that can do <code>PSUBSCRIBE</code>
(using em-hiredis) and send watch messages (using em-http-request since
we only support HTTP by default).</p>

<h1>Ohm</h1>

<p>Finally I slapped <a href="http://ohm.keyvalue.org">Ohm</a> on top as the
abstraction layer for Redis access. Ohm, if you haven&#8217;t used it, is
simply one of if not the best Ruby library for working with Redis. It&#8217;s
easily extensible, very transparent and frankly, it just gets the hell
out of your way. A good example of this is converting some result to a
hash. By default, Ohm only returns the id of the record. Nothing more.
It also makes it VERY easy to drop past the abstraction and operate on
Redis directly. It even provides helpers to get the keys it uses to
query Redis. A good example of this is in the Linking and Tagging code.
The following is a method in the Tag model:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'> <span class="k">def</span> <span class="nf">members</span><span class="o">=</span><span class="p">(</span><span class="n">member</span><span class="p">)</span>
</span><span class='line'>    <span class="nb">self</span><span class="o">.</span><span class="n">key</span><span class="o">[</span><span class="ss">:members</span><span class="o">].</span><span class="n">sadd</span><span class="p">(</span><span class="n">member</span><span class="o">.</span><span class="n">key</span><span class="p">)</span>
</span><span class='line'>    <span class="n">member</span><span class="o">.</span><span class="n">tag!</span> <span class="nb">self</span><span class="o">.</span><span class="n">name</span> <span class="k">unless</span> <span class="n">member</span><span class="o">.</span><span class="n">tags</span><span class="o">.</span><span class="n">member?</span><span class="p">(</span><span class="nb">self</span><span class="p">)</span>
</span><span class='line'>  <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>Because Links and Tags are a one-to-many across multiple models, I drop
down to Redis and use <code>sadd</code> to add the object to a Redis set of objects
sharing the same tag.</p>

<p>It also has a very handy feature which is how the core of Watches are
done. You can define hooks at any phase of Redis interaction - before
and after saves, creates, updates and deletes. the entire Watch system
is nothing more than calling these post hooks to format the state of the
object as JSON, add metadata and send the message using <code>PUBLISH</code>
messages to Redis with the Noah namespace as the channel.</p>

<h1>Distribution vectors</h1>

<p>I&#8217;ve used this phrase with a few people. Essentially, I want as many
people as possible to be able to use the Noah server component. I&#8217;ve
kept the Ruby dependencies to a minimum and I&#8217;ve made sure that every
single one works on MRI 1.8.7 up to 1.9.2 as well as JRuby. I already
distribute the most current release as a war that can be deployed to a
container or run standalone. I want the lowest barrier to entry to get
the broadest install base possible. When a new PaaS offering comes out,
I pester the hell out of anyone I can find associated with it so I can
get deploy instructions written for Noah. So far you can run it on
Heroku (using the various hosted Redis providers), CloudFoundry and
dotcloud.</p>

<p>I&#8217;m a bit more lax on the callback daemon. Because it can be written in
any language that can talk to the Redis pubsub system and because it has
&#8220;stricter&#8221; performance needs, I&#8217;m willing to make the requirements for
the &#8220;official&#8221; daemon more stringent. It currently ONLY works on MRI
(mainly due to the em-hiredis requirement).</p>

<h2>Doing things differently</h2>

<p>Some people have asked me why I didn&#8217;t use technology A or technology B.
I think I addressed that mostly above but I&#8217;ll tackle a couple of key
ones.</p>

<p>ZeroMQ</p>

<p>The main reason for not using 0mq was that I wasn&#8217;t really aware of it.
Were I to start over and still be using Ruby, I&#8217;d probably give it a
good strong look. The would still be the question of the storage
component though. There&#8217;s still a possible place for it that I&#8217;ll
address in part four.</p>

<p>NATS</p>

<p>This was something I simply had no idea about until I started poking
around the CloudFoundry code base. I can almost guarantee that NATS will
be a part of Noah in the future. Expect much more information about that
in part four.</p>

<p>MongoDB</p>

<p>You have got to be kidding me, right? I don&#8217;t trust my data (or anyone
else&#8217;s for that matter) to a product that doesn&#8217;t understand what
durability means when we&#8217;re talking about databases.</p>

<p>Insert favorite data store here</p>

<p>As I said, Redis was the best way to get multiple required functionality
into a single product. Why does a data storage engine have a pubsub
messaging subsystem built in? I don&#8217;t know off the top of my head but
I&#8217;ll take it.</p>

<h2>Wrap up - Part 3</h2>

<p>So again, because I evidently like recaps, here&#8217;s the take away:</p>

<ul>
<li>The key components in Noah are Redis and Sinatra</li>
<li>Noah is written in Ruby because of time constraints in learning a
new language</li>
<li>Noah strives for the server component to have the broadest set of
distribution vectors as possible</li>
<li>Ruby dependencies are kept to a minimum to ensure the previous point</li>
<li>The lightest possible abstractions (Ohm) are used.</li>
<li>Stricter requirements exist for non-server components because of
flexibility in alternates</li>
<li>I really should learn me some erlang</li>
<li>I&#8217;m not a fan of MongoDB</li>
</ul>


<p>If you haven&#8217;t guessed, I&#8217;m doing one part a night in this series.
Tomorrow is part four which will cover the future plans for Noah. I&#8217;m
also planning on a bonus part five to cover things that didn&#8217;t really
fit into the first four.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 2]]></title>
    <link href="http://lusis.github.com/blog/2011/05/17/on-noah-part-2/"/>
    <updated>2011-05-17T18:38:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/17/on-noah-part-2</id>
    <content type="html"><![CDATA[<p><em>This is the second part in a series on Noah. Part 1 is available</em> <a href="http://goo.gl/l3Mgt">here</a></p>

<p>In part one of this series, I went over a little background about
ZooKeeper and how the basic Zookeeper concepts are implemented in Noah.
In this post, I want to go over a little bit about a few things that
Noah does differently.</p>

<!--more-->


<h2>Noah Primitives</h2>

<p>As mentioned in the previous post, Noah has 5 essential data types, four
of which are what I&#8217;ve interchangeably refered to as either Primitives
and Opinionated models. The four primitives are Host, Service,
Application and Configuration. The idea was to map some common use cases
for Zookeeper and Noah onto a set of objects that users would find
familiar.</p>

<p>You might detect a bit of Nagios inspiration in the first two.</p>

<ul>
<li><strong>Host:</strong>
  Analogous to a traditional host or server. The machine or instance running the operating system. Unique by name.</li>
<li><strong>Service:</strong>
  Typically mapped to something like HTTP or HTTPS. Think of this as the listening port on a Host. Services must be bound to Hosts. Unique by service name and host name.</li>
<li><strong>Application:</strong>
  Apache, your application (rails, php, java, whatever). There&#8217;s a subtle difference here from Service. Unique by name.</li>
<li><strong>Configuration:</strong>
  A distinct configuration element. Has a one-to-many relationship with Applications. Supports limited mime typing.</li>
</ul>


<p>Hosts and Services have a unique attribute known as <code>status</code>. This is a
required attribute and is one of <code>up</code>,<code>down</code> or <code>pending</code>. These
primitives would work very well integrated into the OS init process.
Since Noah is curl-friendly, you could add something globally to init
scripts that updated Noah when your host is starting up or when some
critical init script starts. If you were to imagine Noah primitives as
part of the OSI model, these are analagous to Layers 2 and 3.</p>

<p>Applications and Configurations are intended to feel more like Layer 7
(again, using our OSI model analogy). The differentiation is that your
application might be a Sinatra or Java application that has a set of
Configurations associated with it. Interestingly enough, you might
choose to have something like Tomcat act as both a Service AND an
Application. The aspect of Tomcat as a Service is different than the
Java applications running in the container or even Tomcat&#8217;s own
configurations (such as logging).</p>

<p>One thing I&#8217;m trying to pull off with Configurations is limited
mime-type support. When creating a Configuration in Noah, you can assign
a <code>format</code> attribute. Currently 3 formats or types are understood:</p>

<ul>
<li>string</li>
<li>json</li>
<li>yaml</li>
</ul>


<p>The idea is that, if you provide a type, we will serve that content back
to you in that format when you request it (assuming you request it that
way via HTTP headers). This should allow you to skip parsing the JSON
representation of the whole object and instead use it directly. Right
now this list is hardcoded. I have a task to convert this.</p>

<p>Hosts and Services make a great &#8220;canned&#8221; structure for building a
monitoring system on top of Noah. Applications and Configurations are a
lightweight configuration management system. Obviously there are more
uses than that but it&#8217;s a good way to look at it.</p>

<h2>Ephemerals</h2>

<p>Ephemerals, as mentioned previously, are closer to what Zookeeper
provides. The way I like to describe Ephemerals to people is a &#8216;512 byte
key/value store with triggers&#8217; (via Watch callbacks). If none of the
Primitives fit your use case, the Ephemerals make a good place to start.
Simply send some data in the body of your post to the url and the data
is stored there. No attempt is made to understand or interpret the data.
The hierarchy of objects in the Ephemeral namespace is completely
arbitrary. Data living at <code>/ephemerals/foo</code> has no relationship with
data living at <code>/ephemerals/foo/bar</code>.</p>

<p>Ephemerals are also not browseable except via a Linking and Tagging.</p>

<h2>Links and Tags</h2>

<p>Links and Tags are, as far as I can tell, unique to Noah compared to
Zookeeper. Because we namespace against Primitives and Ephemerals, there
existed the need to visualize objects under a custom hierarchy.
Currently Links and Tags are the only way to visualize Ephemerals in a
JSON format.</p>

<p>Tags are pretty standard across the internet by now. You might choose to
tag a bunch of items as <code>production</code> or perhaps group a set of Hosts and
Services as <code>out-of-service</code>. Tagging an item is a simple process in the
API. Simply <code>PUT</code> the name of the tag(s) to the url of a distinct named
item appended by <code>tag</code>. For instance, the following JSON posted to
<code>/applications/my_kick_ass_app/tag</code> with tag the Application
<code>my_kick_ass_app</code> with the tags <code>sinatra</code>, <code>production</code> and <code>foobar</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'>   <span class="p">{</span><span class="s2">&quot;tags&quot;</span><span class="o">:</span><span class="p">[</span><span class="s2">&quot;sinatra&quot;</span><span class="p">,</span> <span class="s2">&quot;production&quot;</span><span class="p">,</span> <span class="s2">&quot;foobar&quot;</span><span class="p">]}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Links work similar to Tags (including the act of linking) except that
the top level namespace is now replaced with the name of the Link. The
top level namespace in Noah for the purposes of Watches is <code>//noah</code>. By
linking a group of objects together, you will be able to (not yet
implemented), perform operations such as Watches in bulk. For instance,
if you wanted to be informed of all changes to your objects in Noah, you
would create a Watch against <code>//noah/*</code>. This works fine for most people
but imagine you wanted a more multi-tenant friendly system. By using
links, you can group ONLY the objects you care about and create the
watch against that link. So <code>//noah/*</code> becomes <code>//my_organization/*</code> and
only those changes to items in that namespace will fire for that Watch.</p>

<p>The idea is also that other operations outside of setting Watches can be
applied to the underlying object in the link as well. The name Link was
inspired by the idea of symlinking.</p>

<h2>Watches and Callbacks</h2>

<p>In the first post, I mentioned that by nature of Noah being
&#8220;disconnected&#8221;, Watches were persistent as opposed to one-shot.
Additionally, because of the pluggable nature of Noah Watches and
because Noah has no opinion regarding the destination of a fired Watch,
it becomes very easy to use Noah as a broadcast mechanism. You don&#8217;t
need to have watches for each interested party. Instead, you can create
a callback plugin that could dump the messages on an ActiveMQ Fanout
queue or AMQP broadcast exchange. You could even use multicast to notify
multiple interested parties at once.</p>

<p>Again, the act of creating a watch and the destination for notifications
is entirely disconnected from the final client that might use the
information in that watch event.</p>

<p>Additionally, because of how changes are broadcast internally to Noah,
you don&#8217;t even have to use the &#8220;official&#8221; Watch method. All actions in
Noah are published post-commit to a pubsub queue in Redis. Any language
that supports Redis pubsub can attach directly to the queue and
PSUBSCRIBE to the entire namespace or a subset. You can write your own
engine for listening, filtering and notifying clients.</p>

<p>This is exactly how the Watcher daemon works. It attaches to the Redis
pubsub queue, makes a few API calls for the current registered set of
watches and then uses the watches to filter messages. When a new watch
is created, that message is like any other change in Noah. The watcher
daemon sees that and immediately adds it to its internal filter. This
means that you can create a new watch, immediately change the watched
object and the callback will be made.</p>

<h2>Wrap up - Part Two</h2>

<p>So to wrap up:</p>

<ul>
<li>Noah has 5 basic &#8220;objects&#8221; in the system. Four of those are
opinionated and come with specific contracts. The other is a &#8220;dumb&#8221;
key/value store of sorts.</li>
<li>Noah provides Links and Tags as a way to perform logical grouping of
these objects. Links replace the top-level hierarchy.</li>
<li>Watches are persistent. The act of creating a watch and notifying on
watched objects is disconnected from the final recipient of the
message. System A can register a watch on behalf of System B.</li>
<li>Watches are nothing more than a set of filters applied to a Redis
pubsub queue listener. Any language that supports Redis and its
pubsub queue can be a processor for watches.</li>
<li>You don&#8217;t even have to register any Watches in Noah if you choose to
attach and filter yourself.</li>
</ul>


<p>Part three in this series will discuss the technology stack under Noah
and the reasoning behind it. A bit of that was touched on in this post.
Part four is the discussion about long-term goals and roadmaps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 1]]></title>
    <link href="http://lusis.github.com/blog/2011/05/16/on-noah-part-1/"/>
    <updated>2011-05-16T23:16:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/16/on-noah-part-1</id>
    <content type="html"><![CDATA[<p><em>This is the first part in a series of posts going over Noah</em></p>

<p>As you may have heard (from my own mouth no less), I&#8217;ve got a smallish
side project I&#8217;ve been working on called
<a href="https://github.com/lusis/Noah">Noah</a>.</p>

<!--more-->


<p>It&#8217;s a project I&#8217;ve been wanting to work on for a long time now and
earlier this year I got off my ass and started hacking. The response has
been nothing short of overwhelming. I&#8217;ve heard from so many people how
they&#8217;re excited for it and nothing could drive me harder to work on it
than that feedback. To everyone who doesn&#8217;t run away when I talk your
ear off about it, thank you so much.</p>

<p>Since I never really wrote an &#8220;official&#8221; post about it, I thought this
would be a good opportunity to talk about what it is, what my ideas are
and where I&#8217;d like to see it go in the future.</p>

<h1>So why Noah?</h1>

<p><em>fair warning. much of the following may be duplicates of information in
the Noah wiki</em></p>

<p>The inspiration for Noah came from a few places but the biggest
inspiration is <a href="http://goo.gl/WGCxY">Apache Zookeeper</a>. Zookeeper is one
of those things that by virtue of its design is a BUNCH of different
things. It&#8217;s all about perspective. I&#8217;m going to (yet again) paste the
description of Zookeeper straight from the project site:</p>

<pre><code>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.
</code></pre>

<p>Now that might be a bit confusing at first. Which is it? Is it a
configuration management system? A naming system? It&#8217;s all of them and,
again, it&#8217;s all about perspective.</p>

<p>Zookeeper, however, has a few problems for my standard use case.</p>

<ul>
<li>Limited client library support</li>
<li>Requires persistent connections to the server for full benefit</li>
</ul>


<p>By the first, I mean that the only official language bindings are C and
Java. There&#8217;s contributed Python support and Twitter maintains a Ruby
library. However all of these bindings are &#8220;native&#8221; and must be
compiled. There is also a command-line client that you can use for
interacting as well - one in Java and two C flavors.</p>

<p>The second is more of a showstopper. Zookeeper uses the client
connection to the server as in-band signaling. This is how watches
(discussed in a moment) are communicated to clients. Persistent
connections are simply not always an option. I can&#8217;t deploy something to
Heroku or AppEngine that requires that persistent connection. Even if I
could, it would be cost-prohibitive and honestly wouldn&#8217;t make sense.</p>

<p>Looking at the list of features I loved about ZK, I thought &#8220;How would I
make that work in the disconnected world?&#8221;. By that I mean what would it
take to implement any or all of the Zookeeper functionality as a service
that other applications could use?</p>

<p>From that thought process, I came up with Noah. The name is only a play
on the concept of a zookeeper and holds no other real significance other
than irritation at least two people named Noah when I talk about the
project.</p>

<p>So working through the feature list, I came up with a few things I
<strong>REALLY</strong> wanted. I wanted Znodes, Watches and I wanted to do it all
over HTTP so that I could have the broadest set of client support. JSON
is really the defacto standard for web &#8220;messaging&#8221; at this point so
that&#8217;s what I went with. Basically the goal was &#8220;If your language can
make HTTP requests and parse JSON, you can write a Noah client&#8221;</p>

<h1>Znodes and Noah primitives</h1>

<p>Zookeeper has a shared hierarchical namespace similar to a UNIX
filesystem. Points in the hierarchy are called <code>znodes</code>. Essentially
these are arbitrary paths where you can store bits of data - up to 1MB
in size. These znodes are unique absolute paths. For instance:</p>

<pre><code>//systems/foo/bar/networks/kansas/router-1/router-2
</code></pre>

<p>Each fully qualified path is a unique znode. Znodes can be ephemeral or
persistent. Zookeeper also has some primitives that can be applied to
znodes such as &#8216;sequence`.</p>

<p>When I originally started working on Noah, so that I could work with a
model, I created some base primitives that would help me demonstrate an
example of some of the use cases:</p>

<ul>
<li>Host</li>
<li>Service</li>
<li>Application</li>
<li>Configuration</li>
</ul>


<p>These primitives were actual models in the Noah code base with a strict
contract on them. As an example, Hosts must have a status and can have
any number of services associated with them. Services MUST be tied
explicity to a host. Applications can have Configurations (or not) and
Configurations can belong to any number of Applications or not.
Additionally, I had another &#8220;data type&#8221; that I was simply calling
Ephemerals. This is similar to the Zookeeper znode model. Originally I
intended for Ephemerals to be just that - ephemeral. But I&#8217;ve backed off
that plan. In Noah, Ephemerals can be either persistent or truely
ephemeral (not yet implemented).</p>

<p>So now I had a data model to work with. A place to store information and
flexibility to allow people to use the predefined primitives or the
ephemerals for storing arbitrary bits of information.</p>

<h1>Living the disconnected life</h1>

<p>As I said, the model for my implementation was &#8220;disconnected&#8221;. When
thinking about how to implement Watches in a disconnected model, the
only thing that made sense to me was a callback system. Clients would
register an interest on an object in the system and when that object
changed, they would get notified by the method of their choosing.</p>

<p>One thing about Watches in Zookeeper that annoys me is that they&#8217;re
one-shot deals. If you register a watch on a znode, once that watch is
triggered, you have to REREGISTER the watch. First off this creates, as
documented by the ZK project, a window of opportunity where you could
miss another change to that watch. Let&#8217;s assume you aren&#8217;t using a
language where interacting with Zookeeper is a synchronous process:</p>

<ul>
<li>Connect to ZK</li>
<li>Register watch on znode</li>
<li>Wait</li>
<li>Change happens</li>
<li>Watch fires</li>
<li>Process watch event</li>
<li>Reregister watch on znode</li>
</ul>


<p>In between those last two steps, you risk missing activity on the znode.
In the Noah world, watches are persistent. This makes sense for two
reasons. The first is that the latency between a watch callback being
fired and proccessed could be much higher than the persistent connection
in ZK. The window of missed messages is simply much greater. We could
easily be talking 100&#8217;s of milliseconds of latency just to get the
message and more so to reregister the watch.</p>

<p>Secondly, the registration of Watches in Noah is, by nature of Noah&#8217;s
design and as a byproduct, disconnected from the consumer of those
watches. This offers much greater flexibility in what watches can do.
Let&#8217;s look at a few examples.</p>

<p>First off, it&#8217;s important to understand how Noah handles callbacks. The
message format of a callback in Noah is simply a JSON representation of
the changed state of an object and some metadata about the action taken
(i.e. delete, create, update). Watches can be registered on distinct
objects, a given path (and thus all the children under that path) and
further refined down to a given action. Out of the box, Noah ships with
one callback handler - http. This means that when you register a watch
on a path or object, you provide an http endpoint where Noah can post
the aforementioned JSON message. What you do with it from there is up to
you.</p>

<p>By virtue of the above, the callback system is also designed to be
&#8216;pluggable&#8217; for lack of a better word. While the out of the box
experience is an http post, you could easily write a callback handler
that posted the message to an AMQP exchange or wrote the information to
disk as a flat file. The only requirement is that you represent the
callback location as a single string. The string will be parsed as a url
and broken down into tokens that determine which plugin to call.</p>

<p>So this system allows for you to distribute watches to multiple systems
with a single callback. Interestingly enough, this same watch callback
system forms the basis of how Noah servers will share changes with each
other in the future.</p>

<h1>Wrap up - Part 1</h1>

<p>So wrapping up what I&#8217;ve discussed, here are the key take aways:</p>

<ul>
<li>Noah is a &#8216;port&#8217; of specific Zookeeper functionality to a
disconnected and asynchronous world</li>
<li>Noah uses HTTP and JSON as the interface to the server</li>
<li>Noah has both traditional ZK-style Ephemerals as well as opinionated
Primitives</li>
<li>Noah uses a pluggable callback system to approximate the Watch
functionality in Zookeeper</li>
<li>Clients can be written in any language that can speak HTTP and
understand JSON (yes, even a shell script)</li>
</ul>


<h1>Part 2 and beyond</h1>

<p>In part two of this series we&#8217;ll discuss some of the additions to Noah
that aren&#8217;t a part of Zookeeper such as Tags and Links. Part 3 will
cover the underlying technology which I am intentionally not discussing
at this point. Part 4 will be a roadmap of my future plans for Noah.</p>
]]></content>
  </entry>
  
</feed>
