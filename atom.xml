<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[blog dot lusis]]></title>
  <link href="http://lusis.github.com/atom.xml" rel="self"/>
  <link href="http://lusis.github.com/"/>
  <updated>2014-06-17T00:54:43-04:00</updated>
  <id>http://lusis.github.com/</id>
  <author>
    <name><![CDATA[John E. Vincent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[PaaS for Realists]]></title>
    <link href="http://lusis.github.com/blog/2014/06/14/paas-for-realists/"/>
    <updated>2014-06-14T23:12:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/06/14/paas-for-realists</id>
    <content type="html"><![CDATA[<p>I realize I was pretty down on PaaS the past couple of days. Lest I send the wrong message, I figure a clarification is in order</p>

<!-- more -->


<p>Before we start, we should define some things. It&#8217;s always important to be on the same page:</p>

<ul>
<li>PaaS: Platform as a Service (no distinction between public or private). When talking about public, it&#8217;s usually Heroku.</li>
<li>Private PaaS: A PaaS run &#8220;in-house&#8221;. I&#8217;m using &#8220;in-house&#8221; loosely here. You could be running this on top of AWS for all I care. You&#8217;re running it yourself. There are a lot of players in this space but the biggest name is CloudFoundry. There&#8217;s also OpenShift and then the plethora of docker-based ones like Deis and Flynn.</li>
<li>Affinity: Definable placement policies for where applications run. I use this liberally to refer to both affinity and anti-affinity. Basically &#8220;I want this to run next to this&#8221; vs. &#8220;I don&#8217;t want this to run next to this&#8221;</li>
<li>Production: Business critical functions that warrant &#8220;waking someone up at 2AM&#8221;</li>
<li>container: linux containers. Nothing else.</li>
<li>docker: a specific container packaging format and ecosystem</li>
</ul>


<p>I also want to be clear that I <em>ONLY</em> care about production workloads. The reason I defined production the way I did is because only the end user can define the business criticality of a service or system function. If you consider an idle bench of engineers unable to work because your build farm is down a bad thing, then your build farm is &#8220;production&#8221;.</p>

<p>I also want to point out that I inherently belive that a Private PaaS is probably a really good thing for your business. My argument is largely that most enterprises are not ready for it and are not willing to live with the shift it will require.</p>

<h1>Benefits of a PaaS</h1>

<p>Let&#8217;s go with the good news first. A PaaS (and more importantly a private PaaS) has a lot of benefits.</p>

<ul>
<li>It can simplify deployment models.</li>
<li>It can unify the workflow of development and production deployments.</li>
<li>It can codify a framework by which you develop your applications</li>
<li>Generally speaking, you also get a more consistent operational surface for your applications</li>
<li>Can create a culture and framework for self-service</li>
</ul>


<h2>Simplified deployment and unified workflow</h2>

<p>Most private PaaS solutions tend to follow the Heroku model of a PaaS. In the Heroku model, You follow a normal development workflow using your VCS as a model for deployment:</p>

<ul>
<li>Develop code</li>
<li>Test code</li>
<li>Push code to Heroku remote for testing</li>
</ul>


<p>That&#8217;s the sum of the deployment. Your deployment to production is no different.</p>

<h2>Codified framework</h2>

<p>With Heroku, the &#8220;moving bits&#8221; of your stack are hidden from you. You don&#8217;t stand up a database in the traditional sense. You tell Heroku to wire up the database to your application as an add-on. Heroku exposes this add-on to your stack via environment variables. You reference those environment variables in your application code instead of hard-coded settings in property or yaml files. Both CloudFoundry and OpenShift follow the same model:</p>

<ul>
<li>Create a service</li>
<li>Bind the service</li>
<li>Update the application to use the service</li>
</ul>


<p>This is really awesome from a development perspective. You simply define the building blocks of your application, tell the platform to expose them to your application and off you go.</p>

<h2>Operational Surface</h2>

<p>From an operations perspective, a private PaaS can create a consistent operational surface area. You spend less time worrying about individual operating systems. Your &#8220;host&#8221; nodes are largely a uniform target. Most of the private PaaS products ship with most common services prebuilt and ready to wire up to applications.</p>

<p>I would even argue that a private PaaS even simplifies the security model a bit as the concept of users are less relevant and much of the PaaS tooling is, by nature, provides proxy access to the things the developers need. Both of the dominant private PaaS solutions leverage kernel cgroups for resource management. Cloudfoundry uses LXC for isolation while OpenShift uses SELinux and MCS currently. I believe, however, that OpenShift is migrating to LXC as well.</p>

<h2>Self-service culture</h2>

<p>Once your private PaaS is up and running, your development team is unleashed to deploy whatever and whenever they want. They aren&#8217;t waiting around for a full-fledged OS to be provisioned that needs further configuration just to be servicable. Developers are free to experiment with arbitrary components (provided they exist in the PaaS service catalog).</p>

<h1>Rainbows and Unicorn Piss</h1>

<p>However not everything is rosy and bright in the land of the private PaaS. There are downsides as well - some cultural and some technical</p>

<h2>Application Readiness</h2>

<p>Unless your private PaaS is a bespoke solution, you WILL have to change your application model. You cannot simply forklift an application into a PaaS. Your application must be designed not only to work in a PaaS environment but also to work in a specific PaaS.</p>

<p>Most traditional &#8220;enterprise&#8221; applications are not ready for a PaaS solution. Many will have to be significantly rewritten. The most common model for this is the <a href="http://12factor.net/">12 Factor Application</a>.</p>

<p>I should state up front that I disagree with quite a few bits of the 12 Factor model. It&#8217;s important to remember that, imho, the 12 Factor model was designed as a business strategy for Heroku. The steps follow exactly what makes an app work best on Heroku, not what is best for an application.</p>

<p>Regardless, as the private PaaS solutions are largely modeled on Heroku you might was state that 12 Factor is the way you&#8217;ll need to design your application.</p>

<h2>Magical autoscaling</h2>

<p>As I said in my previous post, this really doesn&#8217;t exist. Your application has to be DESIGNED to scale this way. As Adrian Cockcroft pointed out in the comments to my previous post Netflix &#8220;overallocated&#8221; on the dependency side up front to minimize the need and impact of things like rebalancing data and load balancer scaling. It&#8217;s also worth noting that Netflix did NOT use a PaaS (though arguably the model for how they used AWS was PaaS-ish).</p>

<p>Most &#8220;enterprise&#8221; applications I&#8217;ve dealt with never scaled cleanly. They needed things like sticky sessions and made assumptions about data access paths. Quite frankly they also were not designed for this level of deployment volatility. I would go even further and say that if you have a release cycle measured in months, don&#8217;t bother.</p>

<h2>Magical Autorecover</h2>

<p>Just like autoscaling, this is also not what you think it is. Unless your application maintains exactly ZERO state, then you will never see this benefit. Do you write files to critical files to disk in your application? Yep those are gone when you &#8220;magically autorecover&#8221;. The autorecovery that was promised you? It redeploys your application. Your state is lost and no you don&#8217;t have NFS or shared storage or anything to fall back to. Get used to shoving your blobs in your database. Oh but what if your database fails?</p>

<p>This is where it gets interesting. I&#8217;m still sussing out the recovery models for the two primary players in this space but most likely you will LOSE that data and have to restore from a backup. I&#8217;m sure someone will call me on this and I&#8217;m willing to listen but I do know for a fact that the autofailover model of things like your MySQL instance depend on migratable or shared storage (at least from my reading of the docs).</p>

<p>This all of course leads me to the next part</p>

<h1>Technical Requirements</h1>

<p>I alluded to this earlier but there are technical requirements that most companies are simply not ready for.</p>

<h2>Distributed Systems</h2>

<p>All applications are inherently distributed systems even if you don&#8217;t want to admit it. However a PaaS is more so than most shops are ready for. Let&#8217;s run down the components for the current version of <a href="http://docs.cloudfoundry.org/concepts/architecture/">CloudFoundry</a>. I count 11 distinct components. If we move over to <a href="http://openshift.github.io/documentation/oo_system_architecture_guide.html">OpenShift</a> I count 4 components.</p>

<p>Both of these applications use a service router, a message bus, a data store and <code>n</code> number of actual nodes running the deployed applications. In both cases, the documentation for these components requires you to already know how to scale and maintain these components. There are any number of places where these stacks can fall apart and break and you will need to be an expert in all of them.</p>

<p>Also one of the more hilarious bits I&#8217;ve found is the situation with DNS. I can&#8217;t count the number of shops where DNS changes where things like wildcard DNS were verboten. Good luck with the PaaS dyndns model!</p>

<h2>Operational Immaturity</h2>

<p>To be clear while I feel that most organizations aren&#8217;t ready for the operational challenges of maintaining a PaaS, the job is made harder by the PaaS software. In both cases, the operational maturity of the products themselves simply isn&#8217;t there.</p>

<p>Look at the &#8220;operators&#8221; documentation <a href="http://docs.gopivotal.com/pivotalcf/concepts/high-availability.html">here</a> for CloudFoundry HA. I can sum it up for you pretty easily:</p>

<blockquote><p>GL;HF</p></blockquote>


<p>Basically they punt everything over to you as if to say &#8220;Fuck if we know. Use that thing you sysadmin types use to make shit redundant.</p>

<p>And lest you think OpenShift is any better, OpenShift uses MongoDB with this nice bit of information:</p>

<blockquote><p>&#8220;All persistent state is kept in a fast and reliable MongoDB cluster.&#8221;</p></blockquote>


<p>What I&#8217;m about to say I stand behind 100%. Any company that tells you that MongoDB is &#8220;reliable&#8221; is basically saying:</p>

<ul>
<li>We have no idea what we&#8217;re talking about</li>
<li>We know f-all about operations</li>
<li>We hate you</li>
</ul>


<p>Any tool that uses MongoDB as its persistent datastore is a tool that is not worth even getting started with. You can call me out on this. You can tell me I have an irrational dislike of MongoDB. I don&#8217;t care. Having wasted too much time fighting MongoDB in even the most trivial of production scenarios I refuse to ever run it again. My life is too short and my time too valuable.</p>

<p>Additionally I&#8217;ve found next to zero documentation on how a seasoned professional (say a MySQL expert) is expected to tune the provisioned MySQL services. The best I can gather is that you are largely stuck with what the PaaS software ships in its service catalog. In the case of OpenShift you&#8217;re generally stuck with whatever ships with RHEL.</p>

<p>Another sign of operational immaturity I noticed in OpenShift is that for pushing a new catalog item you actually have to RESTART a service before it&#8217;s available.</p>

<h2>Disaster Recovery</h2>

<p>After going over all the documentation for both tools and even throwing out some questions on twitter, disaster recovery in both tools basically boils down to another round of &#8220;good luck; have fun&#8221;.</p>

<p>Let&#8217;s assume your PaaS installation is a roaring success. You&#8217;ve got every developer in your org pushing random applications out to production. Self-service is the way of life. We&#8217;ve got databases flying all over the place.</p>

<p>How do you back them all up? Well this is a PaaS, Bob. It&#8217;s all about self-service. The developers should be backing them up.</p>

<p>WAT.</p>

<p>Again based on the research I&#8217;ve done (which isn&#8217;t 1000% exhaustive to be fair), I found zero documentation about how the administrator of the PaaS would back up all the data locked away in that PaaS from a unified central place. If your solution is to tell me that Susan&#8217;s laptop is where the backups of our production database lives, I&#8217;m going to laugh at you.</p>

<h2>Affinity</h2>

<p>Affinity issues make the DR scenario even MORE scary. I have no way of saying &#8220;don&#8217;t run the MySQL database on the same node as my application&#8221;. This makes the risk surface area even more large. Combine that with the fact that a single host could be running multiple business critical applications. I realize that these tools have algos that are supposed to handle this for you but I&#8217;ve not seen any sort of policy enforcement mechanism for that in the documentation.</p>

<h1>So what&#8217;s the answer?</h1>

<p>I don&#8217;t think ANY of the current private PaaS solutions are a fit right now. OpenShift is, imho, built on unsound ground. CloudFoundry in its current Ruby form is a mess of moving parts. In fairness CloudFoundry is going through a rewrite with some firm leadership behind it that I have quite a bit of faith in when it comes to operational concerns.</p>

<p>Additionally both tools are embracing containers and docker packaging to increase security but none of the tools offer, as far as I can tell, anything resembling a hybrid model. I don&#8217;t trust docker storage containers yet personally.</p>

<p>And I want to be clear. I&#8217;m not trying to be a BOFH here with all my talk of &#8220;placement policy&#8221; and &#8220;disaster recovery&#8221;. I fully embrace the idea of a private PaaS. I simply don&#8217;t embrace it in any of the current ecosystem. Even a modicum of due diligence should rule them both out until they address what are basic business sanity checks. These platforms require real operations to run and maintain. If you&#8217;re still throwing things over the wall to your operations team to deploy into your PaaS then you really haven&#8217;t gained anything. Unless your engineering organization is willing to step up to the shared responsibility inherent in a PaaS, then you definitely aren&#8217;t ready. Until then, your time and money is better spent optimizing and standardzing your development workflow and operational tooling to build your own psuedo-PaaS.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[There's no konami code for operations]]></title>
    <link href="http://lusis.github.com/blog/2014/06/13/no-konami-for-operations/"/>
    <updated>2014-06-13T23:45:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/06/13/no-konami-for-operations</id>
    <content type="html"><![CDATA[<p>up up down down left right left right b a select start</p>

<!-- more -->


<p>I went on a bit of a rip today about all sorts of technology. I figured I should at least clarify some of it in long form.</p>

<h2>Vmotion/live migration technologies</h2>

<p>Vmotion is a scam. I have frequently said that only trivial workloads are safe for vmotion. Here&#8217;s the reasoning:</p>

<ul>
<li>live migration realistically requires the workload on the vm being migrated to be quiesced.</li>
<li>any workload that can be quiesced with no impact is most likely a trivial workload</li>
<li>trivial workloads don&#8217;t NEED the benefit of live migration</li>
<li>to that end, you&#8217;re basically paying a lot of money to allow a system to copy its files from one node to another</li>
</ul>


<p>Let&#8217;s also not forget that live migration claims to have accomplished a lot of things such as time travel. You may also know live motion technology by its other names like:</p>

<ul>
<li>Dude where&#8217;s my clock?</li>
<li>I paused your workload for you but you didn&#8217;t notic&#8230;hey why did I just get a network partition in my cluster?</li>
</ul>


<p>But hey it demos really well when you can keep watching that streaming video while your vm is moved from host to another.</p>

<h2>Autoscaling</h2>

<p>Autoscaling is a myth. My reasoning behind this has similarities with vmotion/live migration.
Again we have a set of things we need to clarify:</p>

<ul>
<li>horizontal vs vertical autoscaling</li>
<li>triviality of the workload being autoscaled</li>
<li>workload support for autoscaling</li>
<li>scale up vs scale down</li>
</ul>


<p>I am not concerns with trivial workloads. Trivial workloads are&#8230;well&#8230;trivial. The largely cached static marketing website takes no effort whatsoever to scale. Oh look I just brought up a new server with the same static content! Instant capacity!</p>

<p>Let&#8217;s take a standard architecture here:</p>

<ul>
<li>caching (memcache)</li>
<li>frontend</li>
<li>load balancer</li>
<li>database (this applies to traditional RDBMS as well as NoSQL)</li>
</ul>


<p>When I &#8220;autoscale&#8221; my caching layer, I now have to concern myself with the following things:</p>

<ul>
<li>hashing algos</li>
<li>cache miss increase</li>
</ul>


<p>So sure, feel free to autoscale that group of memcache servers but your performance just went to through the floor. Now you&#8217;ve had a downstream affect on your database as you&#8217;re having to go to origin due to cache misses. Oh and by the way not all servers talking to the caching tier saw the same topology so now you&#8217;ve got possibly incorrect data you&#8217;re serving from the cache</p>

<p>When I autoscale my frontend, I&#8217;ve now add <code>n</code> number of connections to the database. How&#8217;s that network looking? Oh wait did you just autoscale to the point of starving the database of resources? Have you possible shot yourself in the foot because now the stuff that was working before is getting rejected because of connection counts?</p>

<p>Autoscaling load balancers is also a problem as you now have the issue of topology mismatch of your backends as well as dealing with session injection that was PREVIOUSLY handled by only one LB.</p>

<p>Finally let&#8217;s get to autoscaling our database. Vertical or horizontal, relational or &#8220;NoSQL&#8221; it doesn&#8217;t matter. If you vertically scale your DB, do you have to restart the process with larger memory allocations? What about rebalancing of data when you scale horizontally?</p>

<p>And we&#8217;ve not even gotten to if your application is actually ABLE to be autoscaled (are you entirely stateless? 12 factor friendly?).</p>

<p>Combining these things along with unmentioned downstream impacts and transitive dependencies, means that in most cases when you need to autoscale you won&#8217;t be able to respond to the workload for some time. It&#8217;s possible that AFTER that time has passed, the workload may be gone.</p>

<p>And let&#8217;s not even talk about trying to unwind all that madness via scale down.</p>

<h1>private PaaS</h1>

<p>This is almost the most egregious of them all. It ranks right up there with &#8220;private cloud&#8221; in the bullshit-o-meter department. Resources are not infinite.
I have a bit of a guidepost I use when thinking about &#8220;new&#8221; functionality in applications.</p>

<ul>
<li>what functionality am I adding?</li>
<li>is it my core competency</li>
<li>what does the landscape of existing ecosystem look like for that functionality</li>
<li>how successful is that landscape for that functionality</li>
</ul>


<p>In most cases, the landscape is either saturated with business whose core focus and expertise is on that concept/functionality. In worst cases the majority of the businesses in that space are failing. Think long and hard about if you have the expertise to do this thing.</p>

<p>PaaS and IaaS are in the same boat. I&#8217;m a bit more harsh on the IaaS front as I truly believe that if an operations team had been able to deliver on the promise of virtualization but couldn&#8217;t (for any reason) then sticking ANOTHER layer on top isn&#8217;t going to magically make it work. The platform still has real hardware under the covers that has the same limitations it had before - bandwidth capcity, io, patching of hypervisors. This stuff doesn&#8217;t just disappear. In many cases you can actually hit a wall VERY early on for capacity issues.</p>

<p><a href="http://www.slideshare.net/TimMackey/hypervisor-31754727">This slide deck about Cloudstack Hypervisor choices</a> is an amazing read for understanding limitations. Some of these are actually imposed by the hypervisor:</p>

<ul>
<li>Max VLANs</li>
<li>Max Storage</li>
<li>Max vms per hypervisor</li>
<li>Max hypervisors per pool</li>
</ul>


<p>There&#8217;s no cheat code for this shit, folks. Very few PaaS and IaaS products tackle operational issues at all. Time to first success is important but not if it comes at the expense of cost to operate over time. Yes, I can sudocurlbash your product on to my system but that&#8217;s trivial. How do I deal with:</p>

<ul>
<li>Backups</li>
<li>Affinity/Anti-affinity issues</li>
<li>Upgrades</li>
<li>Supportability</li>
</ul>


<p>IaaS actualy isn&#8217;t as bad as a PaaS in some of those but they both have issues. PaaS is mainly worse because you have adopt into an ecosystem and philosophy (not that 12 factor isn&#8217;t entire good but it&#8217;s a start) if you want to have any real success.</p>

<p>You will <em>NOT</em> forklift a workload into one of these models and be successful in the long term. In the short term you will simply have given someone else a lot of your money.</p>

<p>I drove a forklift for two years. I know forklifts.</p>

<h1>End rant</h1>

<p>I&#8217;m not saying that people can&#8217;t be successful at these things. Clearly they are to some degree. But that&#8217;s only the public face. How much shit did they have to wade through to make this work? Where are the bits of baling wire, duct tape and a healthy belief in a higher power that keep it from just failling over the edge of the abyss?</p>

<p>Anyway that&#8217;s just a short list of things from a very tired and worn out person with less hair than he started the day with. I know I shouldn&#8217;t get mad about this stuff but it&#8217;s hard when the people trying to smokescreen you ARE you in a sense (professionally speaking). Frankly I&#8217;m just tired of people thinking that the operational aspects of this stuff are irrelevant because somebody promised them &#8220;autoscaling selfhealing magical rainbow-colored unicorn piss in a bottle&#8221; where they didn&#8217;t have to interact with operations folks ever again.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[omnibus-redux]]></title>
    <link href="http://lusis.github.com/blog/2014/04/13/omnibus-redux/"/>
    <updated>2014-04-13T23:01:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/04/13/omnibus-redux</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been a pretty big proponent of omnibus. I still think it&#8217;s the right way to go but recent changes have removed the primary reason for recommending it</p>

<!-- more -->


<p>I did a lot of evangelism for <a href="https://github.com/opscode/omnibus-ruby">omnibus</a> last year. Presentations, blogposts, a sysadvent article. It is/was a great tool however it no longer fits the primary usecase.</p>

<h1>Original workflow</h1>

<p>Originally the biggest benefit to omnibus (outside of the core of what it did) was the Vagrantfile it generated. Because of this Vagrantfile, I could generate a project and publish the repo for anyone to use. That person didn&#8217;t have to have any ruby tooling installed. They just needed vagrant and two plugins (<code>vagrant-omnibus</code> and <code>vagrant-berkshelf</code>).</p>

<p>They could check out the repository and just run <code>vagrant up</code> and the packages would be nice and neat dropped off in the <code>pkg</code> directory locally. I didn&#8217;t see this as a problem workflow because I didn&#8217;t listen to my own advise.</p>

<p>This is from the original generated README of a fresh omnibus project:</p>

<p><img src="http://s3itch.lusis.org/1wzxjV.png" alt="original README" /></p>

<p>This was the part of the workflow I was bullish on. In fact we went whole hog internally with this. Anyone could contribute because they could test locally with only the exising tools that we already had installed.</p>

<p>But that seems to have all changed with Omnibus version 3. Now omnibus requires a full ruby development environment just to do what it previously did with a <code>Vagrantfile</code> alone.
The reason for this is that instead of Vagrant, now omnibus uses <a href="https://github.com/test-kitchen/test-kitchen">test-kitchen</a>. Additionally it seems to ALSO require <code>Berkshelf</code> locally now.</p>

<p>This is where it gets really ugly.</p>

<p>It doesn&#8217;t just require Berkshelf. It requires an UNRELEASED version of Berkshelf.</p>

<h2>Sidebar on tooling</h2>

<p>I wanted to take a minute to talk a little bit about Chef tooling.</p>

<p>There is evidently a shift going on in the Chef community and I apparently haven&#8217;t been keeping up. The Chef community flocked to Berkshelf for reasons I don&#8217;t understand. It evidently solved a problem I didn&#8217;t have. You see I used chef-client and knife (with several plugins). I work with a lot of folks who are NOT ruby or chef people. For us, Chef is a means to an end. It&#8217;s a tool just like Maven or Artifactory. We use chef-solo as the installer for our platform, for instance. We are not ruby developers or users. All of our tooling is either in Python or Java and our application code base is in Java.</p>

<p>Opscode (or rather Chef) has previously made a big push to make being a Chef user mean not being a ruby expert. There seems to have been not only a shift in that thinking but also in how the tools are to be used.</p>

<p>A good example is chef-metal. This originally confused me because this was the chef model:</p>

<ul>
<li><code>knife</code> is for your workstation</li>
<li><code>chef-client</code> is for your servers</li>
</ul>


<p>With <code>chef-metal</code> that changes a bit because the understanding is that where you might use <code>knife rackspace &lt;blah&gt;</code> you&#8217;ll now run <code>chef-client recipe[rackspace_servers]</code>.</p>

<p>So back to berkshelf and other tools&#8230;</p>

<p>Before these were optional. Slowly but surely they&#8217;re becoming NOT optional. The problem with this is as I described above. No longer is the workflow:</p>

<ul>
<li>install knife</li>
<li>checkout our chef repo</li>
<li>edit/upload cookbooks</li>
</ul>


<p>It&#8217;s now become a ruby developer workflow of somekind. I don&#8217;t have a cookbook directory. All of my cookbooks are somewhere in <code>~/.berkshelf</code> and I&#8217;m expected to have every cookbook be its own repo or something. I still don&#8217;t fully understand what&#8217;s going on here and frankly I don&#8217;t have the time. I have chef novices on my team and I don&#8217;t have any official documentation to point them at because this is something that exists outside of the official chef documentation.</p>

<p>I&#8217;m not trying to slander ANY of these tools. I&#8217;m sure they&#8217;re all wonderful. <code>test-kitchen</code> looks great for being able to break away from tying the provisoner to vagrant but again that&#8217;s not the workflow that works for us (or frankly anyone who just wants to use chef). My argument is simply that if these things are going to be the defacto model then they should be rolled into Chef somehow and be documented on the official documentation.</p>

<h1>So back to omnibus</h1>

<p>So we have two issues here that make omnibus not a fit anymore:</p>

<ul>
<li>Requires a full blown ruby environment to <strong>BUILD</strong> a project whereas before it only required a full blown ruby environment to <strong>CREATE</strong> a project.</li>
<li>the 3.x RELEASE of Omnibus had a dependency on an <strong>UNRELEASED</strong> artifact</li>
</ul>


<p>That second one is really painful to swallow. Quite frankly it&#8217;s just poor form to do that. You can argue about version numbers being meaningless or &#8220;it&#8217;s stable just still in beta&#8221; but when you&#8217;re asking someone to use and depend on your tools it&#8217;s just not right. If your dependencies aren&#8217;t released yet then you don&#8217;t get to release. Let&#8217;s also not forget that EVERY anicllary add-on in the Chef world seems to have its own dependency on Berkshelf now.</p>

<h1>How did this all come to a head</h1>

<p>When Heartbleed hit, I needed to rebuild our two big omnibus packages. Recently I had switched over to a new laptop and didn&#8217;t yet have anything checked out. This was fine because I had the omnibus projects checked out on my desktop. It was running a 1.4 release of vagrant and was where I did most of my builds before. So we generated new packages and were happy.</p>

<p>We also have new team members on our ops team. I was using this as an opportunity to show them the omnibus packages and let them build them as well. So I tell them to check out the repos, make sure they have the plugins installed and run <code>vagrant up</code>. This didn&#8217;t work and it turns out somebody had vagrant 1.5 installed. No big deal I think. We&#8217;ll punt on that one and just make a note that we&#8217;ll need a new <code>vagrant-berkshelf</code> plugin when it&#8217;s released.</p>

<p>But yesterday I went to work on a massive refactor of our omnibus packages since we&#8217;re cleaning up a bunch of extras and changing things around. I knew that omnibus 3 had several things to make the whole build process go faster. It also allowed me greater control in build determinism. So I upgrade and generate a new project to see what the new layout looks like and test the builds. When I realize there&#8217;s no Vagrantfile, I&#8217;m really confused. The readme says a Vagrantfile would be generated.</p>

<p>That set off the things I tweeted and posted on the mailing list. In the end it came down to me evidently relying on something I was never supposed to rely on and being told I should learn to RTFM.</p>

<h1>So where does that leave things?</h1>

<p>Right now I have to take back everything I said about omnibus. It&#8217;s not that I don&#8217;t think it&#8217;s a great tool and I certainly don&#8217;t give two shits about getting subtweeted. I still think it&#8217;s a great tool and I think the idea is the right one.</p>

<p>However the main reason I recommended omnibus and bothered to integrate it is gone. It&#8217;s simply not the straightforward process it was and the removal of the vagrant build lab puts too much on the non-ruby-ecosystem user. This is where I didn&#8217;t listen to my own advice. I frequently warned people that omnibus exists first and foremost for the needs of Chef. We got lucky that it worked this long and it was an awesome ride.</p>

<p>I&#8217;m still trying to figure out the best way to get BACK to the vagrant build-lab but it&#8217;s not working out so well. I&#8217;m attempting to rebuild my <a href="https://github.com/lusis/omnibus-omnibus-omnibus">omnibus-omnibus-omnibus</a> project to ship everything needed but now I&#8217;m back to stepping outside the community framework and making people who wanted to just create packages needing something extra I created.</p>

<h1>One last thing</h1>

<p>I&#8217;m not posting this expecting anyone to change anything in Omnibus and I&#8217;m not trying to be passive-aggressive. I&#8217;m not entitled to anything from anyone. This is more about providing something I can link to for users of the omnibus builds I&#8217;ve already published since they will no longer work out-of-the-box.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stop fighting distros - part 2]]></title>
    <link href="http://lusis.github.com/blog/2013/09/23/stop-fighting-distros-part-2/"/>
    <updated>2013-09-23T23:27:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/09/23/stop-fighting-distros-part-2</id>
    <content type="html"><![CDATA[<p>I&#8217;m a pretty harsh person. My wife and I have this discussion all the time about how things I say get interpreted. As the communicator, this responsibility lies squarely on my shoulders.</p>

<p>So before I start, I don&#8217;t &#8220;hate&#8221; distributions or the packaging format they use or the people doing the work. To this even of you who toil to track countless security reports or maintain some software package in an upstream repository because of your love for that software, here&#8217;s to you.</p>

<!-- more -->


<h2>Packaging policies</h2>

<p>In general I hate very few things. When I say I hate something usually hate is even too strong of a word.</p>

<p>I <strong>really</strong> hate stupid policies. I hate stupid policies that are predicated on the shape of a reality that either no longer exists or never existed in the first place. I hate policies that never evolve to the reality of the world. I hate policies that throw pragmatism and common sense out the window. I also really hate ego-driven policies.</p>

<p>What kinds of things fit that bill?</p>

<ul>
<li><em>most</em> corporate security policies</li>
<li>Various parts of PEP8</li>
<li>a oddly large amount of government regulation and legislation</li>
<li>FHS</li>
</ul>


<p>And yes, distro packaging policies (mostly) fit that bill.</p>

<p>I&#8217;ve said this a lot recently (and I stand by it):</p>

<blockquote><p>Distribution packaging policies are not designed for people who package software</p></blockquote>


<p>Packaging policies exist first and foremost for the benefit of the distribution. This isn&#8217;t a bad thing or a good thing - it&#8217;s just a thing.</p>

<p>Let&#8217;s compare a few packaging policies from different distros. I spent most of my valuable time this evening between fighting a 3 year old who didn&#8217;t want to go to sleep reading these.</p>

<ul>
<li><a href="http://www.debian.org/doc/debian-policy/">Debian</a></li>
<li><a href="https://fedoraproject.org/wiki/Packaging:Guidelines?rd=Packaging/Guidelines">Fedora</a></li>
<li><a href="https://wiki.archlinux.org/index.php/Arch_Packaging_Standards">Arch</a></li>
<li><a href="https://devmanual.gentoo.org/general-concepts/index.html">Gentoo</a></li>
</ul>


<p>Ignoring for a minute the sheer verbosity of some of these guidelines (I think Arch wins at simplicity and pragmatism), they all share a few common key themes:</p>

<ul>
<li>Break shit up into multiple packages</li>
<li>It needs to be able to be built from source</li>
<li>put X here, Y here, Z here</li>
<li>ONLY 1 VERSION IS ALLOWED</li>
<li><em>These guidelines exist to make maintaining the distribution easier</em></li>
</ul>


<p>Note that of all the distributions Debian is quite possibly the most offensive at this. Fedora follows a close second.</p>

<p>I want to address a few of these though I might have done so in the previous version of this post. I&#8217;m also going to skip the &#8220;build from source&#8221; point because I have no real problem with that on a general level.</p>

<h3>Break shit into multiple packages</h3>

<p>There are two arguments here that tend to crop up in support of this:</p>

<ul>
<li>security</li>
<li>disk space</li>
</ul>


<p>Disk space concerns are entirely subjective. I&#8217;m not hurting for disk space and I haven&#8217;t been for quite some time. I&#8217;m anal about which packages I install, not because of disk space, but because I want to have to worry about as few security announcements as possible. Quite honestly this argument is the distro equivalent of PEP8&#8217;s column width. It is not my concern how many ISOs the distribution has to span across. I either install from a private mirror or netboot off the internet. Regardless I haven&#8217;t put an actual cd in a system in a VERY long time.</p>

<p>But the one that gets me is the &#8220;security&#8221; argument. The basis for this is that you can supposedly immediately get the benefit of a fix to, say openssl, in all packages that link against openssl.</p>

<p>This is bullshit.</p>

<p>For that to REALLY be true, you have to turn up the white noise reality distortion field pretty high.</p>

<p>Let&#8217;s take a vulnerability in a library that everything in the system links to like openssl. There have been <a href="http://www.openssl.org/news/vulnerabilities.html">three CVEs</a> against openssl this year (2 of which affect versions that most distributions use today).</p>

<p>For a distribution to get a fix in place for openssl, they should ostensibly be testing EVERY SINGLE PACKAGE that links against openssl. In reality, you can get by with lesser testing of that fix assuming the ABI doesn&#8217;t change but that&#8217;s still a pretty big risk to take. I&#8217;m honestly not sure (and would love feedback) from distros about exactly what the test cycle is for these cases.</p>

<p>But let&#8217;s assume they do test that. If the vulnerability was &#8220;responsibly disclosed&#8221; it&#8217;s possible they&#8217;ve had time to do that but if not, you have a case where every single package on your system that links against openssl could be vulnerable.</p>

<p>So at most, what you have is a wash. I would actually argue that a package that brings its own deps is better off in this case.</p>

<p>Let&#8217;s also not ignore the case that <strong>RedHat 5 STILL USES RUBY 1.8.4 WHICH DOES NOT GET ANY FIXES UPSTREAM WHATSOEVER.</strong> I don&#8217;t think ANY ruby 1.8 version is getting fixes anymore. You are ENTIRELY dependent on Redhat to not only backport fixes from different versions of Ruby but also make sure they don&#8217;t break any other Ruby packages the distro ships. In some cases, Redhat will actually have to figure out how to patch that version of Ruby.</p>

<h3>File placement</h3>

<p>The whole &#8220;put X here, Y here, Z here&#8221; is really about consistency more than anything. I don&#8217;t entirely disagree with it but it really is a matter of preference. I personally like my configs to live close to my application. I hate bouncing around the filesystem.</p>

<h3>Version restrictions</h3>

<p>This is where it gets ugly and when combined with the &#8220;use existing libraries&#8221; model that the whole policy is downright combative to people who write software.</p>

<p>I&#8217;ve said previously that various language communities have coalesced around their own toolchains. One thing that&#8217;s pretty common, however, is that those communities allow for multiple concurrent versions of a given package. How they isolate and define which version to use is specific to the language but it exists.</p>

<p>Shoehorning that into the vendor policy around versioning simply does not work.</p>

<p>Let&#8217;s take a java application. I need version 42 of <code>commons-dingleberry</code>. The vendor has packaged version 39. If this was debian, they would tell me I need to make my application work with version 39. What is more likely the case is that my application simply won&#8217;t ever be included in upstream because I&#8217;m not about to spend my free time that I contribute to an opensource project trying to make my software work with a version of a library that may simply not have the functionality I need.</p>

<p>This is really quite hilarious when you take something like Logstash which shades all the java deps it has in to its own jar. These packaging guidelines (and from what was communicated by someone attempting to get it into Fedora) requires someone to actually CHANGE Logstash such that the ElasticSearch version is its own package. Each version of Logstash is actually dependent on a specific version of ElasticSearch (due to the ability of Logstash to run ES embedded or as a non-data cluster member). Giving a user the &#8220;freedom&#8221; to switch ES versions actually creates a shitty experience for the user.</p>

<h2>And that&#8217;s really the crux of it</h2>

<p>Jordan has asked people repeatedly to NOT try and get Logstash into upstream repositories. Here&#8217;s why:</p>

<blockquote class="twitter-tweet"><p><a href="https://twitter.com/lusis">@lusis</a> <a href="https://twitter.com/robynbergeron">@robynbergeron</a> my fear is having the next 10 years spent telling rhel users &quot;sorry, logstash 1.2 is X years old, please upgrade&quot;</p>&mdash; @jordansissel (@jordansissel) <a href="https://twitter.com/jordansissel/statuses/382251492775710720">September 23, 2013</a></blockquote>


<script async src="http://lusis.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Jordan makes a valid point. And it&#8217;s not just Logstash. The software that people are attempting to run today (yes even in the stodgiest of enterprises) changes too often to ever be valid as packaged in upstream. No software I have ever cared about running outside of the core OS was ever present in a distro&#8217;s upstream repository. Running CentOS or RHEL? EPEL is the FIRST thing I have to add to the system.</p>

<h1>Remediation</h1>

<p>Nothing kills the relevance of a distribution faster than how much effort someone has to go to for it to be usable. Why did Ubuntu start to &#8220;win&#8221;? I see two things:</p>

<ul>
<li>2 year LTS cycle</li>
<li>The addition of PPAs</li>
</ul>


<p>Two years, while still an eternity, is frequent enough to be able to work in relevant changes in the industry. PPAs provide an outlet for someone to provide packages in a way that is largely well integrated into the distribution workflow. However even PPAs are not keeping up with the pace these days. I&#8217;m curious if an omnibus-style package would ever work in a PPA (assuming you would go to the effort of converting an omnibus project into a source deb.</p>

<h1>Software authors share the responsibility</h1>

<p>Using the vendor&#8217;s package format as an excuse not to provide packages doesn&#8217;t fly anymore. FPM has eliminated that. You don&#8217;t have to know a single goddamn thing about the RPM spec format or  deb control files.</p>

<p>And beyond that, with omnibus, you don&#8217;t even need to worry about dependencies on the system. By default and omnibus project will build packages for every current LTS release of Ubuntu and RHEL/RHEL clones. There&#8217;s not even a need to create a proper apt or yum repo. With an omnibus package you don&#8217;t have any external dependencies.</p>

<h1>What can distribution vendors do?</h1>

<p>I&#8217;ve talked at length about this issue with a few folks but none so passionate about it as <a href="https://twitter.com/samkottler">Sam Kottler</a> and <a href="https://twitter.com/robynbergeron">Robyn Bergeron</a>. Robyn actively sought me out at PuppetConf a few years ago. I think we chatted for over an hour on distribution relevance.</p>

<p>In general I think what would help most at this point is for distributions to provide an avenue for authors to provide packages that don&#8217;t meet strict guidelines. If Opscode wants to provide omnibus&#8217;d chef clients in /opt/chef without the user having to curlbash the package, that would awesome.</p>

<p>There is the concern to the user but my crusade in technology has been to not treat the user like a 3 year old. Give users enough clear information about risk and let them make the decision to install a package. Communication goes a long way.</p>

<h1>More to come</h1>

<p>I&#8217;m going to have more to write about this topic. It&#8217;s a never-ending source of fuel really. Thanks for reading.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Historical Compendium of Useless Shit]]></title>
    <link href="http://lusis.github.com/blog/2013/09/17/a-historical-compendium-of-useless-shit/"/>
    <updated>2013-09-17T22:20:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/09/17/a-historical-compendium-of-useless-shit</id>
    <content type="html"><![CDATA[<p>Yes it started on Twitter and ended up here. Circle of life and all that</p>

<!-- more -->


<p><em>Fair warning: I&#8217;m not doing ANY research for this post. Normally I do but I&#8217;m doing this off-the-cuff so to speak.
If I fuck up some specific thing, please feel free to correct. This is basically a personal perspective/history
And it&#8217;s probably full of typos so there&#8217;s that</em></p>

<p>So a discussion on twitter started because I brought up Mesos, Docker and OSv. I commented that none of these things were &#8220;new&#8221; technology really.</p>

<p>To be clear, it is a scientifically provable fact (for some values of &#8216;scientifically provable fact&#8217;) that when containers are brought up FreeBSD jails and Solaris Zones will work its way into the conversation. This is actually perfectly legitimate as the fit the same &#8220;model&#8221; as containers - lightweight &#8216;virtualization&#8217;. And whenever virtualization is brought up, someone will bring up IBM and LPARs. Again, perfectly legit.</p>

<p>In rough order of quick wikipedia&#8217;ing and personal memory, it goes something like this:</p>

<ul>
<li>IBM virt (z, i and later p - can&#8217;t recall if RS6k AIX did LPARS)</li>
<li>FreeBSD jails (wikipedia says first in 4.0 so around 2000?)</li>
<li>Solaris Zones (again the big interbrain says 2004)</li>
<li>Xen (2003)</li>
<li>VServer (???)</li>
<li>OpenVZ (remember that shit? 2005ish)</li>
<li>UML (????)</li>
<li>VMware (late 90s iirc)</li>
</ul>


<p>Also slot QEMU and KVM in here somewhere. These are all forms of &#8216;virtualization&#8217;. I&#8217;m sure there are some pedants are drafting comments now but let&#8217;s call it what everyone who isn&#8217;t a pedant calls it - virtualization (this little bit is important). Each one is better or worse for different reasons.</p>

<p>All I&#8217;m trying to say is that everyone has a claim at some point in the stack. I&#8217;ve always argued that really IBM is the progenitor of this whole space but I&#8217;m sure someone will point out something I missed.</p>

<h1>So about <code>*</code>BSD</h1>

<p>The point was made that everyone seems to forget about FreeBSD jails. I don&#8217;t think that&#8217;s the case at all. Here&#8217;s my little take on why FreeBSD didn&#8217;t &#8220;take off&#8221; (and the same applies to others as well). I&#8217;m probably wrong but whatever. Not like that&#8217;s a first.</p>

<p>Everyone involved in this twitter convo has been in the industry a while. We remember when Linux wasn&#8217;t the defacto operating system you would use. AIX, Solaris, HPUX - THOSE were the things REAL businesses used.</p>

<p>So why is Linux suddenly the defacto choice and why did technically superior solutions fall by the wayside? As I said, that wasn&#8217;t always the case. We used to actually have to HIDE the fact that the print server wasn&#8217;t running WindowsNT. Linux and FreeBSD were what we used at home. The others were what we used at work. I got my start with the whole Linux world around 95 with Slackware just as Yggdrasil was going away.</p>

<p>Anyway here are a few things I think that &#8220;hurt&#8221; FreeBSD a bit:</p>

<h2>Licensing</h2>

<p>Licensing wars were almost as prolific as editor wars in on Slashdot. Regardless of your position on WHICH license is more &#8216;liberal&#8217;, this had an impact on the success. IIRC, the original BSD license was incompatible with the GPL. This meant there was little cross-pollination between the communities.</p>

<p>Regardless, what you ended up seeing was more &#8220;corporate&#8221; adoption of BSD into closed hardware products (firewalls, load balancers) and that never made it back into the community because it didn&#8217;t have to.</p>

<p>On the Linux side, however, the GPL sort of promoted a community because of the forced pollination.</p>

<h2>Speciation</h2>

<p>Probably not the best way to describe it but within the BSD community the 3 major derivitives ended up being pigeonholed</p>

<ul>
<li>FreeBSD was your server OS</li>
<li>NetBSD was for weird-ass random hardware</li>
<li>OpenBSD was for your firewalls (&#8216;cause Theo is mad crazy about security yo)</li>
</ul>


<p>Again, whether or not this characterization was correct that was the general thought process most people I knew had.</p>

<p>Meanwhile, Linux was Linux. Yes there were distros but they largely set themselves apart by what they layered on top in userland. This is somewhat critical.</p>

<h2>Desktop</h2>

<p>As I said above, FreeBSD was never really positioned as being for desktops among the commoners. Meanwhile Linux distributions were catering to that crowd. The thing is Linux was acceptable as a server OS as well. So if we were running Linux at home on our desktops, there was very little cognative disconnect to running it on our servers. Just don&#8217;t install X and it&#8217;s a server, right?</p>

<p>FWIW this is one of the things that I think helped Microsoft as well - knowledge portability.</p>

<h2>IBM</h2>

<p>Quite honestly, IBM was probably the biggest thing to help Linux get that final push. Yes we&#8217;d been running Linux at home, on our personal web servers and in small corners of the office but now here&#8217;s a name our bosses trust saying that Linux is a thing. That bald little boy talking about sharing shit and all that. Holy fuck! We made it. Then the COTS started coming. I remember getting that copy of Oracle 6 (or was it 7?) at Atlanta Linux Expo. Holy shit, Oracle runs on Linux!</p>

<p>So this IBM thing is a pretty big hurdle for a BSD to overcome. Marketing took over. Nobody cared that ufs was superior to ext2 or which is better ipfilter (then pf) vs. ipchains. IBM is backing Linux to the tune of 1 Billion dollars. Who gives a fuck about which license is more &#8220;free&#8221;?</p>

<p>Oh and IBM is doing it again though arguably it means fuckall at this point since Linux is, again, the defacto standard.</p>

<h1>Wrap up</h1>

<p>Mind you this is mostly personal opinion/perspective. Is FreeBSD technically superior? Probably. Did it do certain things first and better? Yep (though as I brought up someone else did it before FreeBSD in some cases).</p>

<p>Let&#8217;s face it, even as Linux fans you have to admit that Linux has been adopting shit from everyone else for a while and arguably as a shittier implementation. Don&#8217;t get me wrong, I love Linux and I owe it a lot. This is one reason I try and give back as much as I can. I wasn&#8217;t able to before.</p>

<p>I think FreeBSD has a chance for a comeback at this point (some of these have been going on for a while):</p>

<ul>
<li>REAL AWS support (not some hackish method). People start in AWS for better or worse.</li>
<li>Having a native JDK by way of OpenJDK (as opposed to the ABI bullshit you used to have to do), will help.</li>
<li>The general industry move AWAY from COTS. This has been going on for a while but nobody really fucking cares about if you can run Websphere.</li>
<li>General embracing of alternative languages which have no problem running on BSD</li>
<li>General frustration with Linux as a server OS. See CoreOS and talk to anyone at Fastly.</li>
</ul>


<h1>Some things I find &#8216;funny&#8217;</h1>

<ul>
<li>For all the bitching we used to do about GPL vs BSD license, everybody I know these days is going with Apache over either of them</li>
<li>After all this fucking time, &#8220;Linux on the desktop&#8221; is still a joke (and I&#8217;ve run Linux on my desktop for the past 15 years)</li>
<li>Linux volume management STILL sucks compared to like&#8230;.everywhere else</li>
<li>People hated the BSD source-based model and yet Gentoo was a thing&#8230;</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Go for System Administrators]]></title>
    <link href="http://lusis.github.com/blog/2013/08/11/go-for-system-administrators/"/>
    <updated>2013-08-11T15:44:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/08/11/go-for-system-administrators</id>
    <content type="html"><![CDATA[<blockquote><p>If I never directly touch a Go concurrency primitive, I&#8217;m convinced I&#8217;m going to write all my cli apps with it just for ease of deployment.</p></blockquote>


<!-- more -->


<p>This is something I said the other day. I figured it deserved a more detailed blog post.</p>

<h2>NKOTB</h2>

<p>Most people who know me professionally know two things about me:</p>

<ul>
<li>I&#8217;m fairly pragmatic and somewhat conservative about technology decisions</li>
<li>I&#8217;m a language tourist</li>
</ul>


<p>This second one is something Bryan Berry attributed to me in an early FoodFight episode. What&#8217;s interesting is the two things seemingly conflict.</p>

<p>I love learning new programming languages. This comes as a pretty big shock to me on a regular basis because I&#8217;m not a professional programmer. I didn&#8217;t go to college for programming (I actually didn&#8217;t go to college at all). My career in IT has been pretty much 100% focused on the area of operations. Anything I&#8217;ve ever touched - qa, dba, dev - has always been from that lens and to satisfy some need operationally.</p>

<p>So it&#8217;s weird that I find myself 18 years later having a working knowledge of ruby, python, perl, java and a few other languages to a lesser degree. Mainly I come to new languages to scratch an itch.</p>

<p>This leads me to picking up Go.</p>

<p>If you haven&#8217;t heard of Go, there are countless articles, blog posts and a shitload of new tooling written in it. The latest batch of hotness around linux containers and new deployment models (docker) is based on Go. There are also quite a few other &#8220;big&#8221; name projects built in Go as well - packer, etcd. Mozilla is doing all new internal tooling in Go (as I understand it) and quite a few folks are switching to it.</p>

<p>Mind you I don&#8217;t pick up languages based on popularity. I don&#8217;t care for JavaScript and Node at all, for instance. Originally I had no interest in Go either. I figured it was another Google experiment that was more academic than anything else. Besides, if I had to get a handle on a c-like language, why not just learn C and be done with it?</p>

<p>I actually attempted that route working on a PAM module for StormPath. While it was somewhat satisfying, it was ultimately VERY frustrating.</p>

<h2>So why Go now?</h2>

<p>One of the reasons I decided to give Go another shot was that it appeared to be around for the long haul after all. That made at least a contender for me.
But then some of the tooling I was using operationally was being built in Go. Since I wanted to be able to fix issues in those tools (especially considering they were new projects which would surely need fixes) I really needed to pick up on the language.</p>

<p>However one tool really pushed me that last step - <a href="https://github.com/coreos/etcd">etcd</a>.</p>

<p>You can read up on etcd yourself but if you know my history with <a href="https://github.com/lusis/Noah">Noah</a>, you realize WHY I have such an interest in this.</p>

<p>What surprised me was when I decided that I&#8217;d probably be writing a lot of tooling myself in Go.</p>

<h2>On Pragmatism</h2>

<p>All the internal tooling my team develops at Dell Enstratius is written in Python. This was a pragmatic choice for us:</p>

<ul>
<li>It&#8217;s the least common denominator on platforms our product supports (So it will always be on customer systems)</li>
<li>It&#8217;s rigid in the right ways for new programmers (of which we had quite a few on our team)</li>
<li>Regardless of skill level with Python, you can usually look at someone else&#8217;s code and follow it thanks to the previous item</li>
</ul>


<p>Why didn&#8217;t we go with Ruby considering I was personally much stronger at Ruby and we had some Ruby experience via Chef internally?</p>

<ul>
<li>Have you seen the state of Ruby on distros?</li>
<li>We didn&#8217;t want to conflict with any possible customer tooling using Ruby</li>
<li>Not enough rigidity for the new folks</li>
<li>As comfortable as I am at Ruby, because of the flexibility of the language and metaprogramming, it can be downright impossible to navigate someone else&#8217;s code</li>
</ul>


<p>Our team weighed all the options here and we all agreed on Python. I set out to write a library for accessing our API. This would give us a foundation for our tooling as well as serve as a reference project - with tests, project structure, bin scripts and the like - for new tooling.</p>

<p>Things are/were going great up until a recent situation with a customer. We try and minimize dependencies in our tooling for obvious reasons. However there are a few libraries that just make things SO much easier - <a href="https://github.com/kennethreitz/requests">requests</a>, <a href="https://github.com/kennethreitz/envoy">envoy</a>. We also like to use <a href="http://fabfile.org">Fabric</a> to wrap some things up.</p>

<p>However we ran into a situation where a customer refused to let us pull packages in externally. So while we could &#8220;sneakernet&#8221; the bulk of our tools over, some things wouldn&#8217;t work. Tracking down all the transitive deps and vendoring everything was a pain in the ass.</p>

<p>This is what lead to my statement above.</p>

<h2>Tooling in Go</h2>

<p>Go, while not as tight a feedback loop as Python, is still pretty tight. Compilations happen fast and you can test fairly quickly. But the dependency issue is really the killer. It simply doesn&#8217;t exist. I can take that binary I compiled and move it around with no problem without needing the runtime installed. There are lots of batteries included in the stdlib as well.</p>

<p>I can also compile that same code on osx, windows or linux with no modification. This bit us in Python with some of our deps as well.</p>

<p>As I said, while the tooling I&#8217;m currently writing has no need for any of the advanced concurrency stuff in go, it&#8217;s nice that&#8217;s it there out of the box should I want to use it.</p>

<h2>This isn&#8217;t a switching story</h2>

<p>We&#8217;re not switching to Go for our tooling but I probably will. I&#8217;m already working on writing a wrapper for our API in Go so I can duplicate some of the tools. This will be really handy when I&#8217;m on a system where dependencies are limited. That&#8217;s really what this post is about.</p>

<p>If you&#8217;re in operations, there is no reason you shouldn&#8217;t learn Go.</p>

<p>The syntax is easy. The stuff that made C painful is largely hidden from you. Meanwhile you don&#8217;t need to worry about what version of Python or Ruby is installed on your systems. It&#8217;s a great language to use for bootstrap tools where you don&#8217;t yet have your deps installed. It&#8217;ll also help should you start adopting tools like docker, packer or etcd.</p>

<p>Give it a shot.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DevOps - The title match]]></title>
    <link href="http://lusis.github.com/blog/2013/06/04/devops-the-title-match/"/>
    <updated>2013-06-04T21:38:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/06/04/devops-the-title-match</id>
    <content type="html"><![CDATA[<p>I&#8217;ve noticed an annoying trend recently. I was content to ignore it for a while but now it&#8217;s getting almost stupid. That trend is the job title of &#8220;DevOps&#8221;.</p>

<!-- more -->


<p>I came across an article tonight that confused the hell out of me. It was an interview. The outlet wasn&#8217;t a technical one per se but it was a technical interview none the less.</p>

<p>This part is what confused me (<em>highlights mine</em>):</p>

<blockquote><p>To do this requires a well-integrated platform that is full of capabilities for the **developer**; contemplates the needs of **DevOps**; provides command, control and visibility for **operations**; and is &#8230;.</p></blockquote>


<p>To quote an internet-famous person:</p>

<p><strong>WAT</strong></p>

<h2>Stepping back</h2>

<p>I don&#8217;t like the idea of <em>devops</em> as a job title in the first place. I don&#8217;t like it as a role either. It makes no sense. You don&#8217;t call someone an <em>agile</em> so why would you call them a <em>devop</em>?</p>

<p>My problem lies in the fact that it implies that there&#8217;s something wrong with being a sysadmin, operations person, developer or whatnot. Not only that but the idea behind devops is the elimination of silos, not the creation of new ones.</p>

<p>I have, however, made a bit of peace with the fact that <em>devops</em> has become a replacement title of sorts for <em>sysadmin</em> or <em>developer</em>.</p>

<p>I get the problem domain. Companies want to be able to qualify the types of people they want. The phrase <em>devops</em> carries a certain meaning with it. People are trying to leverage that. In other cases, it&#8217;s become a codeword for &#8220;generalist&#8221; or &#8220;technologist&#8221;. And, yes, even in some cases it&#8217;s become a code word for &#8220;doing both development and ops work&#8221;</p>

<h2>What does devops mean though</h2>

<p>Here&#8217;s the secret. I&#8217;ll tell you EXACTLY what devops means.</p>

<p>Devops means giving a shit about your job enough to not pass the buck. Devops means giving a shit about your job enough to want to learn all the parts and not just your little world.</p>

<p>Developers need to understand infrastructure. Operations people need to understand code. People need to fucking work with each other and not just occupy space next to each other.</p>

<p>I worked at a company several years ago. We created a dedicated devops team. The rationale was solid - the company had a monolithic idea of roles and titles. We also had a large group on both sides that were only interested in doing their little bit and going home. By creating this title/team, it was easier at a company level to justify them working on non-standard projects.</p>

<p>So a &#8220;devops&#8221; team was created. This was a small team of what essentially boiled down to &#8220;super sysadmins&#8221;. We wrote puppet manifests, worked with the developers to automate build processes&#8230;shit like that.</p>

<p>What ended up happening was that the devops team was seen as elitist by the operations team, nosy and invasive by the developers and everyone just passed the blame on to them - &#8220;Devops did that. Not us&#8221;</p>

<h2>So back to that quote</h2>

<p>Having said all that, what about the quote?</p>

<p>This is indicative of the problem I described above. I think I&#8217;ve finally figured out the question I want to ask people who think this way:</p>

<p>If devops is a distinct role/title apart from development and operations, then what the fuck does a &#8220;devop&#8221; do?</p>

<p>Let&#8217;s look at that quote again. It implies that:</p>

<ul>
<li>&#8220;command, control and visibility&#8221; is something developers have no need for</li>
<li>operations won&#8217;t need the same capabilities as developers</li>
<li>There&#8217;s a third group that has an entirely different set of needs</li>
</ul>


<p>What is that third group? What possible aspect outside of development and operations of the IT needs of a company do those two groups not think affects them in some way and thus have a vested interest in being involved? <em>(Yes I&#8217;m aware companies have dbas, security and what not - those are shitty silos too)</em></p>

<p>This topic comes up all the time on various mailing lists and it never seems to really reach any sort of consensus. So I&#8217;m asking you. If a &#8220;devops&#8221; is something different than someone in operations or development, someone different than a sysadmin or developer&#8230;.</p>

<p>What the fuck is it?</p>

<h2>Disclaimer</h2>

<p>This post was writting on an airplane with an annoying passenger in front of me, quite a bit of rum in me and a lack of sleep. I&#8217;m guessing it really doesn&#8217;t look any different than any other post though does it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Smart Clients]]></title>
    <link href="http://lusis.github.com/blog/2013/05/13/smart-clients/"/>
    <updated>2013-05-13T21:17:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/05/13/smart-clients</id>
    <content type="html"><![CDATA[<p>Currently <a href="http://ricon.io/east.html">RICON|EAST</a> is going on in NYC. <a href="https://twitter.com/tsantero">Tom Santero</a> and the whole Basho crew is doing an awesome job if the content available via the live stream and twitters is to be believed.</p>

<!--more-->


<p><em>(please note this is my first blog entry post-D. As such and because I&#8217;ve yet to talk to any legal folks, I should state that this does not represent any opinion or policy of Dell)</em></p>

<p>One thing that caught my eye/ear when I could listen/watch was an excellent presentation by <a href="https://twitter.com/seancribbs">Sean Cribbs</a>. Sean holds a special place in my hero worship pantheon for a few reasons:</p>

<ul>
<li>I first heard about Riak on one of the first episodes of the ChangeLog show (along with <a href="https://twitter.com/argv0">Andy Gross</a>). It and the whole NoSQL thing made sense then</li>
<li>Sean is a pretty fucking down to earth person. He graciously drove down to one of our local meetups. Uber friendly and an awesome advocate for Basho and Riak.</li>
<li>He&#8217;s a really awesome presenter and if you&#8217;ve never had the priviledge of seeing him (live stream or in person), he rocks the mic.</li>
</ul>


<p>So in Sean&#8217;s presentation he&#8217;s talking about some changes to the Ruby client library for Riak. Many of the changes make the Ruby library a proper smart client. Read <a href="https://github.com/basho/riak-ruby-client/wiki/Connecting-to-Riak">this wiki</a> wiki entry under <strong>Connecting to Clusters</strong> for some of the features. It&#8217;s awesome (especially the transport-related failure handling).</p>

<h1>Client libraries in general</h1>

<p>I want to say a bit about client libraries. Regardless of what they talk to (though I&#8217;ll be talking specifically about database client libraries), this is something many companies get wrong.</p>

<p>Everyone knows I&#8217;m not the biggest MongoDB/10Gen fan in the world. I won&#8217;t go into detail about the technical reasons behind that. Many others have done a much more eloquent dive into that topic.
As much as it&#8217;s easy to make fun of MongoDB as being an marketing-driven database, they did get one thing right. They owned their client driver availability. Not only did they own and maintain all the drivers but they largely had the same API across the various languages.</p>

<p>Then again, they had to. Other databases/applications offer a REST-ish interface over HTTP (or plain-text interface like Redis) so they can punt a bit. Got a libcurl port for your language? You&#8217;re set. MongoDB has its own protocol and that god-forsaken BSON shitshow.</p>

<p>One of the benefits, however, of a plain-text or HTTP-based protocol is that it&#8217;s a pattern we can grok as operators and developers. We load balance webservers. We speak to third-party APIs. It&#8217;s not the most EFFICIENT but it&#8217;s a known quantity. It&#8217;s also, as I said, REALLY fucking easy to add support to your language of choice. No need to FFI some c library or make a binary extension. Any language worth its salt has http client support in stdlib (even if it&#8217;s as big a pile of dog squeeze as net/http). Again, most languages also have libcurl support for something better.</p>

<h1>Back to smart clients</h1>

<p>I largely dislike applications that require smart clients to get the full benefit. As an operations person, I&#8217;m USUALLY using a dynamic language like python, ruby or perl to access the system as opposed to directly from the application. This was my biggest gripe with ZooKeeper (as I&#8217;ve said many times in the past). It&#8217;s also been one of my points of contention with Datomic. If you aren&#8217;t on a JVM language, you&#8217;re shit out of luck for now. Yes, JRuby makes this billions of times easier for those of us using Ruby but Jython is still not where it needs to be for modern Python.</p>

<p>I also had this problem with Voldemort. Disclaimer this is 2-3 year old data from running Voldemort in production. AFAIK, it&#8217;s still the case. For the sake of this discussion, we&#8217;re going to ignore data opacity. At the time, the only way to fully access the data in and maintain a Voldemort cluster was from the JVM. I ended up writing quite a bit of JRuby wrapper around StoreClient just to see the data we had in Voldemort.</p>

<p>Riak (and as another example, ElasticSearch) is nice in this regard. It&#8217;s HTTP. I can curl it from a shell script. I can use the Ruby library Basho is maintaining. If I&#8217;m using a language without &#8216;official&#8217; support, I can write my own. All the metadata is largely attached to http headers and even monitoring is done via the <code>/ping</code> and <code>/stats</code> urls. Something I didn&#8217;t realize until today (thanks to <a href="https://twitter.com/b6n">Benjamin Black</a>) is that the stats interface actually exposes stuff I had previously glossed over including cluster topology. This is where the meat of the discussion on twitter today happened.</p>

<h1>Operational Happiness</h1>

<p>My original statements on this discussion related to using haproxy in front of your Riak cluster. There are several reasons I prefer this but a quick sidebar</p>

<h2>Seed Nodes</h2>

<p>I&#8217;ve had some minor operational experience with Cassandra (nothing to write home about) but one of the things that always bothered me was the idea of &#8216;seed nodes&#8217;. Let me be clear that Cassandra and Riak are pretty much the only two datastores I&#8217;d feel comfortable using these days (with the nod going to Riak) in any sort of scalable environment. Postgres has earned its way back to my into my graces but MySQL can <em>insert Louis C.K. euphemism here</em>.</p>

<p>My problem with seed nodes is the idea that I have special nodes. These nodes have to be hard-coded in a config file somewhere or discovered by some other method. I could store them as DNS lookups but now I&#8217;ve got to deal with TTLs on DNS. And I&#8217;ve got to deal with the fact that DNS doesn&#8217;t actually care if the host I&#8217;ve been given is actually alive or not.</p>

<p>I could store this information in ZooKeeper but what if I don&#8217;t actually have native ZK support in that database? I&#8217;ve got to write something that populates ZK when a new node is available and it&#8217;s not actually a live check. I still have to test that host first. Yes you should do that anyway but it&#8217;s a valid point.</p>

<p>So if I&#8217;m storing seed nodes as DNS names in a config file, I can never change those names without either rolling out new code or configs. That might require a restart somewhere. If I&#8217;m clever, I could probably make that an administrative hook in my application (think JMX) where I can fiddle the seed list. I can poll a config file for changes. I can do a lot of thing but none of them are &#8220;optimal&#8221; to me.</p>

<h2>Back to haproxy</h2>

<p>My prefered method of using Riak is to stick everything behind haproxy. There are several reasons for this but here are a few reasons (note we&#8217;re going to assume use of CM at this point):</p>

<ul>
<li>Operationally, haproxy nodes are easier to manage than application configs (depending on the application).</li>
<li>Many times, at different companies, we&#8217;ve had to roll our own layer on top of a client library (or even write our own due to licensing issues). Load-balancing is not neccessarily a core application developer competency (and it&#8217;s bitten me in the ass before).</li>
<li>From an operational perspective, I can bring nodes in and out of service in haproxy for maintenance without needing to inform the client or have it waste cycles with stale node detection. It simply will never talk to an out of service backend.</li>
</ul>


<p>Basically the haproxy crew has already done all the work in load balancing HTTP connections intelligently and they&#8217;re pretty damn good at it. I love my developers and I know the folks writing the various libraries I use are smart people but again it goes back to core competency. Note that Basho gets a nod here, however, in that I&#8217;m pretty sure that, what with writing webmachine, they grok http pretty well.</p>

<p>I&#8217;ve even gone the approach of using haproxy on every system that needs to connect to some backend locally. This totally eliminates the idea of fixed points in the network for connections (at the expense of having to deal with a bit of drift between CM runs). If I wanted to, I could even make multiple riak clusters transparent behind haproxy (though I can only think of a few REALLY specific use cases for that).</p>

<h2>Trade offs</h2>

<p>Yes there are trade-offs to this approach. As Ben pointed out, the Riak stats interface is really powerful from a topology perspective. A REALLY smart riak client can discovery data layout and make deeply intelligent decisions on that.</p>

<p>With other applications like ElasticSearch I can actually become a full fledged cluster member as a non-data node and actually offload some of the work of the cluster for scatter/gather type operations using the Java library.</p>

<p>With haproxy, I don&#8217;t get those types of benefits.</p>

<h2>What I do get</h2>

<p>Outside of the maintainability of haproxy (which is, again, subjective) I get one benefit I <strong>CAN&#8217;T</strong> get with smart clients that is, unfortunately, neccessary with many customers - a more narrow network allowance.</p>

<p>Enterprises are interesting beasts and still think in terms of traditional tiered application stacks. Nothing wrong with that and it does have SOME security benefits but many deployments I&#8217;ve been involved with have official policies that &#8216;data tiers&#8217; (whatever the hell those are) must be protected in a dedicated network behind an additional firewall. So here&#8217;s Riak, a &#8216;data tier&#8217;, that we have to have with as small an ingress as possible. That rules out smart clients of the toplogy-aware variety. So we stick haproxy in the mix. We tell them to use a load balancer. Some folks use an internal F5 HA pair with some sort of VRRP. We&#8217;ll set up haproxy + keepalived or some other combination depending.</p>

<h1>TMTOWTDI</h1>

<p>One of the things that Riak allows is this level of flexibility. I can use HTTP and haproxy. I can use HTTP and smart client. I can use protobufs and haproxy or a smart client. It&#8217;s really that flexible. I happen to prefer the haproxy approach for reasons I&#8217;ve already mentioned but I totally grok that some folks want a more intelligent client approach. Some folks would argue that there&#8217;s a right way and a wrong way but I don&#8217;t see it like that. What I see is a datastore that, just like it letting me control the consistency levels I want, let&#8217;s me control HOW I access that data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Future of Noah]]></title>
    <link href="http://lusis.github.com/blog/2013/01/20/future-of-noah/"/>
    <updated>2013-01-20T21:15:00-05:00</updated>
    <id>http://lusis.github.com/blog/2013/01/20/future-of-noah</id>
    <content type="html"><![CDATA[<p>This is probably the most difficult blog post I&#8217;ve had to write. What&#8217;s worse is I&#8217;ve been sitting on it for months.</p>

<!-- more -->


<p>When I started Noah a few years ago, I had a head full of steam. I had some grand ideas but was trying to keep things realistic. I simply wanted a simple REST-ish interface for stashing nuggets of information between systems and a flexible way to notify interested parties when that information changed.</p>

<p>It started as a <a href="https://raw.github.com/lusis/Noah/8a2e193c043ab30cce17d7ada25ef33b72baa73e/doc/noah-mindmap-original.png">mindmap</a> laying in bed one night. It was my first serious project and I had no idea what I was getting in to. If you&#8217;re curious, you can read quite a bit of my initial braindumps on the <a href="https://github.com/lusis/Noah/wiki">wiki under &#8216;General Thoughts&#8217;</a>. I watched every day as more and more people started following the project.</p>

<p>It was a game changer for me in many ways. Working on Noah was fun and it was rewarding in more ways than one. But real life gets in the way sometimes.</p>

<h1>On stewardship</h1>

<p>One of the things I&#8217;ve learned over the past few years is that for opensource to REALLY thrive, it can&#8217;t be a one-person show. I&#8217;ve been involved with opensource for most of my 17+ year career. You think I would have learned that lesson before now.</p>

<p>Stewardship is a hard thing. Our arrogance and pride makes us want to keep things close to our chest.</p>

<ul>
<li>&#8220;I just want to get to a 1.0 release&#8221;</li>
<li>&#8220;Things are too in flux right now. It wouldn&#8217;t be fair to bring others in&#8221;</li>
<li>&#8220;I don&#8217;t quite trust anyone else with it yet&#8221;</li>
<li>&#8220;Let me just get this ONE part of the API in place first..&#8221;</li>
</ul>


<p>These are all things I said to myself.</p>

<p>What really changed my mind was a few things. Being involved in the Padrino project. Seeing the Fog community grow after Wesley started allowing committers. Seeing Jordan trust me enough to make me a logstash committer before his daughter was born. The biggest trigger was actually one of my own projects - the chef logstash cookbook.</p>

<p>Bryan Berry (FSM bless him) pestered the hell out of me about getting some changes merged in. He was making neccessary changes and fixes. He was evolving it to make it more flexible beyond my own use case. I don&#8217;t recall if he asked to be a committer but I gave it to him. The pull request queue drained and he added more than I ever had time for. Not long after, I added Chris Lundquist. Those two have been running it since then really.</p>

<p>I think back to when I got added to the committers for Padrino. It was a rush. It was amazing and scary. Above all it was the encouragement I needed. How dare I deny someone else that same opportunity.</p>

<p>Making that first pull request is hard. To have it accepted is a feeling I&#8217;ll keep with me for a long time. I can only hope that some project I create some day will give someone that same confidence and feeling.</p>

<h1>So what about Noah</h1>

<p>Noah is in the same place Logstash was. I&#8217;m not using it and that&#8217;s really hurting it more than anything. It&#8217;s time to let someone who IS using it take control. I care too much about it to watch it die on the vine. I still believe in what it was designed to do and every single day I get emails asking me if it&#8217;s still alive because it&#8217;s a perfect fit for what someone needs. The same stuff is STILL coming up on various mailing lists and Noah is a perfect fit. There are companies actively using it even it the current unloved state. Those folks have a vested interest in it.</p>

<p>When I added Chris and Bryan to the cookbook, I sent them an email with what my vision was for the cookbook. I can&#8217;t find that email now but I recall only had two real requirements:</p>

<ul>
<li>Out of the box, it would work on a single system with no additional configuration (i.e. add the cookbook to a run_list and logstash would work automatically)</li>
<li>A user never had to modify the cookbook to change anything related to roles (i.e. allow the attributes to drive search for discovering your indexer - hence all the role stuff in the attrs now)</li>
</ul>


<p>I need to do the same thing for Noah and see where it leads.</p>

<h1>Dat list</h1>

<p>This list isn&#8217;t comprehensive but I think it hits the key points.</p>

<h2>Simple</h2>

<p>Noah should be simple to interact with. It was born out of frustration with trying to interact with ZooKeeper. Nothing is more simple than being able to use <code>curl</code> IMHO. I can use Noah in shell scripts and I can use it in Java (we had a Spring Configurator at VA that talked to Noah. It was awesome). You should always be able to use <code>curl</code> to interact with Noah. I wish I could find it now but someone once brought up Noah on the ZK mailing list. This led to various rants about how it didn&#8217;t do consensus and a bunch of other stuff that ZK did. One of the Yahoo guys (I wish I could remember who) said something in favor of Noah that stuck with me:</p>

<p><em>Interfaces matter</em></p>

<p>I know I&#8217;m on the right track here because Rackspace just built a product that provides an HTTP interface to ZK. Oh and it does callbacks.</p>

<h2>Friendly to both sysadmins and developers</h2>

<p>Simplicity plays into this but I wanted Noah to be the tool that solved some friction between the people who write the code and the people who run the code. Configuration is all over the place in any modern stack. Configuration management has come into its own. People are using it but you still see disconnects. Where should this config be maintained? What&#8217;s the best way to have puppet track changes to application configuration? I can&#8217;t get my developers to update the ERB templates in the Chef cookbook. All of these things are where Noah is helpful.</p>

<p>I still stand by the statement that <a href="http://lusislog.blogspot.com/2011/03/ad-hoc-configuration-coordination-and.html">not all configuration is equal</a>. Volatility is a thing and it doesn&#8217;t have to mean the end of all the effort in moving to a CM tool. I wanted to remove that friction point.</p>

<p>I was also immensely inspired by Kelsey Hightower here. I&#8217;ve told the story several times of how Kelsey got so frustrated that the developers wouldn&#8217;t cooperate with us on Puppet and config files for our applications that he learned enough Java to write a library for looking up information in Cobbler. Cobbler has an XMLRPC api and that was simple enough that he could port his python skills to java and write the fucking library himself. I wanted Noah to be friendly enough that a sysadmin could do what Kelsey did.</p>

<h2>Watches and Callbacks</h2>

<p>I&#8217;ve said this before but one of the most awesome things that ZK has is watches. They have pitfalls (reregister your watches after they fire for instance) but they&#8217;re awesome. Noah&#8217;s callback system is the thing that needs the most love (it works but the plugin API was never finalized). It&#8217;s also one of the most powerful parts that meets the needs of folks that I see posting on various mailing lists.</p>

<p>The idea is simple. When something changes in Noah, you should be able to fire off a message however the end-user wants to get it. I think this is one of the reasons I love working on Logstash so much. Writing plugins is so simple and it&#8217;s the gateway drug to anyone who wants to contribute to logstash.</p>

<h1>Things I don&#8217;t care about</h1>

<p>What don&#8217;t I care about?</p>

<h2>Language</h2>

<p>I don&#8217;t care about the language it&#8217;s written in. If someone wants to take it and convert it to Python or Erlang or Clojure, be my guest. I just want the ideas to live on somehwere. In fact, I&#8217;ve rewritten various parts of Noah over the last year privately. Not just experimenting with moving from EM to Celluloid but as a Cherry.py app, in Clojure and I even started an Erlang attempt (except that I know almost NO Erlang so it didn&#8217;t get very far).</p>

<h2>Name</h2>

<p>Honestly I don&#8217;t even care about the name. Yeah it&#8217;s witty and fits with the idea of ZooKeeper but I have no qualms about adding a link to your project from the Noah readme and recommending people use it instead.</p>

<h2>Paxos/ZAB</h2>

<p>This was never a requirement for Noah. Noah was specifically designed for certain types of information. If you need that, use the right tool.</p>

<h2>Persistence</h2>

<p>Let&#8217;s be honest. From a simplicity standpoint, it doesn&#8217;t get much simpler than Redis. It&#8217;s one of the reasons we changed the default logstash tutorial to use Redis instead of RabbitMQ. I know Redis reinvents a lot of wheels that have already been solved but it, along with ElasticSearch, are one of the lowest friction bits of software I&#8217;ve dealt with in a long time. Not having external dependencies is a godsend for getting started.</p>

<p>However I&#8217;ve also got small experiments privately where I used ZMQ internally and sqlite. I&#8217;ve written a git-based persistence for it too.</p>

<p>Riak is also a great fit for Noah and takes care of the availability issue on the persistence side. More on Riak in a sec.</p>

<h1>So that&#8217;s it</h1>

<p>That&#8217;s really all that matters. If you want to take ownership of the project, contact me. Let me know and we&#8217;ll talk. Who knows. Maybe I&#8217;m overestimating the level of interest. Maybe ZK isn&#8217;t as unapproachable to people anymore. The language bindings have certainly gotten much better. I just want the project to be useful to folks and I&#8217;m getting in the way of that.</p>

<h1>What are the other options?</h1>

<p>I don&#8217;t know of many other options out there. Doozer is picking up steam again as I understand it and it has a much smaller footprint than ZK does. There was a python project that did a subset of Noah but I can&#8217;t find it now.</p>

<p>One thing that is worth considering is a project that I found earlier today - <a href="https://github.com/cocagne/zpax">zpax</a>. While this is just a framework experiment of sorts, it could inspire you to add your own frontend to it. The same author is also working on DTLS on top of ZMQ.</p>

<p>I&#8217;ve thought about ways I could actually do this with Logstash plugins. It&#8217;s doable but not really feasible without making Logstash do something it isn&#8217;t shaped for.</p>

<p>Another idea that I&#8217;m actually toying around with is simply using Riak plus a ZeroMQ post-commit hook so that plugins could be written in a simpler way. <a href="https://github.com/seancribbs/riak_zmq">Sean Cribbs already took the idea and made a POC 2 years ago</a> based on a gist from Cody Soyland. You wouldn&#8217;t have the same API up front as Noah but you could stub that out in some framework and also have it be the recipient of the ZMQ publishes.</p>

<p>Finally you could just use ZooKeeper. Yes it has MUCH greater overhead but you DO get a lot more bang for the buck. There really isn&#8217;t anything in the opensource world right now that compares. It also provides additional features that I never really cared about or needed in Noah.</p>

<h1>Wrap up</h1>

<p>I&#8217;m not done in this space. I don&#8217;t know where I&#8217;m going next with it. Maybe I&#8217;ll start from scratch with a much simpler API. Maybe I&#8217;ll just run with the Riak idea.</p>

<p>I just want to give a shoutout to the countless people who helped me evangelize Noah over the last few years. It was recommended on mailing lists, twitter and many other places. It meant a lot to me and I only hope that someone will take up the mantle and make it something you would recommend again.</p>

<p>For those of you still using Noah, I hope we can find a home for it so that it can continue to provide value to you.</p>

<p>Thanks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How we vagrant]]></title>
    <link href="http://lusis.github.com/blog/2012/12/17/how-we-vagrant/"/>
    <updated>2012-12-17T22:10:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/12/17/how-we-vagrant</id>
    <content type="html"><![CDATA[<p>People may or may not have noticed but I&#8217;ve been largely offline for the past 4 weeks or so. This is because I&#8217;ve been in the middle of a pretty heavy redesign of a few key parts of our application stack. This also required me to learn Java so I&#8217;ve been doubly slammed.</p>

<p>As part of that redesign, I worked on what we lovingly refer to internally as the &#8220;solo installer&#8221;. I gave a bit of background on this in a post to the Chef mailing list at one point but I&#8217;ll go over it again as part of this post.</p>

<!-- more -->


<h1>Beginnings</h1>

<p>To understand why this is something of a departure for us, it&#8217;s worth understanding from whence we came. enStratus, like most hosted solutions, has experienced largely organic growth. One of the nice things about a SaaS product is that you have the freedom to experiment to some degree. You might be in the middle of a MySQL to Riak migration and still need two data stores for the time being. You might be in the process of changing how some background process works so you&#8217;ve got an older designed system running along side the newer system which is only doing a subset of the work.</p>

<p>With a hosted platform these kinds of things are hidden from the end-user to some degree. They don&#8217;t know what&#8217;s going on behind the scenes and they really don&#8217;t care as long as you&#8217;re doing X, Y and Z that you&#8217;re being paid to do.</p>

<p>Now for those of you running/developing/managing some sort of SaaS/Hosted solution I want you to take a journey with me. Imagine that tomorrow someone walked into your office and said:</p>

<blockquote><p>We need to take our production environment and make it so that a customer can run it on-premise. Oh and it has to be able to run entirely isolated and can&#8217;t always talk back to any external service.</p></blockquote>


<p>That&#8217;s pretty much the place enStratus found itself. enStratus is a cloud management platform. Not all clouds are public. Maybe someone wants to use your service but has regulatory needs that prevent them from using it as is. Maybe it needs to run entirely isolated from the rest of the world. There are valid reasons for all of these despite my general attitude towards security theater in the enterprise.</p>

<p>Now you&#8217;ve got an interesting laundry list in front of you:</p>

<ul>
<li>How do you teach someone to manage this organic &#8220;thing&#8221; you&#8217;ve built?</li>
<li>How do you take not one application but an entire company and its stack and shove it in a box?</li>
<li>Do you wrap up all the external components (monitoring, file servers, access control) and deal with those?</li>
</ul>


<p>We aren&#8217;t the first company to do this and we won&#8217;t be the last. Take a look at Github. They offer a private version of Github. But we&#8217;re not just talking about one part of Github - e.g. Gist. We&#8217;re talking about the entire stack the company runs to provide Github as we know it.</p>

<p>Unless you design with this in mind, you can&#8217;t really begin to understand how difficult of a task this can be. As I understand it, Github finally went the appliance route and offer prefab vms with some setup glue. Please correct me if I&#8217;m wrong here.</p>

<h1>Early iterations</h1>

<p>Obviously you can see that this was/is a daunting task. Original versions of our install process were based around a collection of shell scripts. Because of certain details of the stack (such as encryption keys for our key management system), we had to maintain state between each component of the stack when it was installed. Currently there are roughly 7 core components/services that make up enStratus:</p>

<ul>
<li>The console</li>
<li>The api endpoint</li>
<li>The key management subsystem</li>
<li>The provisioning subsystem</li>
<li>The directory integration service</li>
<li>The &#8220;worker&#8221; system</li>
<li>The &#8220;monitor&#8221; system</li>
</ul>


<p>and those are just the enStratus components. You also need RabbitMQ, MySQL and Riak (as we&#8217;re currently transitioning from MySQL to Riak). All of these things largely talk to each other over some web service or via RabbitMQ. With one or two exceptions, they can all be loadbalanced in an active/active type of configuration and scaled horizontally simply by adding an additional &#8220;whatever&#8221; node.</p>

<p>So the original installation process was a set of shell scripts that persisted some state and this &#8220;state&#8221; file had to be copied between systems. Yes, we could use some sort of external configuration store but that&#8217;s another component that we would have to install just to do the installation.</p>

<h1>Phase two</h1>

<p>One of my coworkers, <a href="https://twitter.com/zomgreg">Greg Moselle</a> was sort of &#8220;sysadmin number one&#8221; at enStratus. This was in addition to his duties as managing all customer installs. So he did what most of us would do and brute forced a workable solution with the original shell scripts. As enStratus started to offer Chef and Puppet support in the product, Greg gets this wild hair up his ass and thinks:</p>

<blockquote><p>I wonder if I can rewrite these shell scripts into something a bit more cross-platform and idempotent using chef-solo.</p></blockquote>


<p>You might be thinking the same thing I originally did that this was largely a bad idea. In my mind we had a workable solution for the interim in the existing shell scripts that had the install of enStratus down to a day or so. Pragmatism right? It&#8217;s also worth noting that this was how he wanted to learn Chef&#8230;</p>

<p>So off he goes and does what I recommend any new Puppet or Chef user does - exec resources all over the fucking place. Wrap your shell scripts in <code>exec</code>. Hardcode all the fucking things.</p>

<p>Once he did this, then I started working with him on some basic attributes to make them a bit more flexible. Before too long we had a stack of roles matching different components and we had moved everything from <code>cookbook_file</code> to <code>remote_file</code>. It was still a mess of execs but it worked.</p>

<p>But we still had this &#8220;state&#8221; we had to maintain between runs. This is not going away anytime soon. In production we store this state in attributes and use chef-server. We didn&#8217;t have that luxury here.</p>

<p>Then <a href="https://twitter.com/jimsander">Jim Sander</a> drops in and writes a small setup script that maintains some of that state for us. Basically a wrapper around raw <code>chef-solo</code>. Side note, if you ever need someone to drop some shell scripting knowledge on your ass, Jim&#8217;s the man to see. Ask him about his Tivoli days to really piss him off.</p>

<p>At this point, I start working on cleaning up the recipes as a sort of tutorial for folks. I&#8217;d pick a particular recipe and refactor it to all native resources and make it data driven. I&#8217;d commit these in small chunks so folks could easily see what the differences were easily - stuff like &#8220;instead of execing to call rpm, we&#8217;ll use the yum provider&#8221;.</p>

<p>At this point we&#8217;ve got something pretty far evolved from where we were. Now that we&#8217;ve got this workable chef-solo repository, I decide to hack out a quick Vagrantfile. The problem was it wasn&#8217;t entirely idempotent and we still had some manual steps that had to be dealt with. In addition to finishing up the recipes and ended up rewriting large chunks of the setup script. Now that I had something largely repeatable and localized, we suddenly had a Vagrant setup that folks could use for development. It wasn&#8217;t fully automated but it worked. We also still had this shared state thing.</p>

<p>So I set out to refactor the setup script a bit more. What&#8217;s important to keep in mind is that the primary use-case for this chef-solo repository wasn&#8217;t for Vagrant. This is our &#8220;installer&#8221;. The interesting part to me is that the improvements to how we do on-premise installs are coming as a direct result of making this work better with Vagrant. There&#8217;s a lot of wrapper work tied up in the setup script that wouldn&#8217;t need to be done if we used a base box that had more stuff baked in. However not baking stuff in actually gives us a more real-world scenario for installation.</p>

<p>Additionally we needed to be able to somehow pass user-specific configuration settings into the <code>vagrant up</code> process and get those into <code>chef-solo</code> by way of the setup wrapper. We have things like license keys, hostnames and my personal hated favorite - database credentials - that need to be handled in a way that we can make it so a developer can just type <code>vagrant up</code> and be running. If I have to require someone to edit a json file or anything else, the whole thing will fall flat on its face.</p>

<p>So any time we needed something like that, we added support to the setup wrapper and then used environment variables to pass that information in to vagrant.</p>

<h1>So how do we vagrant?</h1>

<p>We leverage environment variables pretty heavily in our Vagrantfile. If it&#8217;s something that someone might need to tune for whatever reason, it&#8217;s an environment variable that triggers an option to our setup script.</p>

<h2>Current list of tunables</h2>

<p>This is just a subset of the tunables we control via environment variables. The majority of these map directly to options for the setup script:</p>

<ul>
<li><code>ES_DLPASS</code> and <code>ES_LICENSE</code>: the basic set of credentials needed to fetch our assets and your personal license key.</li>
<li><code>ES_MEM</code>: this is the result of some of our front end developers having less memory than others.</li>
<li><code>ES_CACHE</code>: We have an office in New Zealand and bandwidth there is &#8220;challenging&#8221;. This allows us to cache as much as possible between calls to <code>vagrant up</code>. This not only triggers caching of system packages downloaded but also triggers the <code>prefetch</code> option in our setup script that predownloads all the assets. These assets are all stored in the <code>cache</code> directory of the repository which is not coincidently the value of <code>file_cache_path</code> in chef-solo. Remember that we may not always have external network access during installation so we offer a way to warm the <code>cache</code> directory with as many assets as possible.</li>
<li><code>ES_BOX</code>: let&#8217;s you specify an alternate base box to use. This is how we test the installer on different distros.</li>
<li><code>ES_DEVDIR</code>: shares an additional directory from the host to the vagrant image. This is how development is done (at least for me). I map this to the root of all of my git repository checkouts.</li>
<li><code>ES_VAGRANT_NW</code>: Allows you to configure bridged networking in addition to the host-only network we use.</li>
<li><code>ES_PROFILE</code>: This directly maps to an option in our setup script for persisting state between runs.</li>
</ul>


<p>There are other options that are specific to the enStratus product as well but you get the idea.</p>

<h2>The setup script</h2>

<p>I can&#8217;t post the full thing here but I can give you a general idea of how it works and some of the options it supports. This is a &#8220;sanitized&#8221; truncated version of the help output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: setup.sh [-h] [-e] [-f] -p &lt;download password&gt; -l &lt;license key&gt; [-s savename] [-c &lt;console hostname&gt;] [-n &lt;number of nodes&gt;] [-m &lt;mapping string&gt;] [-a &lt;optional sourceCidr string&gt;]
</span><span class='line'>-------------------------------------------------------------------------
</span><span class='line'>-p: The password for downloading enStratus
</span><span class='line'>-l: The license key for enStratus
</span><span class='line'>
</span><span class='line'>For most single node installations, specify the download password and license key.
</span><span class='line'>
</span><span class='line'>optional arguments
</span><span class='line'>------------------
</span><span class='line'>-h: This text
</span><span class='line'>-e: extended help
</span><span class='line'>-f: fetch-only mode. Downloads and caches *MOST* assets. Requires download password and *WILL* install chef
</span><span class='line'>-c: Alternate hostname to use for the console. [e.g. cloud.mycompany.com] (default: fqdn of console node)
</span><span class='line'>-a: Alternate string to use for the sourceCidr entry. You know if you need this.
</span><span class='line'>-s: A name to identify this installation
</span><span class='line'>-n: Number of nodes in installation [1,2,4] (default: 1)
</span><span class='line'>-m: Mapping string [e.g. frontend:192.168.1.1,backend:backend.mydomain.com]
</span><span class='line'>
</span><span class='line'>About savename:
</span><span class='line'>---------------
</span><span class='line'>Savename is a way to persist settings between runs of enStratus.
</span><span class='line'>If you specify a save name, a directory will be created under local_settings
</span><span class='line'>will be created. It will contain a YAML file with your settings as well 4 JSON files.
</span><span class='line'>
</span><span class='line'>The YAML file is the source of truth for the named installation. The JSON files MAY
</span><span class='line'>be recreated if the contents of the YAML file change. They exist to migrate between systems.
</span><span class='line'>If a save file is found, no other arguments are honored. If you need to change the 
</span><span class='line'>download password or license key, please update the YAML file itself
</span><span class='line'>
</span><span class='line'>If you lose this YAML file you will not be able to recover this enStratus installation.
</span><span class='line'>You should save it somewhere secure and optionally version it.</span></code></pre></td></tr></table></div></figure>


<h3>Persisting settings</h3>

<p>One of the &#8220;gotchas&#8221; we have is how do we basically build a node JSON file for chef-solo to use with any information we need to persist. Since we don&#8217;t know the state of all the systems involved when we go in, we have to &#8220;punt&#8221; on a few things. What we end up doing is something we call the <code>savename</code>. If you use this option, the settings you define will be persisted to a directory that git ignores called <code>local_settings</code>. This directory will contain directories named after the above <code>savename</code> parameter. The setup script (written for now in bash) will create a yaml file (easy to do in bash with HEREDOC as opposed to JSON) and also a copy of the generated encryption keys in a plain text file for the customer to store.</p>

<p>The only thing we can count on being on the system up front is the Chef omnibus install (since that&#8217;s a requirement). Instead of complicating things with ruby at this point (and chicken/egg issues since the setup script actually installs chef omnibus), we use the <code>erubis</code> binary that gets installed with omnibus to pass the yaml to to a JSON erb template. That generated JSON is the node json with attribute overrides. We actually support multi-node installation in the setup script if you provide a mapping of where certain components are running when calling setup. If you rerun setup using an existing <code>savename</code> parameter, the yaml file is updated (only certain values) and then regenerate the JSON file.</p>

<h1>The upshot</h1>

<p>The best part of all of this is that we can now say the same process is used when installing enStratus locally in Vagrant, in our dev, staging and production environments (though production uses chef-server) as well as what we install on the customer&#8217;s site. We version this repository around static points in our release cycle. We branch for a new release and create tags at given points in the branch based on either a patch release for enStratus itself in that release OR a patch to the installer itself.</p>

<p>It&#8217;s not all unicorns pooping rainbows. The process is much more complicated than it needs to be but it&#8217;s almost a world of difference from where it was when I started and it was entirely a team effort. This setup allowed us to do full testing to switch entirely off the SunJDK (and the need to manually download the JCE during customer installs) onto OpenJDK. We were able to migrate from Tomcat to Jetty and refactor our build process using this method. I was able to do this work without affecting anyone else. All I had to do when we were ready for full testing was tell everyone to switch branches, run <code>vagrant up</code> and test away.</p>

<h1>Special thanks</h1>

<p>I want to give a serious shout-out to Mitchell Hashimoto and John Bender for the work they did with Vagrant. Last year I said that no two software products impacted my career more than ElasticSearch and ZeroMQ. This year, without a doubt, Vagrant is at the top of that list.</p>

<h1>Addendum</h1>

<p>What follows is the sanitized version of our <code>Vagrantfile</code>. If anyone has any suggestions, I&#8217;m all ears:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="no">Vagrant</span><span class="o">::</span><span class="no">Config</span><span class="o">.</span><span class="n">run</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_BOX&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span> <span class="o">=</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_BOX&#39;</span><span class="o">]</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span> <span class="o">=</span> <span class="s2">&quot;es-dev&quot;</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box_url</span> <span class="o">=</span> <span class="s2">&quot;https://opscode-vm.s3.amazonaws.com/vagrant/boxes/opscode-ubuntu-12.04.box&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_VAGRANT_NW&#39;</span><span class="o">]</span> <span class="o">==</span> <span class="s2">&quot;bridged&quot;</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">network</span> <span class="ss">:bridged</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="c1"># If you change this address, the conditional logic</span>
</span><span class='line'>    <span class="c1"># in console.rb will break</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">network</span> <span class="ss">:hostonly</span><span class="p">,</span> <span class="s2">&quot;172.16.129.19&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># These entries allow you to run code locally and talk to a</span>
</span><span class='line'>  <span class="c1"># &quot;working set&quot; of data services</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">15000</span><span class="p">,</span> <span class="mi">15000</span>   <span class="c1"># api</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">3302</span><span class="p">,</span> <span class="mi">3302</span>     <span class="c1"># dispatcher</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">2013</span><span class="p">,</span> <span class="mi">2013</span>     <span class="c1"># km</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">5672</span><span class="p">,</span> <span class="mi">5672</span>     <span class="c1"># RabbitMQ (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">8098</span><span class="p">,</span> <span class="mi">8098</span>     <span class="c1"># Riak HTTP (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">8097</span><span class="p">,</span> <span class="mi">8097</span>     <span class="c1"># Riak protobuf (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">3306</span><span class="p">,</span> <span class="mi">3306</span>     <span class="c1"># MySQL (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">55672</span><span class="p">,</span> <span class="mi">55672</span>   <span class="c1"># RabbitMQ management interface</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_MEM&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">customize</span> <span class="o">[</span><span class="s2">&quot;modifyvm&quot;</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">&quot;--memory&quot;</span><span class="p">,</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_MEM&#39;</span><span class="o">]]</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">customize</span>  <span class="o">[</span><span class="s2">&quot;modifyvm&quot;</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">&quot;--memory&quot;</span><span class="p">,</span> <span class="mi">8192</span><span class="o">]</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DEVDIR&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">share_folder</span> <span class="s2">&quot;es-dev-data&quot;</span><span class="p">,</span> <span class="s2">&quot;/es_dev_data&quot;</span><span class="p">,</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DEVDIR&#39;</span><span class="o">]</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_CACHE&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="nb">puts</span> <span class="s2">&quot;Shared cache enabled&quot;</span>
</span><span class='line'>    <span class="no">FileUtils</span><span class="o">.</span><span class="n">mkdir_p</span><span class="p">(</span><span class="no">File</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span><span class="s2">&quot;apt&quot;</span><span class="p">,</span><span class="s2">&quot;partial&quot;</span><span class="p">))</span> <span class="k">unless</span> <span class="no">Dir</span><span class="o">.</span><span class="n">exists?</span><span class="p">(</span><span class="no">File</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span><span class="s2">&quot;apt&quot;</span><span class="p">,</span> <span class="s2">&quot;partial&quot;</span><span class="p">))</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">share_folder</span><span class="p">(</span><span class="s2">&quot;apt&quot;</span><span class="p">,</span> <span class="s2">&quot;/var/cache/apt/archives&quot;</span><span class="p">,</span> <span class="s2">&quot;cache/apt&quot;</span><span class="p">)</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">provision</span> <span class="ss">:shell</span> <span class="k">do</span> <span class="o">|</span><span class="n">shell</span><span class="o">|</span>
</span><span class='line'>    <span class="no">ES_LICENSE</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_LICENSE&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_DLPASS</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DLPASS&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_PROFILE</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_PROFILE&#39;</span><span class="o">]</span> <span class="o">||</span> <span class="s2">&quot;vagrant-</span><span class="si">#{</span><span class="no">Time</span><span class="o">.</span><span class="n">now</span><span class="o">.</span><span class="n">to_i</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="no">ES_LICENSE</span><span class="o">.</span><span class="n">nil?</span> <span class="ow">or</span> <span class="no">ES_DLPASS</span><span class="o">.</span><span class="n">nil?</span>
</span><span class='line'>      <span class="nb">puts</span> <span class="s2">&quot;You must set the environment variables: ES_LICENSE and ES_DLPASS!&quot;</span>
</span><span class='line'>      <span class="nb">exit</span> <span class="mi">1</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>    <span class="no">ES_CLOUD</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_CLOUD&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_CIDR</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_CIDR&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_DEBUG</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DEBUG&#39;</span><span class="o">]</span> <span class="o">||</span> <span class="kp">false</span>
</span><span class='line'>    <span class="n">setup_opts</span> <span class="o">=</span> <span class="s2">&quot;-l </span><span class="si">#{</span><span class="no">ES_LICENSE</span><span class="si">}</span><span class="s2"> -p </span><span class="si">#{</span><span class="no">ES_DLPASS</span><span class="si">}</span><span class="s2"> -s </span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2"> &quot;</span>
</span><span class='line'>    <span class="n">setup_opts</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;-c </span><span class="si">#{</span><span class="no">ES_CLOUD</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="no">ES_CLOUD</span>
</span><span class='line'>    <span class="n">setup_opts</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;-a </span><span class="si">#{</span><span class="no">ES_CIDR</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="no">ES_CIDR</span>
</span><span class='line'>    <span class="no">ES_DEBUG</span> <span class="p">?</span> <span class="n">chef_opts</span><span class="o">=</span><span class="s2">&quot;-l debug -L local_settings/</span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2">/chef-run.log&quot;</span> <span class="p">:</span> <span class="s2">&quot;&quot;</span>
</span><span class='line'>    <span class="n">shell</span><span class="o">.</span><span class="n">inline</span> <span class="o">=</span> <span class="s2">&quot;cd /vagrant; ./setup.sh </span><span class="si">#{</span><span class="n">setup_opts</span><span class="si">}</span><span class="s2">; chef-solo -j local_settings/</span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2">/single_node.json -c solo.rb </span><span class="si">#{</span><span class="n">chef_opts</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_POSTRUN&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">provision</span> <span class="ss">:shell</span> <span class="k">do</span> <span class="o">|</span><span class="n">shell</span><span class="o">|</span>
</span><span class='line'>      <span class="n">shell</span><span class="o">.</span><span class="n">inline</span> <span class="o">=</span> <span class="s2">&quot;chef-solo -j /vagrant/local_settings/</span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2">/single_node.json -c /vagrant/solo.rb -o </span><span class="se">\&quot;</span><span class="si">#{</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_POSTRUN&#39;</span><span class="o">]</span><span class="si">}</span><span class="se">\&quot;</span><span class="s2">&quot;</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What production means]]></title>
    <link href="http://lusis.github.com/blog/2012/07/09/what-production-means/"/>
    <updated>2012-07-09T21:31:00-04:00</updated>
    <id>http://lusis.github.com/blog/2012/07/09/what-production-means</id>
    <content type="html"><![CDATA[<p>This post is something that&#8217;s been brewing for a while. While it may sound targeted in tone, it&#8217;s more general than that. Let&#8217;s just call it an open letter to family, friends and coworkers around the world.</p>

<!-- more -->


<p>One thing that I have the hardest time communicating to friends and family who aren&#8217;t in the IT industry is the concept of &#8220;production&#8221; and what it means to be on-call. Even coworkers have a hard time understanding what it means.</p>

<p>The topic recently came up again and the confusion bothered me so much that I resolved to write this blog post as soon as humanly possible.</p>

<h1>A few clarifications</h1>

<p>I want to clarify a few very important things:</p>

<ul>
<li>I&#8217;m not whining about what I do</li>
<li>I love what I do</li>
<li>I&#8217;m not burned out</li>
<li>I&#8217;m not being self-important</li>
<li>I&#8217;ve always had a hard time &#8216;ranking&#8217; problems. EVERYTHING is important to me.</li>
<li>I&#8217;m not really interested in critiques of what SHOULD have been done. Riak wasn&#8217;t around, for instance, when I was managing the financial stuff for instance.</li>
<li>Yes, rotations are important but not always viable. Luckily we have a solid rotation at enStratus.</li>
</ul>


<h1>What production means to me (and why)</h1>

<p>Production environments take many forms. It&#8217;s even harder to define. For me, production has always meant &#8220;any system, service or component that the business requires to do business&#8221;.
I&#8217;ve worked in several different companies over the last 17 years. In some cases, production was an ERP system or a file server. In other cases, production was a web presence. What&#8217;s interesting each of these is that in some cases, production had a time associated with it.</p>

<p>Let&#8217;s take a few of these and compare them:</p>

<h2>The retail financial company</h2>

<p>Long ago I worked for a company that did payday and title loans. We operated over 600+ retail locations from east to west coast. The primary application used by these outlets was a web-based loan management application (websphere + db2). Stores were located across the country and store hours were from 9AM to 6PM (IIRC). From this you might think &#8220;production was the web application and it needed to be online from 9AM EST to 10PM EST&#8221;. You would be correct at the highest level.</p>

<p>However employees started the day at 8AM (lining up customers to call and what not) and left at 7PM (closing out books for the day). After the last store went &#8220;offline&#8221;, we began various nightly batch jobs. Being that this was financial in nature, batch jobs were the norm. We also had backups that had to run as we rebuilt our QA database nightly from sanitized production database backups. If the batch jobs didn&#8217;t finish in time, we actually had to delay the start of the day for the stores. Our datawarehouse was also loaded from a secondary copy of the database restored from backup. If I recall the main reason for this was that our nightly window was SO crunched for time that we couldn&#8217;t even load the warehouse from the main database because we had to start various batch jobs as soon as backup was done.</p>

<p>But that&#8217;s just the main system. We had ancillary services as well. None of the retail outlets had access to the internet except through a squid proxy. There were print servers that did server-side check printing. In the backoffice, we had collections and other things that depending on the reports that came out of the data warehouse. We had nightly backups of the LAN stuff. DNS servers that the stores had to use. VPN concentrators. ALL of this had the same SLA as production.</p>

<p>All told, I recall the final number for any business hours outage as costing us something like $100k for 15 minutes of downtime.</p>

<h2>The Learning Management System</h2>

<p>This system was used by a charter school system in the state of Ohio. It was an online classroom system that provided education for at-risk students. This WAS the school for these kids. Obviously it had normal school hours but since the students had no physical textbooks of any kind, the system HAD to be online for something as simple as homework. As with the previous setup, there were all sorts of ancillary services that we had to have available. All of the static content was shared across all of the tomcat servers via a SAN (OCFS2 - I have scars). It was backed by MySQL. We still had to do backups. We had to maintain connectivity. Everything we had was &#8216;production&#8217;. Since we had developers in other countries on different hours, we burned what money we had when the development environment or our SVN repo was offline. That was production too.</p>

<h2>Web applications in general</h2>

<p>To those in the industry I&#8217;m not telling you anything new. But to those not in the know, the internet doesn&#8217;t have office hours. Yes, you can gauge where your largest userbase is but take a system like enStratus.</p>

<p>It manages cloud resources for people all across the world. For many of these people, the only access they have to their cloud account is VIA enStratus. Take AWS for instance. enStratus is responsible for detecting outages and replacing components in the infrastructure for these companies or autoscaling to meet some demand. If enStratus is offline, these actions are NOT being taken on behalf of the user. The biggest fear for me is that enStratus is offline when AWS is having an outage. Some customers are paying us for this use case alone. Mind you in the last few outages, not even enStratus could fix the problem because of control plane issues. One thing enStratus can do is scale across multiple clouds so even AWS control plane issues are no excuse.</p>

<p>enStratus production itself is a pretty complex beast. The stack in general is designed to be fairly &#8220;SOA&#8221;. We use RabbitMQ pretty heavily for workers. However we&#8217;ve had some issues in the recent past where our workers were getting OutOfMemory exceptions. We run multiple workers (obviously) but in this case an OOM on one worker would eventually translate into OOMs across all the workers. When all the workers OOMd, they would stop processing messages from the queue. When that happened, RabbitMQ could eventually tank from units of work waiting to be picked up. We never had this happen, mind you but that was the end game.</p>

<p>This meant we had to be diligent on these OOMs. All the time. 24x7x365.</p>

<p>Luckily this problem was fairly short-lived but until the bug was identified and fixed (it happened to be related to an edge case with S3 bucket sizes), we had to be on guard for these OOM exceptions.</p>

<h1>What does it mean?</h1>

<p>The thing that I want to get across is that &#8220;production&#8221; is DIRECTLY related to the bottom line of the business. If &#8220;production&#8221; is offline, customers can&#8217;t use the system. Customers are unhappy. If customers are unhappy they eventually go elsewhere. If customers go elsewhere the company loses money. If the company loses money eventually the company lays people off. This isn&#8217;t rocket science. We talk about complex systems and cascading failures. This is a cascading failure that means someone doesn&#8217;t have a job at Christmas.</p>

<p>Yes, I take it that seriously. When I talk about production, that&#8217;s what I mean.</p>

<h1>On-call</h1>

<p>Now that you know what production means and what impact it has, I shouldn&#8217;t need to say much about being &#8220;on-call&#8221;. Yes, I&#8217;m on-call now and then. Yes, just like a doctor. Sometimes, depending on the company, I&#8217;ve been the only person on-call. Ever. No rotation. No help. Just me. The one person keeping shit running all the time so that customers (internal or external) aren&#8217;t impacted by the slightest glitch in the system. Yes we should build more resilient systems and we strive to do that. However, tech debt is a thing. It&#8217;s not always immediately an option.</p>

<p>So when I&#8217;m on-call it means I&#8217;m the person responsible for production as defined above and all the baggage that comes with it.</p>

<p>Someone once said to me &#8220;Most of us work nights and weekends, although we try to balance it when we can.&#8221;</p>

<p>While I appreciate the sentiment, I don&#8217;t just work nights and weekends, I work ALL the time. I have to be able to respond at a moments notice to a production issue when I&#8217;m on-call. Think about that for a moment. I have to essentially be 15 minutes from a working internet connection and may need to sit in front of a computer for an unspecified amount of time until the issue is resolved. I can&#8217;t just go to bed or decide that I&#8217;ve worked enough for the day.</p>

<h1>The joys of working remote</h1>

<p>I&#8217;ve been pretty fortunate in the past several years to be in a situation where I can support production from pretty much anywhere. I need at most a 3G connection and my laptop. I can VPN into production and fix most any problem. Yes, I always bring my laptop on &#8220;vacation&#8221;. If I don&#8217;t have a decent signal, then there&#8217;s a chance I&#8217;ll have to drive to the McDonald&#8217;s 15 minutes away to use the wifi. I try not to be on-call when I&#8217;m on vacation but sometimes it&#8217;s not always an option.</p>

<p>I just got back from visiting my in-laws Up North in Michigan. The only time I was able to get a single bar of 3G was late at night when enough subscribers stopped using the cell towers. Up until a year or so ago, there was no option for any sort of broadband internet in the area.</p>

<p>I&#8217;ve finally gotten my in-laws to understand much of what I&#8217;ve written here. They&#8217;ll be wiring the cottage for cable internet so that I can take the family up for longer periods of time. Yes, I&#8217;ll be &#8220;working&#8221; on vacation but I&#8217;d rather work a bit on vacation and stay longer than have to cut the trip short just so I can get back.</p>

<h1>Poor poor me</h1>

<p>As I said at the start, please don&#8217;t misinterpret what I&#8217;m saying as whining or some sort of god complex. This is the field I&#8217;ve chosen. I love what I do and I take that responsibility very seriously. As we start to build more resilient systems and accept that failure WILL happen and design for it, things will only get better. I still remember the first time I was able to replace a problematic system from scratch in 20 minutes via configuration management and call it a night.</p>

<p>As enStratus migrates its backend to Riak DS and builds out secondary and tertiary datacenters, I look forward to losing a data node not being the end of the world. DevOps über alles and all that.</p>

<p>So dear friends and family, the next time someone tells you they&#8217;re on-call for production&#8230;.cut &#8216;em some slack.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why EBS was a bad idea]]></title>
    <link href="http://lusis.github.com/blog/2012/06/15/why-ebs-was-a-bad-idea/"/>
    <updated>2012-06-15T09:41:00-04:00</updated>
    <id>http://lusis.github.com/blog/2012/06/15/why-ebs-was-a-bad-idea</id>
    <content type="html"><![CDATA[<p>Since I just tweeted about this and I know people would want an explaination, I figured I&#8217;d short circuit 140 character hell and explain why I think EBS was the worst thing Amazon ever did to AWS.</p>

<!-- more -->


<p><em>First time I&#8217;ve had to do this but: the following is my personal opinion and in no way reflects any policy or position of my employer</em></p>

<h1>A journey through time</h1>

<p>I remember when EC2 was first unleashed. At the time I was working at Roundbox Media (later Roundbox Global - because we had an office in Costa Rica!). I was asked frequently if we could possibly host some of our production stuff there.</p>

<p>It was pretty much a no go from the start:</p>

<ul>
<li>No persistent IPs</li>
<li>No persistent storage</li>
</ul>


<p>Sure we could bake a bunch of stuff into the AMI root but the ephemeral storage wasn&#8217;t big enough to hold a fraction of our data. Still, we leveraged it as much as possible. We ran quite a bit of low risk stuff on it - test installs of our platform and demo sites for customers.</p>

<p>After I left RBX in Feb of 2008, I didn&#8217;t get an opportunity to work with AWS for a year or so and by then quite a bit had changed. If Amazon does one thing really well, it&#8217;s iterating quickly on its service offerings.</p>

<h1>So why is EBS a bad thing?</h1>

<p>For Amazon, EBS is NOT a bad thing. It was probably one of the smartest business moves they made (along with Elastic IPs). They could now claim that EC2 was JUST like running your own kit - you have a SAN! you have static IPs!</p>

<p>The problem is it&#8217;s not.</p>

<h1>The nature of block storage</h1>

<p>Anyone who&#8217;s dealt with any sort of networked filesystem knows the pains it can cause with certain application profiles. Traditional databases are notorious for expecting actual local storage and real block devices. It amazes me the number of people who put up with the pain of running a database in something like vmware using virtual disks hosted on an NFS device.</p>

<p>The point is the block devices have specific semantics and presumptions.</p>

<p>With EBS you&#8217;re promised a tasty block device that your OS can address as if it were local disk. Only it&#8217;s not&#8230;.</p>

<h2>Latency</h2>

<p>Let&#8217;s get the biggest elephant out of the way. EBS is a block device to the OS but under the hood it&#8217;s using the network. It may or may not be shared with non-block device traffic but it&#8217;s still subject to network latencies. God I hope that EBS at least gets its own port on the host side&#8230;</p>

<h2>Shared</h2>

<p>There&#8217;s a whole lot of sharing going on here to:</p>

<ul>
<li>local bandwidth from the physical server where your instance is to a given EBS subsystem (array, CEC, whatever)</li>
<li>aggregate bandwidth from all pysical servers talking to a given EBS subsystem</li>
<li>disk I/O itself on a given EBS subsystem</li>
</ul>


<p>I don&#8217;t know how the connection from server to EBS is done. I would hope at least there are bonded ports or multiple uplinks/multipathing going on. I would REALLY hope that network I/O and Disk I/O are not on the same channel. Regardless, you&#8217;re still sharing whatever the size of that connection is with everyone else on the physical server your instance is on if they&#8217;re using EBS as well.</p>

<p>And the physical EBS array where your volume is? Depending on the size of your EBS volume, you&#8217;re dealing with network I/O on that unit&#8217;s connection from an unknown number of other customers. And to top it off, you&#8217;re not just sharing network bandwidth, you&#8217;re sharing disk bandwidth as well. There are still spindles under there folks. Sticking an API in front of it doesn&#8217;t change the fact that there is spinning rust under the covers.</p>

<p>Above ALL of that, you&#8217;ve got competing workloads - sequential vs random read.</p>

<p>Sure, just stick your root OS volume on that. That&#8217;s a great idea.</p>

<h1>Mixed messages</h1>

<p>To me, however, the biggest problem with EBS is not the latency. It&#8217;s not the shared resources. It&#8217;s not even taking something that is fundamentally locality oriented and trying to shoehorn it into something distributed.</p>

<p>It&#8217;s the fact that it sends the wrong damn message. I&#8217;ve said this before, I&#8217;ll say it again and I&#8217;ll stand by it.</p>

<p><strong>Unless you are willing, able or have designed your applications to have any single part of your infrastructure - connectivity, disk, node, whatever - ripped from under you with no warning whatsoever, you should not be running it on Amazon EC2.</strong></p>

<p>By providing EBS, Amazon sends the message that &#8220;you can treat this just like your own datacenter&#8221;. Just use EBS and you can treat it just like a SAN. Look, we have snapshots!</p>

<p>Hell, I get pissy when folks refer to instances as &#8220;boxes&#8221; and talk about them like they&#8217;re something they physically own. Stop trying to map physical datacenter analogies to AWS. It won&#8217;t work and you&#8217;ll be disappointed.</p>

<p>You want to know the real kicker? You should be designing like this ANYWAY. Yes, you have much greater control over failure points when you run everything yourself. You have much greater control over resource sharing and I/O profiles. That doesn&#8217;t remove the need to design for failure. How far you take it is up to you (and realistically your budget) but when you&#8217;re running on AWS, you need to be much more attentive to it.</p>

<h1>For the record</h1>

<p>I still think AWS and public clouds are awesome. I really do. I think private clouds are just as awesome. The flexibility they offer is almost unmatched but that flexibility comes at a price - performance hits, multiple layers of abstraction and other things.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring sucking just a little bit less]]></title>
    <link href="http://lusis.github.com/blog/2012/06/05/monitoring-sucking-just-a-little-bit-less/"/>
    <updated>2012-06-05T12:22:00-04:00</updated>
    <id>http://lusis.github.com/blog/2012/06/05/monitoring-sucking-just-a-little-bit-less</id>
    <content type="html"><![CDATA[<p>So <a href="http://blog.zenoss.com/2012/06/turning-monitoringsucks-into-monitoringsucksless">it&#8217;s come to my attention</a> that today is the &#8220;anniversary&#8221; of when I wrote my <a href="http://blog.lusis.org/blog/2011/06/05/why-monitoring-sucks/">first &#8220;#monitoringsucks&#8221; blog post</a>.</p>

<!--more-->


<p>So the question I found myself asking today is &#8220;Does monitoring suck less than it did a year ago?&#8221;. I&#8217;d have to be an idiot to say anything other than &#8220;yes&#8221;.</p>

<p>I didn&#8217;t set out to start a &#8220;movement&#8221;. I&#8217;m not someone who handles compliments very well. Mainly because I suffer from a bad case of <a href="http://kartar.net/2012/05/imposter-syndrome/">imposter syndrome</a>. The other side of this is that I know that I&#8217;ve done next to nothing to make it any better. Meanwhile the real heros are people releasing code every day. People rethinking how we think about monitoring, trending, alerting and everything else. Companies like Etsy, Netflix, Yammer and countless more are sharing the deep squishy bits of how they do things and are releasing code to back it up.</p>

<h1>What&#8217;s gotten better?</h1>

<p>I think the biggest thing that&#8217;s gotten better is that people are really starting to leverage advances we&#8217;ve had in ancillary tooling in recent times. Not that any of these ideas are new (message busses existed long before RabbitMQ) but the barrier to entry is much lower.</p>

<p>I still feel like what drove this was a byproduct of configuration management uptake. As it became easier to stamp out servers, the rate of change in our infrastructure surpassed what tools were original designed to deal with. As we started having more time to think about what we wanted to monitor (because we weren&#8217;t spending all our time building hand-crafted artisan machines), we started to feel the pain more. We wanted our monitoring systems to work as smoothly as the rest of the kit but it didn&#8217;t.</p>

<p>Combine that with:</p>

<ul>
<li>We now have data storage engines of varying complexity for storing time-series data. And with greater resolution and the ability to change that resolution.</li>
<li>We have tooling that can read that timeseries data and represent it in dynamic ways.</li>
<li>We revisted the idea of push vs. pull and leave/join of components in our infra.</li>
</ul>


<p>The world is a brighter place because of this and to everyone who had something to do with it - whether discussing on a mailing list, talking on IRC, tweeting, writing code or whatever - thank you.</p>

<h1>What&#8217;s coming down the pipe</h1>

<p>As a side effect of this monitoring thing, people ask my opinion a lot. Like I said, I&#8217;m still weirded out by this. Stepping back, though, and just geeking out on things, I see some really cool stuff in the future. Here&#8217;s just a subset of &#8216;stuff&#8217; I&#8217;ve been thinking about:</p>

<h2>Presence-based Discovery</h2>

<p>I couldn&#8217;t think of a better way to describe it but the idea is simply (or not so simply) that by virtue of coming online, a system is saying &#8220;I wish to be monitored in this way&#8221;. This is pretty dependent on configuration management for this to go smoothly, imho.</p>

<p>I mentioned this in a post on the devops toolchain but it goes something like this:</p>

<ul>
<li>new node comes online</li>
<li>new node registers its presence in some way (I&#8217;m kind of keen on the XMPP idea) with a notification of services it offers</li>
<li>centralized system is monitoring (har har) this presence system and starts monitoring the system based on predefined criteria at a &#8220;well-known&#8221; endpoint</li>
<li>optionally the system can dictate what it wants monitored</li>
</ul>


<p>Obviously this would all be very painful without some sort of configuration management system. However, it&#8217;s very easy for me in my base group or role for a system to say &#8220;Install <code>W</code>, register via <code>X</code>, listen on <code>Y</code> for active checks and publish everything to <code>Z</code> endpoint&#8221;. What <code>W</code>,<code>X</code>,<code>Y</code> and <code>Z</code> are is irrelevant. We can cookie cutter this stuff. FWIW this is nothing new. We&#8217;re just seeing &#8220;consumer-grade&#8221; options that are usable by everyone.</p>

<h2>Push vs Poll</h2>

<p>I&#8217;ve said many times that poll-based monitoring is dead. That&#8217;s a bit of hyperbole. What&#8217;s dead is the idea that we can only check <code>X</code> every <code>N</code> times over <code>K</code> period. This is a hold-over from ineffecient polling mechanisms that would crumble under too-frequent polling as well as systems that weren&#8217;t able to handle being polled that often. I see polling moving from &#8220;check host <code>X</code> every 5 minutes for memory usage&#8221; to &#8220;watch this bus for memory usage stats and if there&#8217;s not anything in 2 minutes, make sure the world is okay&#8221;. We&#8217;ll always need the outside-in checks of things but that&#8217;s much less intensive than polling ALL the things.</p>

<p>We&#8217;re so close with tools like Graphite now which can accept arbitrary metrics from anywhere with no need to preconfigure it to accept them. There are some concerns here around bad data being injected from unauthorized sources. As we automate more and make decisions based on this data, we need to be aware of it. Another discussion for another day though something akin to the way mcollective does trust is probably in order.</p>

<h2>Self-service and Culture</h2>

<p>This is a big one too and I think will cut down on many complaints that people have around even something like Nagios.</p>

<p>We have to be able to say &#8220;You know what? We don&#8217;t need to monitor that. Let&#8217;s disable that check&#8221;. If something is unactionable, then why the hell are you alerting on it? This is where decoupling the trending/visualization from alerting can be so powerful. I&#8217;m currently rebuilding our monitoring setup to do most checks based on data in Graphite. Why? Because if I flip the relationship around, I&#8217;ve now got to deal with the alert question before I can even get the information. Instead of alerting on data and then storing it as an afterthought (perfdata anyone?) let&#8217;s start collecting the data, storing it and then alerting based on it.</p>

<p>This also provides for options around self-service. Not everyone needs to know about the disk space on nodeX. Only the people who can fix it do. Maybe your database folks want to get alerts on queries taking longer than N. As an operations person, you can&#8217;t do anything about that and certainly not at that moment (in most cases). You&#8217;re just going to push it down the line ANYWAY. And do you really want to have to deal with changing thresholds on behalf of someone?</p>

<p>I&#8217;m also a big fan of the idea that components in your infrastructure - applications, os, whatever - self-host a pubsub type of endpoint where users can get realtime information about the system themselves. I do this with every logstash install I setup where possible. Every remote logstash agent I&#8217;ve setup in enStratus also provides a pubsub 0mq socket that you can use to live tail log information from that host broken down in topic keys around metadata.</p>

<h2>Application health by default</h2>

<p>I&#8217;ve fawned over Coda Hale&#8217;s &#8220;Metrics&#8221; talk several times. I&#8217;m far from a fanboy but &#8220;Metrics&#8221; gets it right. This ties into self-service quite a bit. Developers need to be free to instrument the application without creating &#8220;yet another place to look&#8221;. Metrics does this so well with the idea of pushing instrumentation out of the application (oh look - push again!) and into Graphite or whereever is appropriate. And if you aren&#8217;t ready for push yet, you can still poll via JMX.</p>

<p>The idea has been ported to multiple other languages at this point. There no excuse not to deeply instrument your applications. If you have an application that CAN&#8217;T be instrumented properly, maybe you should consider a different application?</p>

<h2>Applying science and common sense</h2>

<p>The last thing that I see as being a step forward is we start applying science to our process. No more <code>-w 60,60,60 -c 75,75,75</code> canned thresholds. We start thinking about our thresholds. Maybe we do away with them entirely as static constructs. Instead we apply event processing and historical data to build thresholds that are intelligent.</p>

<p>We start looking at the shape of our infra. Is it REALLY important that you get woken up at 3AM because a Riak node is down? Not if it&#8217;s just one but maybe if it&#8217;s two depending on your cluster size. Maybe both of those nodes were in the same rack. Okay that&#8217;s bad.</p>

<p>We start to consider context and start applying science! We step out of the shamanistic ages of monitoring (who the hell still sets swap space to double physical memory and is 75% of a 1TB volume in use really something that can&#8217;t wait?) where &#8220;We&#8217;ve always done it this way&#8221;. Start thinking like the Apple 1984 commercial, whip out your hammer and smash your preexisting notions around what constitutes an alert.</p>

<h1>What I&#8217;m building</h1>

<p>Right now I&#8217;m spending most of my time being pragmatic. I&#8217;m still adding new checks to Nagios but the information is coming from different sources now. I&#8217;m dumping data into graphite via logstash and doing checks on that. Collectd is now pushing directly to graphite as well. Nagios is becoming less and less of a factor. When I finally strip it down to its bare essentials, I&#8217;ll have a better idea what gap needs to be filled. It&#8217;s starting to look like riemann at that point.</p>

<p>I still want to tackle this presence based idea in some form. Even if presence is just a signal to run chef-client. At my previous company, we used Noah for this. I&#8217;ve not yet had time to decide if Noah is the right fight here.</p>

<h1>What others are building</h1>

<p>What&#8217;s more important is what others are building. There are too many to list but I&#8217;m going to give it a shot off the top of my head. Please don&#8217;t take offense if your project isn&#8217;t here.</p>

<ul>
<li>Sensu</li>
<li>Graphite-tattle</li>
<li>Cepmon</li>
<li>Logstash</li>
<li>Librato</li>
<li>PagerDuty</li>
<li>Umpire</li>
<li>alerting-controller</li>
<li>Graphite</li>
<li>Statsd</li>
<li>Logster</li>
<li>Metrics</li>
<li>Incinga (yes, they&#8217;re starting to diverge from Nagios)</li>
<li>ZeroMQ</li>
<li>Chef</li>
<li>Puppet</li>
<li>Zookeeper</li>
<li>Riemann</li>
<li>OpenTSDB</li>
<li>Ganglia</li>
<li>TempoDB</li>
<li>CollectD</li>
<li>Datadog</li>
<li>Folsom</li>
<li>JMXTrans</li>
<li>Pencil</li>
<li>Rocksteady</li>
<li>Boundary</li>
<li>Circonus</li>
<li>GDash</li>
</ul>


<p>Probably the best bet is to head over to <a href="https://github.com/monitoringsucks/tool-repos">the monitoringsucks tool repo on github</a>. I can&#8217;t do the awesomeness of what people are doing justice here.</p>

<p>and so many more.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Configuration Drift and Next-gen CM]]></title>
    <link href="http://lusis.github.com/blog/2012/05/24/configuration-drift-and-next-gen-cm/"/>
    <updated>2012-05-24T01:10:00-04:00</updated>
    <id>http://lusis.github.com/blog/2012/05/24/configuration-drift-and-next-gen-cm</id>
    <content type="html"><![CDATA[<p>It always starts with a tweet. However it normally doesn&#8217;t start with a tweet from <a href="https://twitter.com/moonpolysoft">Cliff Moon</a>.</p>

<blockquote><p>Of all the problems to fix in chef or puppet, the diffusion and drift of state that occurs in idiomatic usage seems highest priority.</p></blockquote>




<!-- more -->


<p>Now for sure what spawned this comment was something unrelated but it got me thinking. Oddly enough <a href="https://twitter.com/dysinger">Tim Dysinger</a> was either poking around in my head or just had the same idea:</p>

<blockquote><p>Devops tools should move towards an active assertion of state (instead of passive/polling). This the next-level.</p></blockquote>


<p>Tim and I hooked up via Skype and bantered about this stuff back and forth. We were on the same wavelength. That&#8217;s pretty cool because Tim is pretty fucking smart (and he was able to explain Maybe Monads to me over dinner).</p>

<h1>My thoughts on the subject</h1>

<p>What follows is something of a brain dump on what both Cliff and Tim had to say. However I&#8217;m going to be scoping in the context of security because</p>

<ul>
<li><a href="https://twitter.com/beaker">Beaker</a> gave a <a href="http://www.rationalsurvivability.com/presentations/SMCES-Gluecon2012.pdf">presentation at Gluecon today</a> (<em>warning! bigass pdf</em>)</li>
<li>I work with <a href="https://twitter.com/mortman">David Mortman</a> who is one of the folks I say &#8220;gets it&#8221; w.r.t configuration management and security</li>
<li>Security was the FIRST context that came to mind</li>
<li>I was lucky enough to be involved with <a href="https://twitter.com/markburgess_osl">Mark Burgess</a>, <a href="https://twitter.com/cjeffblaine">Jeff Blaine</a> and <a href="https://twitter.com/filler">Nick Silkey</a> about a very similar topic where Mark said</li>
</ul>


<blockquote><p>Good point. I wonder why folks often tear down a perfectly good machine and rebuild it instead of fixing what is broken.</p></blockquote>


<p>What I&#8217;m going to say isn&#8217;t new to anyone and smarter folks than I are already working on this (I&#8217;m sure) but this is the Internet. I get to babble too!</p>

<h2>On Drift</h2>

<p>So what exactly <em>IS</em> the problem here? What&#8217;s configuration drift and how the hell does it even happen?</p>

<p>The problem here is that, as Tim said, configuration management systems aren&#8217;t assertive enough. Look at how a typical CM client run behaves:</p>

<ul>
<li>Hey guys, cm is running</li>
<li>Oh look, this file doesn&#8217;t look like it&#8217;s supposed to</li>
<li>/me changes file</li>
<li>File looks good</li>
<li>Hey guys, cm isn&#8217;t running anymore</li>
</ul>


<p>That last line is part of the problem. I&#8217;ve talked about my Noah project to largish groups of folks (both Puppet and Chef users) a few times now and the answer to the question of &#8220;Do you leave puppet (or chef) running in the background?&#8221; has always been &#8220;No&#8221;. There are plenty of valid reasons for this but this is what I would consider the primary cause of drift at the node level. Now maybe this isn&#8217;t EXACTLY what Cliff was talking about. I&#8217;m not quite on his level so I sometimes misinterpret but when I heard &#8216;drift of state that occurs in idiomatic usage&#8217;, this was what came to mind.</p>

<p>The thing is that these tools are designed to verify state of a resource at the point they run</p>

<ul>
<li>Does this file look right? No! Fix it.</li>
<li>Is this service running? Yes! Cool.</li>
</ul>


<p>And then they go away. They don&#8217;t manage the state of those resources until they next inspect them. The act of managing those resources is not in response to those resources changing but in response to a user ASKING them to be checked. I would even wager that when a user runs <code>chef-client</code> the first thing on her mind isn&#8217;t &#8220;I sure hope chef fixes my sudoers file&#8221; but &#8220;I need chef to update my Nagios configs again&#8221;. The incorrect state of the sudoers file isn&#8217;t really even thought about. That&#8217;s because we shove that stuff into some &#8220;base&#8221; role or group. Something that&#8217;s applied to all nodes in our infrastructure. We don&#8217;t think of a node as being a &#8220;managed sudoers&#8221; node. We think of it as a &#8220;web server&#8221;.</p>

<p>Additionally, because we aren&#8217;t in a constant state of verification about these resources, we may have drift that occurs across nodes of different types whilst they share a common base block. Sure I just ran my CM tool to update my Nagios server but what about my web servers? I don&#8217;t want to run it there because I <strong>KNOW</strong> nothing has changed in the web server role.</p>

<p>To me, this is the &#8220;idiomatic&#8221; usage Cliff spoke about. The tools encourage us to think in terms of composition and reusable patterns but the final function of the node is the way we classify it. Mind you, the answer here is really to run your CM tool in the background but that still doesn&#8217;t take us to the next level. We&#8217;re still exposed to drift even if it&#8217;s for a short period of time. What&#8217;s worse is these tools operate by default with a splay value. This actually makes the drift exposure even worse as you can&#8217;t even guarantee that it will run at the interval specified.</p>

<p>I first heard about CFEngine when Mark talked about &#8220;Anomoly Detection&#8221; at LISA &#8216;04 in Atlanta. My mind was blown but I could never get past the idea that I couldn&#8217;t dictate state immediately. The idea (partially a naive understanding on my part) that systems would not become X when I said &#8220;Become X&#8221; bothered me. The idea that systems have a personality that needs to be respected bothered me.</p>

<p>The point here is that when I want a system to look like X, I want it to look like X right then. I want it to STAY looking like X and I don&#8217;t want it to try and account for localized variations. I might feel differently if I were managing a network of servers that were essentially treated like desktops.</p>

<h2>But does drift really matter?</h2>

<p>Yes and no. If you&#8217;re living the cloud life, probably not. The reason is that resources tend to have a short shelf life. If I&#8217;m autoscaling via &#8216;the cloud&#8217; to meet capacity demands then it&#8217;s highly likely that those systems won&#8217;t be around long enough to drift that far. In the land of configuration management, drift is largely time driven. The longer systems stay around, the greater the chance for drift.</p>

<p>However if you&#8217;re running physical hardware that you don&#8217;t tear down regularly, then drift is likely to become more pronounced. Interestingly enough there&#8217;s a psychological factor at play here. Systems become like pets instead of cattle. We become attached to them. &#8220;Oh that&#8217;s just db1 acting up again. You know it&#8217;s like the oldest one in the fleet&#8221;. People start forgetting that the system will fail (<em>Everything fails. Embrace failure!</em>). The start storing one-off scripts on there. Maybe it&#8217;s core kit and while it&#8217;s managed with Puppet, it&#8217;s not frequently touched.</p>

<p>Here&#8217;s another interesting point. As time progresses and modules, cookbooks, recipes, bundles, promises whatever are more infrequently touched, the confidence in them goes down. I&#8217;ve frequently found myself saying &#8220;Shit&#8230;I wrote that code like&#8230;I don&#8217;t even fucking remember when. I have no idea what would happen if I ran it now.&#8221;. This uncertainty eventually leads to me MANUALLY COMPARING the current state of resources that would be modified with the versions that would be generated. I&#8217;ve even copied comment blocks wholesale from one-off changes I&#8217;ve made into ERB templates just to ensure that a service restart didn&#8217;t happen.</p>

<p>How fucked is that? Pretty fucked, Alex.</p>

<p>This partially leads into a quote from <a href="https://twitter.com/allspaw">John Allspaw</a> on the dangers of OVER automation:</p>

<blockquote><p>Some people, when confronted with a problem, think &#8220;I&#8217;ll use more automation!&#8221; Now they have Three Mile Island problems.</p></blockquote>


<p>and is even discussed in <a href="https://twitter.com/mcdonnps">Patrick McDonnell&#8217;s</a> talk at <a href="http://www.youtube.com/watch?v=nSnJCJiZDDU">ChefConf</a>.</p>

<p>I don&#8217;t agree with John 100% on his take but I can totally understand his perspective. Maybe I&#8217;m just more optimistic around exactly how far we can automate.</p>

<h2>So where does security fit into this?</h2>

<p>Here&#8217;s yet another quote from Tim Dysinger on this topic:</p>

<blockquote><p>If a super-user logs in and changes the sshd_config, you could have the service change it back before he even exited vi.  they&#8217;d then find a warning email in their in-box.  if they tried it again, even on another box, it could send a warning to the team lead and lock the user out.</p></blockquote>


<p>Mind you security is only one aspect of this thought process. The thing that makes it applicable is that the security domain has already tackled this problem a bit. The problem is it still requires human response. We have tools like Tripwire, Samhain and OSSEC that do active inspection of state but the response is left up to a human. Additionally they&#8217;re cumbersome to configure (even with CM). What&#8217;s missing is the &#8216;glue&#8217; between the two problem spaces.</p>

<p>In my head I envisioned something much like Tim described. In fact I even thought about a way that existing tools could be leveraged. It&#8217;s not pretty but it&#8217;s possible. The idea here, if it wasn&#8217;t already blindingly obvious, is that the response to an event that the security tool recognizes should be to run configuration management to correct the errant state.</p>

<p>There are a few problems with this approach that should also be immediately obvious:</p>

<ul>
<li>CM is currently an all or nothing approach. Something like the idea behind &#8216;partial run lists&#8217; in Chef could sort of address this</li>
<li>If we start down the path of partial CM, we now have to take into account dependencies.</li>
<li>We have no way to express this. We lack primitives with which to build this logic</li>
<li>Security is still somewhat in the dark ages. Many security decisions are still binary in nature</li>
</ul>


<p>What&#8217;s also missing in this picture is something to identify patterns. So now we&#8217;re bolting three tools together - the tripwire component, our CM tool and some sort of CEP. But again, even if we had this wondertool nirvana, how do we express it?</p>

<p>Let me be clear that I firmly believe that configuration management is absolutely a part of the security story. I stand by my assertion that consistent and repeatable configuration of a system from base OS to in-service is the foundation. Being able to express in code what state a system should have means that you never have to think &#8220;Did I forget to disable that apache module?&#8221; or &#8220;Did I make sure and disable root logins over SSH&#8221;. Where we find gaps in this is how we assert the negative without going insane.</p>

<p>Denied unless explicitly allowed is the mantra I followed for years when I was responsible for security. Ask me some time WHY I got out of security and why I don&#8217;t have ulcers anymore.</p>

<p>The questions I find myself asking are:</p>

<ul>
<li>If we use our CM system as the source of truth, can we sanely infer policy based on our CM codebase?</li>
<li>If we use the network as the source of truth, can we sanely infer policy based on our neighbors? Should we even trust our neighbors?</li>
<li>Do we even have the language to express what we mean and is it flexible and primitive enough to be used in composition? You can only go so far with &#8220;trusted&#8221;, &#8220;untrusted&#8221;,&#8221;mauve&#8221; and &#8220;taupe&#8221;.</li>
</ul>


<h1>Wrap up</h1>

<p>As I said, the original discussion was around configuration drift. I realize I went off on a tangent about security but that was intended as an example. I do believe that folks are working on this idea of &#8220;active assertion of state&#8221; as Tim puts it. I just wanted to brain dump my take on it. I don&#8217;t know that it can be solved with even the most flexible of CM tools. I do feel like it&#8217;s going to have to be a new generation of tool that takes these ideas into account and includes them in a ground-up design.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[It sucks to be right]]></title>
    <link href="http://lusis.github.com/blog/2012/03/20/it-sucks-to-be-right/"/>
    <updated>2012-03-20T09:00:00-04:00</updated>
    <id>http://lusis.github.com/blog/2012/03/20/it-sucks-to-be-right</id>
    <content type="html"><![CDATA[<p>So it looks like Adrian Cockcroft finally spilled the beans on <a href="http://perfcap.blogspot.com/2012/03/ops-devops-and-noops-at-netflix.html">Netflix (no)Operations</a> and sadly it reads like I expected.</p>

<!-- more -->


<h1>Netflix still does operations</h1>

<p>Regardless of what words Adrian uses, Netflix still does operations. <a href="http://twitter.com/allspaw">John Allspaw</a> summed it up pretty well in this tweet:</p>

<p><img src="http://i.imgur.com/OW0kh.png" alt="Imgur" /></p>

<p>and here are the things, he mentions:</p>

<ul>
<li>Metrics collection</li>
<li>PaaS/IaaS evaluation/investigation</li>
<li>Automation (auto-build, auto-recovery)</li>
<li>Fault tolerance</li>
<li>Availability</li>
<li>Monitoring</li>
<li>Performance</li>
<li>Capex and Opex forecasting</li>
<li>Outage response</li>
</ul>


<h1>So what does Adrian get wrong?</h1>

<p>These are just a few things that jumped out at me (and annoyed me)</p>

<blockquote><p>However, there are teams at Netflix that do traditional Operations, and teams that do DevOps as well.</p></blockquote>


<p>Ops is ops is ops. No matter what you call it, Operations is operations.</p>

<blockquote><p>Notice that we didn&#8217;t use the typical DevOps tools Puppet or Chef to create builds at runtime</p></blockquote>


<p>There&#8217;s no such thing as a &#8220;DevOps tool&#8221;. People were using CFengine, Puppet and Chef long before DevOps was even a term. These are configuration management tools. In fact Adrian has even said they use Puppet in their legacy datacenter:</p>

<p><img src="http://i.imgur.com/RJIX1.png" alt="Imgur" /></p>

<p>yet he seems to make the distinction between the ops guys there and the &#8220;devops&#8221; guys (whatever those are).</p>

<blockquote><p>There is no ops organization involved in running our cloud&#8230;</p></blockquote>


<p>Just because you outsourced it, doesn&#8217;t mean it doesn&#8217;t exist. Oh and it&#8217;s not your cloud. It&#8217;s Amazon&#8217;s.</p>

<h1>Reading between the lines</h1>

<p>Actually this doesn&#8217;t take much reading between the lines. It&#8217;s out there in plain sight:</p>

<blockquote><p>In reality we had the usual complaints about how long it took to get new capacity, the lack of consistency across supposedly identical systems, and failures in Oracle, in the SAN and the networks, that took the site down too often for too long.</p></blockquote>




<blockquote><p>We tried bringing in new ops managers, and new engineers, but they were always overwhelmed by the fire fighting needed to keep the current systems running.</p></blockquote>




<blockquote><p>This is largely because the people making decisions are development managers, who have been burned repeatedly by configuration bugs in systems that were supposed to be identical.</p></blockquote>




<blockquote><p>The developers used to spend hours a week in meetings with Ops discussing what they needed, figuring out capacity forecasts and writing tickets to request changes for the datacenter.</p></blockquote>




<blockquote><p>There is no ops organization involved in running our cloud, no need for the developers to interact with ops people to get things done, and less time spent actually doing ops tasks than developers would spend explaining what needed to be done to someone else.</p></blockquote>


<p>I&#8217;m glad to see this spelled out in such detail. This is what I&#8217;ve been telling people semi-privately for a while now. Because Netflix had such a terrible experience with its operations team, they went to the opposite extreme and disintermediated them.</p>

<p>Imagine you were scared as a kid by a clown. Now imagine you have kids of your own. You hate clowns. You had a bad experience with clowns. But it&#8217;s your kid&#8217;s birthday party so here you are making baloon animals, telling jokes and doing silly things to entertain the kids.</p>

<p>Just because you aren&#8217;t wearing makeup doesn&#8217;t make you any less of a clown. You&#8217;re doing clown shit. Through the eyes of the kids, you&#8217;re a clown. Deal with it.</p>

<p>Netflix is still doing operations. What should be telling and frightening to operations teams everywhere is this:</p>

<p>The Netflix response to poorly run operations that can&#8217;t service the business is going to become the norm and not the exception. Evolve or die.</p>

<p>Please note that I don&#8217;t lay all the blame on the Netflix operations team. I would love to hear the flipside of this story from someone who was there originally when the streaming initiative started. It would probably be full of stories we&#8217;ve heard before - no resources, misalignment of incentives and a whole host of others.</p>

<p>Adrian, thank you for writing the blog post. I hope it serves as a warning to those who come. Hopefully someday you&#8217;ll be able to see a clown again and not get scared ;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why you should stop fighting distro vendors]]></title>
    <link href="http://lusis.github.com/blog/2012/03/16/why-you-should-stop-fighting-distro-vendors/"/>
    <updated>2012-03-16T14:37:00-04:00</updated>
    <id>http://lusis.github.com/blog/2012/03/16/why-you-should-stop-fighting-distro-vendors</id>
    <content type="html"><![CDATA[<p>Recently I saw a tweet from <a href="https://twitter.com/#!/kohsukekawa/status/180717301795008512">Kohsuke Kawaguchi</a> that really got me frustrated.</p>

<!-- more -->


<p>I&#8217;ve addressed this topic a bit before <a href="http://lusislog.blogspot.com/2010/09/distributions-and-dynamic-languages.html">here</a>. At the time it was addressing specifically dynamic languages. However the post that Kohsuke wrote (and the post that inspired it) have led me to a new line attitude.</p>

<p><strong>Don&#8217;t bother trying to get your packages into upstream vendor distros</strong></p>

<h1>Wait. What? Let&#8217;s step back a sec</h1>

<p>Let me clarify something first. System packages are a good thing. The hassle has always been with BUILDING those packages. It was simply easier to build the software on the machine and install to <code>/usr/local/</code> than to try and express anything more than the most moderately simple application in RPM or DEB build scripts:</p>

<ul>
<li>If what you are packaging has dependencies not shipped with the OS, now you&#8217;ve got to package those</li>
<li>If your dependency conflicts with a vendor-shipped version, you&#8217;re screwed.</li>
<li>If your dependency is a language runtime, give up.</li>
<li>If your dependency is a specific version of python, just go into another line of work.</li>
<li>If it&#8217;s a distro LTS release, just don&#8217;t bother</li>
</ul>


<h1>Ahh but we can work around this!</h1>

<p>Yes, you&#8217;re right. We now have tools like <a href="https://github.com/jordansissel/fpm">fpm</a> that take the pain out of it! Maven has had plugins that generate rpms and debs for you for a while now. Things are looking up! Let&#8217;s just use those tools.</p>

<p>So now you think, I&#8217;ll just get these things submitted to Debian&#8230;.</p>

<p><strong>KABLOCK</strong></p>

<p>I could rant a bit about Debian&#8217;s packaging policy but it&#8217;s addressed in the posts above. So maybe the Fedora people are more flexible?</p>

<p><img src="http://i.imgur.com/px5ug.png" alt="Imgur" /></p>

<p><strong>WAT</strong></p>

<p>So here we have the two major distros that won&#8217;t even consider your package unless you give the end-user the &#8220;freedom&#8221; to make your application unusable. Essentially you are told if you want your package to be included in upstream then you have to make sure they can swap out <code>libfunkytown.so.23</code> with <code>libfunkytown.so.1</code>.</p>

<p>But maybe your application doesn&#8217;t work on that version. So maybe you think, I&#8217;ll just vendor ALL the things and shove it into <code>/opt</code> or <code>/usr/local</code>? Yeah that doesn&#8217;t fly either (for various reasons).</p>

<p>The point is that you&#8217;ll probably never be able to get your package included upstream because you&#8217;ll never be able to jump through the hoops to do it.</p>

<h1>So stop trying</h1>

<p>I know, I know. It would be awesome if you could tell users to just <code>yum install kickass</code> or <code>apt-get install kickass</code> but it&#8217;s not worth it for several reasons as enumerated above.</p>

<p>Distributions are not your friend. One could argue that its not thier job to be your friend. I would even agree with that argument. The distros have (or at least SHOULD have) an allegience to their user base. My argument is that position is directly opposed to your needs as a software provider.</p>

<h2>Things you should not do</h2>

<ul>
<li>Waste your time trying to ensure that your software works on some busted as old version of libfunkytown that won&#8217;t get upgrade for 7 years.</li>
<li>Waste your time breaking your application into 436 interdependent subpackages just to please upstream</li>
<li>Ignore the prexisting dependency management ecosystem of your language of choice (especially if it works)</li>
</ul>


<h2>Things you should do</h2>

<ul>
<li>Use your language&#8217;s preexisting dependency management system to collect all your dependencies</li>
<li>Rebar, bundle, virtualenv, mavenize, fatjar whatever ALL the dependencies</li>
<li>Use FPM or some homegrown script to create a monolithic rpm or deb of your codebase that installs to <code>/opt/appname</code></li>
<li>Make these packages available to your users on your download site</li>
<li>Alternately, create a repo and repo config file they can use to stay up to date</li>
</ul>


<p>You will be happy. Your users will be happy. The distros can go lick themselves. We have reached something of a crossroads. As I argued in the previous post, the concept of a distribution is becoming somewhat irrelevant. Distros are more concerned about politics and making statements and broken concepts like software that doesn&#8217;t need upgrading for 7 years (or even 2 years) than providing a framework and ecosystem that encourages developers to target software at it.</p>

<p>If someone takes up the noble cause of trying to get your software included upstream, I would go so far as to make it plainly clear on whatever communication you have that you simply cannot support an unofficial repackaging of your software. Be polite. These are still your potential userbase. Simply state that those were not created by you and that the official packages are here.</p>

<h1>A case in point</h1>

<p>What I&#8217;m suggesting you do is not unheard of and honestly is the most tenable long term path for your users. Look at projects like Vagrant, Chef and Puppet among others. All of these tools are &#8220;owning their availability&#8221; the right way and are arguably providing better end user experiences than getting included in upstream could provide. In fact the experience of official packaging is above and beyond trying to do it yourself. As it should be.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphs in operations]]></title>
    <link href="http://lusis.github.com/blog/2012/03/06/graphs-in-operations/"/>
    <updated>2012-03-06T23:59:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/03/06/graphs-in-operations</id>
    <content type="html"><![CDATA[<p>So anyone who knows me knows I spend an inordinate amount of time bitching about Maven. I don&#8217;t know if it&#8217;s the type of companies I end up working for or what but I always seem to find myself ass-deep in Maven.</p>

<!-- more -->


<p><em>please note that I&#8217;m drifiting into deeply unfamiliar territory for me. Someone once told me the best way to learn about something is to write about it. Keep that in mind when making comments?</em></p>

<p>One of the more interesting parts of maven is the dependency graph and concepts like transitive and (god forbid) circular dependencies. These problems aren&#8217;t exlcusive to java, mind you. See bundler for Ruby.</p>

<h2>A bit on graphs</h2>

<p>Graph is a fairly overloaded term. In the context of this discussion I&#8217;m talking about graph theory (insofar as I can grok it). Specifically I want to talk about it in the context of IT operations.</p>

<p>Graphs are nothing &#8220;new&#8221;. Programmers have binary trees. Network geeks have OSPF. Puppet and Git are fans of the DAG (directed acyclic graph). These are all rooted in the same place no? You have nodes and edges. It&#8217;s math all the way down. Unfortunately I suck at math.</p>

<p>If the topic interests you at all, wikipedia has a good couple of articles worth reading. Seeing as I&#8217;m far from a domain expert, I can&#8217;t vouch for the quality:</p>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Graph_theory">Graph Theory</a></li>
<li><a href="http://en.wikipedia.org/wiki/Graph_(mathematics">Graph (mathematics)</a>)</li>
<li><a href="http://en.wikipedia.org/wiki/Glossary_of_graph_theory">Glossary of graph theory</a></li>
</ul>


<h1>How can graphs apply to IT operations</h1>

<p>I&#8217;ve said for a while now that I feel like there&#8217;s something fuzzy on the horizon that I can&#8217;t make quite make out and it involves orchestration and graphs. I&#8217;m still not clear on how to express it but I&#8217;ll try.</p>

<p>Anyone who has ever used Puppet or Git has dabbled in graphs even if they don&#8217;t know it. However my interest in graphs in operations relates to the infrastructure as a whole. James Turnbull expressed it very well last year in Mt. View when discussion orchestration. Obviously this is a topic near and dear to my heart.</p>

<p>Right now much of orchestration is in the embryonic stages. We define relationships manually. We register watches on znodes. We define hard links between components in a stack. X depends on Y depends on Z. We&#8217;re not really being smart about it. If someone disagrees, I would LOVE to see a tool addressing the space.</p>

<p>Justin Sheehy did an awesome high level presentation on distributed systems, databases and the like at Velocity last year. While the talk was good, one thing that stuck out with me was his usage of the Riak logo:</p>

<p><img src="https://assets.github.com/img/b4d183fe3181209da593ed5c6bf0f4c805ab2a62/687474703a2f2f6769746875622d696d616765732e73332e616d617a6f6e6177732e636f6d2f626c6f672f323031302f7269616b2d6c6f676f2e706e67" alt="Riak Logo" /></p>

<p>During the presentation he would zoom out of the logo and replace it with the same logo. It expressed the idea of moving up the stack. Macro versus micro. I have the same feeling about where orchestration is going.</p>

<h2>Express yourself</h2>

<p>Currently we do a great job (and have the tools) to express relationships and dependencies at the node level:</p>

<ul>
<li>webapp needs container</li>
<li>container needs java</li>
<li>container needs system user</li>
</ul>


<p>Going a level higher, we even have some limited ability to express relationships between nodes:</p>

<ul>
<li>Load balancer needs app servers</li>
<li>App server needs database</li>
</ul>


<p>We&#8217;re not quite as good at this part yet but people have workarounds. I use Noah for this. enStratus also handles this very well.</p>

<p>But we&#8217;re still defining those relationships manually.</p>

<p>When we get to this next level up, things get REALLY fuzzy. As people start to (re)discover SOA, we now have stacks that have dependencies on other stacks. Currently we use tools like Zookeeper to broker that relationship. But we still have to explcitly manage it.</p>

<p>The level of coupling here isn&#8217;t the problem. You can mitigate failure in one stack as it relates to another stack. Fail fast and fall back to sane/safe defaults. Read any article about how Netflix architects to get an idea.</p>

<h1>What&#8217;s missing?</h1>

<p>What I feel like we&#8217;re missing is a way to express those relationships and then trigger on them all the way up and down the chain as needed. We&#8217;re starting to get into graph territory here.</p>

<p>We must we be able to express and act on changes at the micro level (<em>I changed a config, I must restart nginx</em>) and even at the intranode level (<em>something changed in my app tier, need to tell my load balancer</em>) but now we need a way handle it at that macro level. Not only do we need a way to handle it but we must also be able to calculate what is impacted by that change.</p>

<ul>
<li>If I have this internode change, does it affect the intranode relationship?</li>
<li>If I have an intranode change, does it affect the intrastack relationship?</li>
</ul>


<p>It seems to me that a graph of SOME kind is the best way to express this. I just can&#8217;t quite make it out. Does current graph technology even handle that subgraph relationship? Excuse the pun but where do we draw the line? Are there multiple lines?</p>

<p>Maybe this isn&#8217;t an issue. Maybe through resilience engineering we simply keep that &#8220;intrastack&#8221; dependency as loose as possible so that we don&#8217;t have this problem?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZeroMQ and Logstash - Part 2]]></title>
    <link href="http://lusis.github.com/blog/2012/02/08/zeromq-and-logstash-part-2/"/>
    <updated>2012-02-08T21:08:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/02/08/zeromq-and-logstash-part-2</id>
    <content type="html"><![CDATA[<p>A few days ago I wrote up some notes on how we&#8217;re making Logstash better by adding ZeroMQ as an option for inputs and outputs. That night we decided to take it a bit further and add support for ZeroMQ as a filter plugin as well.</p>

<!-- more -->


<p>I&#8217;ve had a lot of people ask me what&#8217;s so hot about ZeroMQ. It&#8217;s hard to explain but I really would suggest you read the excellent <a href="http://zguide.zeromq.org">zguide</a>. The best way I can describe it is that it&#8217;s sockets on steroids. Sockets that behave the way you would expect sockets to behave as opposed to the way they do now. <a href="http://www.quora.com/What-is-the-background-of-the-just-open-a-socket-meme">Just open a socket!</a>.</p>

<h1>Inputs and Outputs</h1>

<p>I&#8217;m only going to touch briefly on inputs and outputs. They were discussed briefly previously and I have a full fledged post in the wings about it.</p>

<p>They essentially work like the other implementations (AMQP and Redis) with the exception that you don&#8217;t have a broker in the middle. Let me show you:</p>

<pre><code>[Collector 1] ------ load balanced events ----&gt; [Indexer 1, Indexer 2, Indexer 3, Indexer 4]
[Collector 2] ------ load balanced events ----&gt; [Indexer 1, Indexer 2, Indexer 3, Indexer 4]
[Collector 3] ------ load balanced events ----&gt; [Indexer 1, Indexer 2, Indexer 3, Indexer 4]
[Collector 4] ------ load balanced events ----&gt; [Indexer 1, Indexer 2, Indexer 3, Indexer 4]
</code></pre>

<p>As you can see we&#8217;re doing a pattern very similar to before. We want to send events of our nodes over to a cluster of indexers that do filtering. The difference here is that we don&#8217;t have a broker. Not big deal, right? One less thing to worry about! You don&#8217;t have to learn some new tool just to get some simple load balancing of workers. This works great&#8230;..until you need to scale workers.</p>

<p>Even using awesome configuration management, you&#8217;ve now got to cycle all your collectors to add the new endpoints. This means lost events. This makes me unhappy. It makes you unhappy. The world is sad. Why are you doing this to us?</p>

<p>Luckily I&#8217;ve been authorized by the Franklin Mint to release the source code to an enterprise class ZeroMQ broker that you can use. Not only is it enterprise class but it has built-in clustering. You can <a href="https://github.com/lusis/enterprise-zeromq-broker">grab the code here from github</a>.</p>

<p>Here are the configs for the logstash agents (output.conf is collector config, input.conf is indexer config):</p>

<p>output.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input { stdin { type =&gt; "stdin" } }
</span><span class='line'>output {
</span><span class='line'>  zeromq {
</span><span class='line'>    topology =&gt; "pushpull"
</span><span class='line'>    address =&gt; ["tcp://localhost:5555", "tcp://localhost:5557"]
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>input.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input { 
</span><span class='line'>  zeromq {
</span><span class='line'>    type =&gt; "pull-input"
</span><span class='line'>    topology =&gt; "pushpull"
</span><span class='line'>    address =&gt; ["tcp://localhost:5556", "tcp://localhost:5558"]
</span><span class='line'>    mode =&gt; "client"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>output { stdout { debug =&gt; true }}</span></code></pre></td></tr></table></div></figure>


<h2>Action shot</h2>

<p>Here&#8217;s a shot of our fancy clustered broker in action (click to zoom):</p>

<p><a href="http://lusis.github.com/images/posts/zeromq-part2/zeromq-broker-ss.png"><img src="http://lusis.github.com/images/posts/zeromq-part2/zeromq-broker-ss.png" alt="zeromq-broker-ss.png" /></a></p>

<p>As you can see the two events we sent were automatically load balanced across our <em>&#8220;brokers&#8221;</em> which then load balanced across our indexers.</p>

<h2>What have we bought ourselves?</h2>

<p>Obviously this is all something of a joke. All we have done is point our collectors at other nodes instead of directly at our indexers. But realize that you can create 2 fixed points on your network with 8 lines of core code and use those as the static information in your indexers and collectors. You can then scale either side without ever having to update a configuration file.</p>

<p>I dare say you can even run those on t1.micro instances on Amazon.</p>

<p>Oh and if you don&#8217;t like Ruby, write it in something else. That&#8217;s the beauty of ZeroMQ.</p>

<h1>Filters</h1>

<p>The thing that has me most excited is the addition of ZeroMQ as a filter to logstash. As you&#8217;ve already seen, ZeroMQ makes it REALLY easy to wire network topologies up with complex patterns. In the inputs and outputs we&#8217;ve exposed a few topologies that make sense. However there&#8217;s another topology that we had not yet exposed because it didn&#8217;t make sense - <code>reqrep</code>.</p>

<h2>REQ/REP</h2>

<p><code>reqrep</code> is short for request and reply. The reason we didn&#8217;t expose it previously is that it didn&#8217;t really make sense with the nature of inputs and outputs. However after talking with Jordan, we decided it actually DID make sense to use it for filters. After all, filters get a request -> do something -> return a response.</p>

<p>If it&#8217;s not immediately clear yet how this makes sense, I&#8217;ve got another example for you. Let&#8217;s take the case of needing to look something up externally to mutate a field. You COULD write a Logstash filter to do this ONE thing for you. Maybe you can make it generic enough to even submit a pull request.</p>

<p>Or you could use a ZeroMQ filter:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input { stdin { type =&gt; "stdin-type" } }
</span><span class='line'>filter { zeromq { } }
</span><span class='line'>output { stdout { debug =&gt; true } }</span></code></pre></td></tr></table></div></figure>


<p>Here&#8217;s the code for the filter:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s1">&#39;rubygems&#39;</span>
</span><span class='line'><span class="nb">require</span> <span class="s1">&#39;ffi-rzmq&#39;</span>
</span><span class='line'><span class="nb">require</span> <span class="s2">&quot;json&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">context</span> <span class="o">=</span> <span class="no">ZMQ</span><span class="o">::</span><span class="no">Context</span><span class="o">.</span><span class="n">new</span>
</span><span class='line'><span class="n">socket</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="no">ZMQ</span><span class="o">::</span><span class="no">REP</span><span class="p">)</span>
</span><span class='line'><span class="n">socket</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&quot;tcp://*:2121&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span><span class='line'><span class="nb">puts</span> <span class="s2">&quot;starting up&quot;</span>
</span><span class='line'><span class="k">while</span> <span class="kp">true</span> <span class="k">do</span>
</span><span class='line'>  <span class="n">socket</span><span class="o">.</span><span class="n">recv_string</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span class='line'>  <span class="n">modified_message</span> <span class="o">=</span> <span class="no">JSON</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s2">&quot;Message received: </span><span class="si">#{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>  <span class="c1"># Simulate using an external data source to </span>
</span><span class='line'>  <span class="c1"># to something that you need</span>
</span><span class='line'>  <span class="k">case</span> <span class="n">modified_message</span><span class="o">[</span><span class="s2">&quot;@source&quot;</span><span class="o">]</span>
</span><span class='line'>  <span class="k">when</span> <span class="s2">&quot;stdin://jvstratusmbp.lusis.org/&quot;</span>
</span><span class='line'>    <span class="nb">puts</span> <span class="s2">&quot;Doing db lookup&quot;</span>
</span><span class='line'>    <span class="nb">sleep</span> <span class="mi">10</span>
</span><span class='line'>    <span class="n">modified_message</span><span class="o">[</span><span class="s2">&quot;@source&quot;</span><span class="o">]</span> <span class="o">=</span> <span class="s2">&quot;john&#39;s laptop&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s2">&quot;Message responded: </span><span class="si">#{</span><span class="n">modified_message</span><span class="o">.</span><span class="n">to_json</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>  <span class="n">socket</span><span class="o">.</span><span class="n">send_string</span><span class="p">(</span><span class="n">modified_message</span><span class="o">.</span><span class="n">to_json</span><span class="p">)</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>By default, the filter will send the entire event over a ZeroMQ <code>REQ</code> socket to <code>tcp://localhost:2121</code>. It will then take the reply and send it up the chain to the Logstash output with the following results:</p>

<p><a href="http://lusis.github.com/images/posts/zeromq-part2/zeromq-filter-event.png"><img src="http://lusis.github.com/images/posts/zeromq-part2/zeromq-filter-event.png" alt="zeromq-filter-event.png" /></a></p>

<p>Alternately, you can send a single field to the filter and have it to work with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">input</span> <span class="p">{</span> <span class="n">stdin</span> <span class="p">{</span> <span class="n">type</span> <span class="o">=&gt;</span> <span class="s2">&quot;stdin-test&quot;</span> <span class="p">}</span> <span class="p">}</span>
</span><span class='line'><span class="n">filter</span> <span class="p">{</span> <span class="n">zeromq</span> <span class="p">{</span> <span class="n">field</span> <span class="o">=&gt;</span> <span class="s2">&quot;@message&quot;</span> <span class="p">}</span> <span class="p">}</span>
</span><span class='line'><span class="n">output</span> <span class="p">{</span> <span class="n">stdout</span> <span class="p">{</span> <span class="n">debug</span> <span class="o">=&gt;</span> <span class="kp">true</span> <span class="p">}}</span>
</span></code></pre></td></tr></table></div></figure>


<p>and the code:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s1">&#39;rubygems&#39;</span>
</span><span class='line'><span class="nb">require</span> <span class="s1">&#39;ffi-rzmq&#39;</span>
</span><span class='line'><span class="nb">require</span> <span class="s2">&quot;json&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">context</span> <span class="o">=</span> <span class="no">ZMQ</span><span class="o">::</span><span class="no">Context</span><span class="o">.</span><span class="n">new</span>
</span><span class='line'><span class="n">socket</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="no">ZMQ</span><span class="o">::</span><span class="no">REP</span><span class="p">)</span>
</span><span class='line'><span class="n">socket</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="s2">&quot;tcp://*:2121&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span><span class='line'><span class="nb">puts</span> <span class="s2">&quot;starting up&quot;</span>
</span><span class='line'><span class="k">while</span> <span class="kp">true</span> <span class="k">do</span>
</span><span class='line'>  <span class="n">socket</span><span class="o">.</span><span class="n">recv_string</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s2">&quot;Recieved message: </span><span class="si">#{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>  <span class="n">modified_message</span> <span class="o">=</span> <span class="s2">&quot;this field was changed externally&quot;</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s2">&quot;Modified message: </span><span class="si">#{</span><span class="n">modified_message</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>  <span class="n">socket</span><span class="o">.</span><span class="n">send_string</span><span class="p">(</span><span class="n">modified_message</span><span class="p">)</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>and the result:</p>

<p><a href="http://lusis.github.com/images/posts/zeromq-part2/zeromq-filter-field.png"><img src="http://lusis.github.com/images/posts/zeromq-part2/zeromq-filter-field.png" alt="zeromq-filter-field.png" /></a></p>

<p>Many people have been asking for an <code>exec</code> filter for some time now. Dealing with that overhead is insane when coming from the JVM. By doing this type of work over ZeroMQ, there&#8217;s much less overhead AND a reliable conduit for making it happen.</p>

<p>Here&#8217;s just a few of the use cases I could think of:</p>

<ul>
<li>Artifically throttling your flow. Just use a sleep and return the original event.</li>
<li>Doing external lookups for replacing parts of the event</li>
<li>Adding arbitrary tags to a message using external criteria based on the event.</li>
<li>Moving underperforming filters out of logstash and into an external process that is more performant</li>
<li>Reducing the need to modify configs in logstash for greater uptime.</li>
</ul>


<h1>Wrap up</h1>

<p>All the ZeroMQ support is currently tagged experimental (hence the warnings you saw in my screenshots). It also exists in the form described only in master. If this interests you at all, please build from master and run some tests of your own. We would love the feedback and any bugs or tips you can provide are always valuable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZeroMQ and Logstash - Part 1]]></title>
    <link href="http://lusis.github.com/blog/2012/02/06/zeromq-and-logstash-part-1/"/>
    <updated>2012-02-06T01:07:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/02/06/zeromq-and-logstash-part-1</id>
    <content type="html"><![CDATA[<p>Every once in a while, a software project comes along that makes you rethink how you&#8217;ve done things up until that point. I&#8217;ve often said that ElasticSearch was the first of those projects for me. The other is ZeroMQ.</p>

<!-- more -->


<h1>Edit and update</h1>

<p>Evidently my testing missed a pretty critical usecase - pubsub. It doesn&#8217;t work right now. Due to the way we&#8217;re doing sockopts works for setting topics. However we don&#8217;t have a commensurate setting on the PUB side. I&#8217;ve created <a href="https://logstash.jira.com/browse/LOGSTASH-399">LOGSTASH-399</a> and <a href="https://logstash.jira.com/browse/LOGSTASH-400">LOGSTASH-400</a> to deal with these issues. I am so sorry about that however it doesn&#8217;t change the overall tone and content of this message as <code>pair</code> and <code>pushpull</code> still work.</p>

<h1>A little history</h1>

<p>In January of this year, <a href="https://twitter.com/jordansissel">Jordan</a> merged the first iteration of ZeroMQ support for Logstash. Several people had been asking for it and I had it on my plate to do as well. Funny side note, the pull request for the ZeroMQ plugin was my inspiration for adding <a href="http://logstash.net/docs/1.1.0/plugin-status">plugin_status</a> to Logstash.</p>

<p>The reason for wanting to mark it experimental is that there was concern over the best approach to using ZeroMQ with Logstash. Did we create a single context per agent? Did we do a context per thread? How well would the multiple layers of indirection work (jvm + ruby + ffi)?</p>

<p><a href="https://twitter.com/_masterzen_">Brice&#8217;s</a> original pull request only hadnled one part of the total ZeroMQ package (PUBSUB) but it was an awesome start. We actually had two other pull requests around the same time but his was first.</p>

<p>A week or so ago, I started a series of posts around doing load balanced filter pipelines with Logstash. The first was <a href="http://goo.gl/vWyCH">AMQP</a> and then <a href="http://goo.gl/6W8Lv">Redis</a>. The next logical step was ZeroMQ (and something of a &#8220;Oh..and one more thing..&#8221; post). Sadly, the current version of the plugin was not amenable to doing the same flow. Since it only supported PUBSUB, I needed to do some work on the plugin to get the other socket types supported. I made this my weekend project.</p>

<h1>Something different</h1>

<p>One thing that ZeroMQ does amazingly well is make something complex very easy. It exposes common communication patterns over socket types and makes it easy to use them. It really is just plug and play communication.</p>

<p>However it also makes some really powerful flows available to you if you dig deep enough. Look at this example from the <a href="http://zguide.zeromq.org">zguide</a></p>

<p><img src="https://github.com/imatix/zguide/raw/master/images/fig14.png" alt="complex-flow" /></p>

<p>Mind you the code for that is pretty simple (<a href="http://zguide.zeromq.org/rb:taskwork2">ruby example</a>) but we need to enable that level of flexibility and power behind the Logstash config language. We also wanted to avoid the confusion that we faced with the AMQP plugin around exchange vs. queue.</p>

<p>Jordan came up with the idea of removing the socket type confusion and just exposing the patterns. And that&#8217;s what we&#8217;ve done.</p>

<h1>Configuration</h1>

<p>In the configuration language, Logstash exposes the ZeroMQ socket type pairs in the using the same syntax on both inputs and outputs. We call these a &#8220;topology&#8221;. In fact, out of the box, Logstash ZeroMQ support will work out of the box with two agents on the same machine:</p>

<h2>Output</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>  stdin { type =&gt; "stdin-input" }
</span><span class='line'>}
</span><span class='line'>output {
</span><span class='line'>  zeromq { topology =&gt; "pushpull" }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Input</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>  zeromq { topology =&gt; "pushpull" type =&gt; "zeromq-input" }
</span><span class='line'>}
</span><span class='line'>output {
</span><span class='line'>  stdout { debug =&gt; true }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>Opinionated</h2>

<p>Because any side of a socket type in ZeroMQ can be the connecting or binding side (the underlying message flow is disconnected from how the connection is established), Logstash follows the recommendation of the zguide. The more &#8220;stable&#8221; parts of your infrastructure should be the side that binds/listens while they ephemeral side should be the one that initiates connections.</p>

<p>Following this, we have some sane defaults around the plugins:</p>

<ul>
<li>Logstash inputs will, by default, be the <code>bind</code> side and bind to all interfaces on port 2120</li>
<li>Logstash outputs will, by default, be the <code>connect</code> side</li>
<li>Logstash inputs will be the consumer side of a flow</li>
<li>Logstash outputs will be the producing side of a flow</li>
</ul>


<p>The last two are obviously pretty &#8220;duh&#8221; but worth mentioning. Right now Logstash exposes three socket types that make sense for Logstash:</p>

<ul>
<li>PUSHPULL (Output is PUSH. Input is PULL)</li>
<li>PUBSUB (Output is PUB. Input is SUB)</li>
<li>PAIR</li>
</ul>


<p>It&#8217;s worth reading up on ALL <a href="http://api.zeromq.org/2-1:zmq-socket">the socket types in ZeroMQ</a>.</p>

<p>By default, because of how ZeroMQ will most commonly be slotted into your pipeline, it sets the default message format to the Logstash native <em>json_event</em>.</p>

<p>You can still get to the low-level tuning of the sockets via the <code>sockopts</code> configuration setting. This is a Logstash config hash. For example, if you wanted to tune the high water mark of a socket (<code>ZMQ_HWM</code>), you would do so with this option:</p>

<p><code>zeromq { topology =&gt; "pushpull" sockopts =&gt; ["ZMQ::HWM", 20] }</code></p>

<p>These options are passed directly to the <code>ffi-rzmq</code> library we use (hence the syntax on the option name). If a new option is added in a later release, it&#8217;s already available that way.</p>

<h1>Usage of each topology</h1>

<p>While I have a few more blog posts in the hopper around ZeroMQ (and various patterns with Logstash), I&#8217;ll briefly cover where each type might fit.</p>

<h2>PUBSUB</h2>

<p>This is exactly what it sounds like. Each output (PUB) broadcasts to all connected inputs (SUB).</p>

<h2>PUSHPULL</h2>

<p>This most closely mimics the examples in my previous posts on AMQP and Redis. Each output (PUSH) load-balances across all connected inputs (PULL).</p>

<h2>PAIR</h2>

<p>This is essentially a one-to-one streaming socket. While messages CAN flow both directions, Logstash does not support (nor need) that. Outputs stream events to the input.</p>

<p>ZeroMQ has other topologies (like REQREP - request response and ROUTER/DEALER) but they don&#8217;t really make sense for Logstash right now. For the type of messaging that Logstash does between peers, PAIR is a much better fit. We have plans to expose these in a future release.</p>

<h1>Future</h1>

<p>As I said, I&#8217;ve got quite a few ideas for posts around this plugin. It opens up so many avenues for users and makes doing complex pipelines much easier. Here&#8217;s a sample of some things you&#8217;ll be able to do:</p>

<ul>
<li>Writing your own &#8220;broker&#8221; to sit between edges and indexers in whatever language works best (8 lines of Ruby)</li>
<li>Log directly from your application (e.g. log4j ZMQ appender) to logstash with minimal fuss</li>
<li>Tune ZeroMQ sockopts for durability</li>
</ul>


<p>Current ZeroMQ support only exists in master right now. However building from source is very easy. Simply clone the repo and type <code>make</code>. You don&#8217;t even need to have Ruby installed. This will leave your very own jar file in the <code>build</code> directory.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Load balancing Logstash with Redis]]></title>
    <link href="http://lusis.github.com/blog/2012/01/31/load-balancing-logstash-with-redis/"/>
    <updated>2012-01-31T23:24:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/01/31/load-balancing-logstash-with-redis</id>
    <content type="html"><![CDATA[<p>After yesterday&#8217;s post about load balancing logstash with AMQP and RabbitMQ, I got to thinking that it might be useful to show a smilar pattern with other inputs and outputs.
To me this, is the crux of what makes Logstash so awesome. Someone asked me to describe Logstash in one sentence. The best I could come up with was:</p>

<blockquote><p>Logstash is a unix pipe on steroids</p></blockquote>


<p>I hope this post helps you understand what I meant by that</p>

<!-- more -->


<h1>Revisiting our requirements and pattern</h1>

<p>If you recall from the post <a href="http://goo.gl/vWyCH">yesterday</a>, we had the following &#8216;requirements&#8217;:</p>

<ul>
<li>No lost messages in transit/due to inputs or outputs.</li>
<li>Shipper only configuration on the source</li>
<li>Worker based filtering model</li>
<li>No duplicate messages due to transit mediums (i.e. fanout is inappropriate as all indexers would see the same message)</li>
</ul>


<h2>EDIT</h2>

<p>Originally our list stated the requirements as <em>No lost messages</em> and <em>No duplicate messages</em>. I&#8217;ve amended those with a slight modification to closer reflect the original intent. Please see <a href="http://blog.lusis.org/blog/2012/01/31/load-balancing-logstash-with-amqp/#comment-426175086">comment from Jelle Smet here</a> for details. Thanks Jelle!</p>

<p>Our design looked something like this:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/gliffy-overview.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-amqp/gliffy-overview.png" alt="gliffy-overview.png" /></a></p>

<p>One of the reasons that post was so long was that AMQP is a complicated beast. There was quite a bit of dense frontloading I had to do to cover AMQP before we got to the meat.
We&#8217;re going to take that same example, and swap out RabbitMQ for something a bit simpler and achieve the same results.</p>

<h1>Quick background on Redis</h1>

<p><a href="http://redis.io">Redis</a> is commonly lumped in with a group of data storage technologies called NoSQL. Its name is short for &#8220;REmoteDIctionaryServer&#8221;. It typically falls into the &#8220;key/value&#8221; family of NoSQL.
Several things set Redis apart from most key/value systems however:</p>

<ul>
<li>&#8220;data types&#8221; as values</li>
<li>native operations on those data types</li>
<li>atomic operations</li>
<li>built-in PUB/SUB subsystem</li>
<li>No external dependencies</li>
</ul>


<h2>Data types</h2>

<p>I&#8217;m not going to go into too much detail about the data types except to list them and highlight the one we&#8217;ll be leveraging. You can read more about them <a href="http://redis.io/topics/data-types">here</a></p>

<ul>
<li>Strings</li>
<li>Lists*</li>
<li>Sets</li>
<li>Hashes</li>
<li>Sorted Sets</li>
</ul>


<h3>How Logstash uses Redis</h3>

<p>Looking back at our AMQP example, we note three distinct exchange types. These are mapped to the following functionality in Redis (and Logstash <code>data_type</code> config for reference):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/mapping-table.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/mapping-table.png" alt="mapping-table.png" /></a></p>

<p>This is a somewhat over simplified list. In the case of a message producer, mimicing <code>direct</code> exchanges is done by writing to a Redis <code>list</code> while consumption of that is done via the Redis command <code>BLPOP</code><a href="http://redis.io/commands/blpop">*</a>. However mimicing the <code>fanout</code> and <code>topic</code> functionality is done strictly with the commands <code>PUBLISH</code><a href="http://redis.io/commands/publish">*</a>, <code>SUBSCRIBE</code><a href="http://redis.io/commands/subscribe">*</a> and <code>PSUBSCRIBE</code><a href="http://redis.io/commands/psubscribe">*</a>. It&#8217;s worth reading each of those for a better understanding.</p>

<p>Oddly enough, the use of Redis as a messaging bus is something of a side effect. Redis supported lists that are auto-sorted by insert order. The <code>POP</code> command variants allowed single transaction get and remove of the data. It just fit the use case.</p>

<h1>The configs</h1>

<p>As with our previous example, we&#8217;re going to show the configs needed on each side and explain them a little bit.</p>

<h2>Client-side/Producer</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input { stdin { type =&gt; "producer"} }
</span><span class='line'>output {
</span><span class='line'>redis {
</span><span class='line'> host =&gt; 'localhost'
</span><span class='line'> data_type =&gt; 'list'
</span><span class='line'> key =&gt; 'logstash:redis'
</span><span class='line'>}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>data_type</h3>

<p>This is where we tell Logstash how to send the data to Redis. In the case, again, we&#8217;re storing it in a list data type.</p>

<h3>key</h3>

<p>Unfortunately, key means different things (though with the same effect) depending on the <code>data_type</code> being used. In the case of a <code>list</code> this maps cleanly to the understanding of a <code>key</code> in a key/value system. It&#8217;s common in Redis to namespace keys with a <code>:</code> though it&#8217;s entirely unneccesary.</p>

<p>As an aside, when using <code>key</code> on <code>channel</code> data type, this behaves like the routing key in AMQP parlance with the exception of being able to use any separator you like (in other words, you can namespace with <code>.</code>,<code>:</code>,<code>::</code> whatever).</p>

<h2>Indexer-side/Consumer</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>redis {
</span><span class='line'>  host =&gt; 'localhost'
</span><span class='line'>  data_type =&gt; 'list'
</span><span class='line'>  key =&gt; 'logstash:redis'
</span><span class='line'>  type =&gt; 'redis-input'
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>output {stdout {debug =&gt; true} }</span></code></pre></td></tr></table></div></figure>


<h3>data_type</h3>

<p>This needs to match up with the value from the output plugin. Again, in this example <code>list</code>.</p>

<h3>key</h3>

<p>In the case of a <code>list</code> this needs to map EXACTLY to the output plugin. Following on to our previous aside, for <code>data_type</code> values of <code>channel</code> input, the key must match exactly while <code>pattern_channel</code> can support wildcards. Redis PSUBSCRIBE wildcards actually much simpler than AMQP ones. You can use <code>*</code> at any point in the key name.</p>

<h1>Starting it all up</h1>

<p>We&#8217;re going to simplify our original tests a little bit in the interest of brevity. Showing 2 producers and 2 consumers gives us the same benefit as showing four of each. Since we don&#8217;t have the benefit of a pretty management interface, we&#8217;re going to use the redis server debug information and the <code>redis-cli</code> application to allow us to see certain management information.</p>

<h2>redis-server</h2>

<p>Start the server with the command <code>redis-server</code> I&#8217;m running this from homebrew but you literally build Redis on any machine that has <code>make</code> and a compiler. That&#8217;s all you need. You can even run it straight from the source directory:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/redis-server.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/redis-server.png" alt="redis-server.png" /></a></p>

<p>You&#8217;ll notice that the redis server is periodically dumping some stats - number of connected clients and the amount of memory in use.</p>

<h2>Starting the logstash agents</h2>

<p>We&#8217;re going to start two producers (redis output) and two consumers (redis input):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/agents.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/agents.png" alt="agents.png" /></a></p>

<p>Back in our redis-server window, you should now see two connected clients in the periodic status messages. Why not four? Because the producers don&#8217;t have a persistent connection to Redis. Only the consumers do (via BLPOP):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/two-clients.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/two-clients.png" alt="two-clients.png" /></a></p>

<h1>Testing message flow</h1>

<p>As with our previous post, we&#8217;re going to alternate messages between the two producers. In the first producer, we&#8217;ll type <code>window 1</code> and in the second <code>window 2</code>. You&#8217;ll see the consumers pick up the messages:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/delivery.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/delivery.png" alt="delivery.png" /></a></p>

<p>If you look over in the redis-server window, you&#8217;ll also see that our client count went up to four. If we were to leave these clients alone, eventually it would drop back down to two.</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/new-connections.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/new-connections.png" alt="new-connections.png" /></a></p>

<p>Feel free to run the tests a few times and get a feel for message flow.</p>

<h2>Offline consumers</h2>

<p>This is all well and good but as with the previous example, we want to test how this configuration handles the case of consumers going offline. Shut down the two indexer configs and let&#8217;s verify. To do this, we&#8217;re going to also open up a new window and run the <code>redis-cli</code> app. Technically, you don&#8217;t even need that. You can telnet to the redis port and just run these commands yourself. We&#8217;re going to use the <code>LLEN</code> command to get the size of our &#8220;backlog&#8221;.</p>

<p>In the producer windows, type a few messages. Alternate between producers for maximum effect. Then go over to the <code>redis-cli</code> window and type <code>LLEN logstash:redis</code>. You should see something like the following (obviously varied by how many messages you sent):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/llen.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/llen.png" alt="llen.png" /></a></p>

<p>You&#8217;ll also notice in the redis server window that the amount of memory in use went up slightly.</p>

<p>Now let&#8217;s start our consumers back up and ensure they drain (and in insert order):</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/drain.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/drain.png" alt="drain.png" /></a></p>

<p>Looks good to me!</p>

<h1>Persistence</h1>

<p>You might have noticed I didn&#8217;t address disk-based persistence at all. This was intentional. Redis is primarily a memory-based store. However it does have support for a few different ways of persisting to disk - RDB and AOF. I&#8217;m not going to go into too much detail on those. The Redis documentation does a good job of explaining the pros and cons of each. You can read that <a href="http://redis.io/topics/persistence">here</a>.</p>

<h1>Wrap up</h1>

<p>One thing that&#8217;s important to note is that Redis is pretty damn fast. The limitation for Redis is essentially memory. However if speed isn&#8217;t your primary concern, there&#8217;s an interesting alpha project called <a href="http://inaka.github.com/edis">edis</a> worth investigating. It is a port of Redis to Erlang. Its primary goal is better persistence for Redis. For this post I also tested Logstash against edis and I&#8217;m happy to say it works:</p>

<p><a href="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/edis.png"><img src="http://lusis.github.com/images/posts/load-balancing-logstash-with-redis/edis.png" alt="edis.png" /></a></p>

<p>I hope to do further testing with it in the future in a multinode setup.</p>

<h2>Part three</h2>

<p>I&#8217;m also working on a part three in this &#8220;series&#8221;. The last configuration I&#8217;d like to show is doing this same setup but using <a href="http://zeromq.org">0mq</a> as the bus. This is going to be especially challenging since our 0mq support is curretly &#8216;alpha&#8217;-ish quality. Beyond that, I plan on doing a similar series using pub/sub patterns. If you&#8217;re enjoying these posts, please comment and let me know!</p>
]]></content>
  </entry>
  
</feed>
