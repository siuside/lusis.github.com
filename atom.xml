<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[blog dot lusis]]></title>
  <link href="http://lusis.github.com/atom.xml" rel="self"/>
  <link href="http://lusis.github.com/"/>
  <updated>2016-04-07T20:10:27-04:00</updated>
  <id>http://lusis.github.com/</id>
  <author>
    <name><![CDATA[John E. Vincent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How Is Rundeck Formed]]></title>
    <link href="http://lusis.github.com/blog/2016/04/07/how-is-rundeck-formed/"/>
    <updated>2016-04-07T18:31:35-04:00</updated>
    <id>http://lusis.github.com/blog/2016/04/07/how-is-rundeck-formed</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve mentioned a few times across various tweets that I&rsquo;m a big fan of <a href="http://rundeck.org">Rundeck</a>. Since a few folks have asked why/how we use it I figured a blog post is better</p>

<!-- more -->


<h1>What is rundeck?</h1>

<p>The way I typically describe rundeck is &ldquo;a web interface for shell scripts&rdquo;. That&rsquo;s a bit unfair but that&rsquo;s the simplest thing for people to grok.
If I recall correctly most of the concepts of rundeck were spun out of a larger product from <a href="http://dtosolutions.com/">DTO Solutions</a> called <a href="http://www.controltier.com/">Control Tier</a>.
In the same way some people use Jenkins freeform jobs to run things on remote systems, I use rundeck but without the &ldquo;constraints&rdquo; of using a system designed for build automation.</p>

<p>The funny thing is I&rsquo;ve not always been a fan of Rundeck.</p>

<p>When I first heard of Rundeck, I was almost instantly turned off. I was going through my configuration management zealotry phase. Rundeck felt like a MAJOR step backwards.
If people didn&rsquo;t have to give up those shitty shell scripts then they would never &ldquo;do it right&rdquo;. I mean CM is the fucking future right?</p>

<p>Well after some maturing and trying to shoehorn everything into my CM tooling&rsquo;s concept of how to do things, I realized that maybe there was a place for it.</p>

<p>What really blew it open for me was realizing that while Rundeck is a web interface for shell scripts, it inherently is an API for shell scripts.
Once I got that religion, it was only a matter of time before I became a Rundeck zealot ;)</p>

<h1>Key advantages</h1>

<p>Before I get into how I use/have used rundeck, I want to discuss some key things that make it really valuable:</p>

<ul>
<li>You can port any &ldquo;legacy&rdquo; scripts to a multi-user system</li>
<li>You can enforce constraints on the data passed to those scripts</li>
<li>You can ensure that those scripts are run ONLY the &ldquo;right&rdquo; way via validation</li>
<li>You can get historical auditing and logging of execution of those scripts</li>
<li>You can easily parallelize those scripts across multiple nodes</li>
</ul>


<p>Frankly that should be enough right there. Scripts (regardless of the language they&rsquo;re written in) aren&rsquo;t going away and honestly I don&rsquo;t think they need to.
For better or worse, shell scripting is sort of an entry level skill for operations work. Some things don&rsquo;t need to be written in go or python. That would be complicating it.
What&rsquo;s needed is to solve the &ldquo;that thing Susie does on her laptop when the foo breaks&rdquo; in a sane way. Rundeck can make that multi-user and help dull some of the sharp edges.</p>

<h1>So how do I Rundeck</h1>

<p>At my last gig (Enstratus/D***), Rundeck was the backbone of all our deployment automation. We rarely used the rundeck web interface though.</p>

<p>Since rundeck exposes an API to its &ldquo;jobs&rdquo;, we talked to that API from our chatbot.</p>

<p>For instance a release typically went like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>user: hubot rundeck run release
</span><span class='line'>hubot: running job release &lt;jobid&gt; &lt;link to rundeck job status page&gt;
</span><span class='line'>user: hubot rundeck output &lt;job id&gt;
</span><span class='line'>hubot: &lt;some of the logging output from the job&gt;
</span><span class='line'>rundeck: job release completed successfully
</span><span class='line'>user: hubot dell me happy
</span><span class='line'>hubot: &lt;image of a happy Michael Dell&gt;</span></code></pre></td></tr></table></div></figure>


<p>Now that might not be the most &ldquo;devops-y&rdquo; (where devops is misconstrued as being only about CD) but our release cycles were weekly so whatever.</p>

<p>The thing is our release process had to be coordinated across multiple systems in a strict order. That &ldquo;release&rdquo; job was actually a rundeck job that called OTHER rundeck jobs and coordinated the failure handling and such.</p>

<p>At my current gig, we&rsquo;re going even farther as Rundeck is almost the entire core of how we do ANYTHING operationally. When I came in we had a lot of knowledge locked away in shell scripts that were finicky and had sharp edges.</p>

<p>The first thing we did was migrate all of these shell scripts into rundeck to solve the finicky sharp edge problem.</p>

<p>Rundeck let&rsquo;s you define arbitrary options to any job. You can reference these options via a special variable syntax in your jobs. So if I need to do something like ensure that a specific option is required or it matches a specific format, I can do that. I can define a remote url or file to pull a list of valid options from too. Basically I can bring sanity to a process that isn&rsquo;t going away anytime soon.</p>

<p>Here&rsquo;s a few screenshots:</p>

<h2>Taking sensitive input</h2>

<p><img src="http://lusis.github.com/images/posts/rundeck/secret_option.png" alt="Defining a secure input" />
Unless you explicitly print out this variable, Rundeck will never log it and the text box for entering it is a password type.</p>

<h2>Enforcing a regex</h2>

<p><img src="http://lusis.github.com/images/posts/rundeck/regex_option.png" alt="Defining a regex based input" />
This is handy for values where you need to match a specific pattern</p>

<p><img src="http://lusis.github.com/images/posts/rundeck/run_job_error.png" alt="no way, jose" />
As you can see, attempting to input invalid data prevents the job from running</p>

<h2>List of remote options</h2>

<p><img src="http://lusis.github.com/images/posts/rundeck/remote_options.png" alt="Defining a list option that is sourced from an external file" />
In many of our jobs we share options. By using this mechanism we don&rsquo;t have to duplicate the enumeration of valid values. We just store them in a file and use that across multiple jobs</p>

<h1>Wrapping powerful CLI tools</h1>

<p><em>I assume a bit of terraform knowledge here. If there&rsquo;s &ldquo;demand&rdquo; I can make a separate blog post</em></p>

<p>As many people know, I&rsquo;m a HUGE fan of <a href="http://terraform.io">terraform</a>. However, like most shell scripts, terraform can have some <a href="http://charity.wtf/2016/02/23/two-weeks-with-terraform/">sharp edges</a>. It also has some limitations currently due to how its resource graph works. A good example is that it doesn&rsquo;t have conditionals. Another example is that variables cannot contain references to other variables.</p>

<p>We use terraform for provisioning private deploys of our stack for customers as well as standing up staging instances. Pretty much any interaction we have with the AWS api comes from terraform.</p>

<p>But as I said, it has some limitations. To work around this, we wrap all of our terraform stuff with Makefiles.
Part of the job of the makefile is to take variables passed in, pass those through sed to a templated <code>terraform.tfvars</code> file and write the final file: The first part of almost all of our makefiles looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ifneq ($(wildcard override.mk),)
</span><span class='line'>$(info $(shell echo "\033[0;31m\n**** Local overrides found. Applying: ****\n\033[0m\n"))
</span><span class='line'>include override.mk
</span><span class='line'>endif
</span><span class='line'>
</span><span class='line'>SHELL     := /bin/bash
</span><span class='line'>REQUIRED_VARS := $(shell grep -oP '@@\K(.*)@@' terraform.tfvars.tmpl | sed -e 's|@@||g' | uniq | sed -e 's|\n| |g')
</span><span class='line'>_moddir := $(shell basename `pwd`)
</span><span class='line'>export ORGNAME := $(ORGNAME)
</span><span class='line'>export AWSREGION ?= us-west-2
</span><span class='line'>export BACKENDINSTANCE ?= c3.2xlarge
</span><span class='line'>export IS_RUNDECK ?= true
</span><span class='line'>export TFFORCEDESTROY ?= false
</span><span class='line'>export TFAPPLY ?= true
</span><span class='line'>
</span><span class='line'>.PHONY: help
</span><span class='line'>
</span><span class='line'>.DEFAULT_GOAL := help
</span><span class='line'>
</span><span class='line'>help:
</span><span class='line'>  @grep -h -P '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>check-env: ## Check the environment to be sure it's okay to build
</span><span class='line'>ifeq ($(ORGNAME),)
</span><span class='line'>  $(error ORGNAME is required)
</span><span class='line'>endif
</span><span class='line'>ifeq ($(BACKENDINSTANCE),)
</span><span class='line'>  $(error BACKENDINSTANCE is required)
</span><span class='line'>endif
</span><span class='line'>ifeq ($(AWSREGION),)
</span><span class='line'>  $(error AWSREGION is required)
</span><span class='line'>endif
</span><span class='line'>ifeq ($(AWS_ACCESS_KEY_ID),)
</span><span class='line'>  $(error AWS_ACCESS_KEY_ID is required)
</span><span class='line'>endif
</span><span class='line'>ifeq ($(AWS_SECRET_ACCESS_KEY),)
</span><span class='line'>  $(error AWS_SECRET_ACCESS_KEY is required)
</span><span class='line'>endif
</span><span class='line'>  @if [[ `expr length "$(ORGNAME)"` -gt 16 ]]; then echo "orgname must be 16 characters or less"; exit 1; fi
</span><span class='line'>  @if [[ "$(ORGNAME)" =~ [^a-zA-Z0-9] ]]; then echo "orgname must be alphanumeric only"; exit 1; fi
</span><span class='line'>  @echo "Everything checks out"
</span><span class='line'>  @echo "Orgname: $(ORGNAME)"
</span><span class='line'>  @echo "AWS region: $(AWSREGION)"
</span><span class='line'>  @echo "is rundeck?: $(IS_RUNDECK)"
</span><span class='line'>  @echo "backend instance type: $(BACKENDINSTANCE)"</span></code></pre></td></tr></table></div></figure>


<p>The repo that stands up copies of our infra is actually multiple distinct terraform &ldquo;projects&rdquo; that use our own collection of modules. Each of those projects uses terraform modules and writes its state to our artifactory server for use in other projects. This helps us isolate sharp edges AND allows us to modify distinct bits VERY selectively (i.e. we can add something to the environment without maintaing a huge dependency graph or risking blowing EVERYTHIGN up at once).</p>

<p>So our directory layout looks something like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>├── backend
</span><span class='line'>├── cassandra
</span><span class='line'>├── chef
</span><span class='line'>├── config
</span><span class='line'>├── consul
</span><span class='line'>├── dns
</span><span class='line'>├── elb
</span><span class='line'>├── files
</span><span class='line'>├── frontend
</span><span class='line'>├── idsite
</span><span class='line'>├── rds
</span><span class='line'>├── rundeck
</span><span class='line'>├── s3-iam-ssh
</span><span class='line'>├── scripts
</span><span class='line'>├── templates
</span><span class='line'>└── vpc</span></code></pre></td></tr></table></div></figure>


<p>Each of those directories has its own makefile and there&rsquo;s a specific order they need to be run in (i.e. the first step is the s3-iam-ssh &ldquo;project&rdquo; which creates ssh keys, iam roles and s3 buckets. That run pushes its state up and those are used downstream by vpc provisioning. So on and so on.</p>

<p>Obviously this could all be documented in a README (and there are READMEs in each directory) but having someone run that manually is &ldquo;dangerous&rdquo;. We could write a parent Makefile and submake things but that&rsquo;s not as easy either. So we wrap this in a rundeck job that enforces order, handles failures by backing out previous steps.</p>

<p>Additionally, there are NON-terraform tasks that have to happen like provisioning a dedicated Chef org for each &ldquo;environment&rdquo; and running Chef. Terraform can do some of that but it&rsquo;s easier outside of Terraform right now.</p>

<p>As I said, all of these individual things that need to happen are their own Rundeck job. There&rsquo;s a wrapper job that handles calling them in the right order and passing the relevant information along. That wrapper job looks like this:</p>

<p><img src="http://lusis.github.com/images/posts/rundeck/terraform_options.png" alt="SHIELDS UP" /></p>

<p>Now that&rsquo;s a LOT of input needed but it&rsquo;s also fire-and-forget to some degree. What&rsquo;s nice is that we don&rsquo;t have to store any AWS keys on the rundeck server (and the keys that are used initially are only used long enough to provision NEW limited keys for further terraform jobs). They&rsquo;re never logged either (because we use the secure input for that entry).</p>

<p>We can enforce a list of valid instance sizes (again due to lack of conditionals in terraform, we have to have specific modules for certain classes of instances - i.e. variable numbers of ephemeral volumes per instance type). We can enforce a specific regions (we only run in regions with 3 AZs).</p>

<p>This was all evolutionary too. This process was originally a bunch of shell scripts calling the AWS cli tools and some manual work with the console. The benefit of rundeck here was that we could bring some initial sanity to the process and then migrate to something &ldquo;better&rdquo;.</p>

<p>Now I can understand that you would look at that and say &ldquo;gee golly john this is nice and all but it&rsquo;s still a manual process and it&rsquo;s a web ui and blah blah&rdquo;. The thing is this is only the rundeck part. We&rsquo;ve not yet integrated this into our chatbot but we HAVE fronted all of this rundeck &ldquo;ugliness&rdquo; with something much nicer and something more user friendly. We call it all via the rundeck api.</p>

<p>The thing is terraform is really good at what it does. Could I have spent a shedload of time writing my own webapp that made AWS api calls and did coordination and bootstrapping nodes with Chef and all that? Sure but I already had a lot of that logic implemented elsewhere that I could reuse.</p>

<p>I didn&rsquo;t have to decide on a data store or a managing that state or building a fucking orchestration engine.</p>

<h1>Wrap-up</h1>

<p>Rundeck isn&rsquo;t perfect. No tool is. We fought a few things:</p>

<ul>
<li>while rundeck has ssh as a default first-class execution engine, it has ZERO concept of using a jump host</li>
<li>api keys are limited and can&rsquo;t really be used when using an external authentication source (we use LDAP for our rundeck auth - soon to be migrated to using Stormpath)</li>
<li>jobs are REALLY hard to write outside of the web interface. Yes you can do it (yaml or xml) but simply due to the sheer flexibility of a job definition it&rsquo;s rather pointless. I don&rsquo;t entirely fault them for this.</li>
<li>managing permissions is really cumbersome and has its own dsl</li>
</ul>


<p>The upshot here is that it <em>IS</em> flexible:</p>

<ul>
<li>You can create custom execution &ldquo;drivers&rdquo; so we were able to pretty easily work around the jump box issue.</li>
<li>Node definitions can come from multiple sources and multiple types of sources. And they&rsquo;re cumulative (i.e. we can define multiple wildcard paths very easily as well as an external source)</li>
<li>Node definitions support arbitrary data that can be referenced. We used that bit to define a node&rsquo;s jump host and jump user/keypair</li>
<li>There&rsquo;s a built in key store system (which we are not yet using) that can help with a few things</li>
<li>managing permissions is REALLY REALLY fine-grained</li>
</ul>


<p>We&rsquo;re already looking at migrating to using the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/execute-remote-commands.html">AWS SSM/RunCommand stuff</a> as the execution engine.</p>

<p>Anyway, that&rsquo;s how I used rundeck and why it&rsquo;s become so valuable internally.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aws Api Gateway for Fun and Profit]]></title>
    <link href="http://lusis.github.com/blog/2015/12/09/aws-apigateway-for-fun-and-profit/"/>
    <updated>2015-12-09T22:06:59-05:00</updated>
    <id>http://lusis.github.com/blog/2015/12/09/aws-apigateway-for-fun-and-profit</id>
    <content type="html"><![CDATA[<p>At we&rsquo;re currently in the process of reconfiguring our monitoring, logging and alerting setup.</p>

<!-- more -->


<p>Obviously this is something near and dear to my heart. At my previous employer we did everything in house due to various constraints.
I&rsquo;ve got a rich set of experiences in this space so my first inclination was to build not buy. However, due to OTHER constraints, building was not a practical solution at present.</p>

<p>After much research and evaluation, we finally settled on the following stack:</p>

<ul>
<li>Loggly</li>
<li>SignalFx</li>
<li>OpsGenie</li>
</ul>


<p>I&rsquo;m not going into reasons for switching in this post or details about WHY these providers were chosen. This is to discuss a specific AWS service and how it can really change the nature of ChatOps and more.</p>

<h1>Integration</h1>

<p>Our monitoring and metrics provider, SignalFx, is still building out its integrations.
They have a rich set already and are iterating very quickly but coupled with the migration to OpsGenie, we needed to work out a solution for wiring the two together.
Using the power of google, I came across a great post from <a href="https://twitter.com/ripienaar">R.I. Pienaar</a> about leveraging <a href="https://www.devco.net/archives/2015/08/13/translating-webhooks-with-aws-api-gateway-and-lambda.php">Lambda and the AWS API Gateway products</a>.</p>

<p>In his post, he didn&rsquo;t really go over the API Gateway configuration so I figured I would document what I &ldquo;figured out&rdquo; for others.</p>

<h2>The flow</h2>

<p>The above post covers most of it but the general idea is:</p>

<ul>
<li><a href="https://support.signalfx.com/hc/en-us/articles/203824569-Detectors-and-alerts">SignalFx Alerting</a> webhook fires</li>
<li>AWS API Gateway gets webhook and fires off a Lambda task</li>
<li>Lambda task translates webhook into OpsGenie <a href="https://www.opsgenie.com/docs/web-api/alert-api">create alert</a> api call</li>
<li>OpsGenie wakes you from a dead sleep</li>
</ul>


<p>Now you might wonder why go through all this hassle? OpsGenie can create email integrations and SignalFx can send alerts to email address.</p>

<h1>It&rsquo;s about context.</h1>

<p>When email alerts come in to OpsGenie, they&rsquo;re just &ldquo;pings&rdquo;:</p>

<blockquote><p>Hey this thing sent us an email and here's the subject. Log in to the website to see the body</p></blockquote>


<p>Frankly there&rsquo;s not much else they can do. SignalFx will also send another email when an alert auto-clears but to OpsGenie, that&rsquo;s another &ldquo;ping&rdquo;. It&rsquo;s unrelated to the previous email.</p>

<p><img src="http://lusis.github.com/images/posts/apigateway-lambda/hipchat-emails.png" alt="Hipchat alerts from OpsGenie" /></p>

<p>As you can see above this could get painful and I already have issues with &ldquo;alert fatigue&rdquo;.</p>

<p>Additional, even if you <strong>DID</strong> log into the OpsGenie website to look at the details, there&rsquo;s really not much there:</p>

<p><img src="http://lusis.github.com/images/posts/apigateway-lambda/opsgenie-emails.png" alt="Emails in OpsGenie" /></p>

<p>Yeah that&rsquo;s helpful&hellip;</p>

<p>So we need to accomplish two things:</p>

<ul>
<li>Correlate alerts and auto-closes from SignalFx</li>
<li>Get some damn context into the alert so we can act intelligently on it</li>
</ul>


<p>The end result, gives us this:</p>

<p><img src="http://lusis.github.com/images/posts/apigateway-lambda/webhook-hipchat.png" alt="Hipchat alerts from OpsGenie" />
<img src="http://lusis.github.com/images/posts/apigateway-lambda/signalfx-context-1.png" alt="SignalFx Metadata in OpsGenie" />
<img src="http://lusis.github.com/images/posts/apigateway-lambda/opsgenie-alert-history.png" alt="OpsGenie Alert History" /></p>

<p>and that&rsquo;s MUCH more useful.</p>

<h1>How we did it</h1>

<p>AWS API Gateway is a PRETTY intimidating thing. There&rsquo;s lots of very specific terminology and frankly I found the docs not so useful.
The general idea is this:</p>

<ul>
<li>Create an api</li>
<li>Create a &ldquo;resource&rdquo; this is just a route that the api gateway responds to i.e. <code>/signalfx_to_loggly</code> via <code>POST</code></li>
<li>Create a &ldquo;model&rdquo;: This is a json schema that describes what the incoming <code>POST</code> body looks like and its content type</li>
<li>Create an &ldquo;integration request&rdquo;: What do you do when you get it? In this case call a Lambda task</li>
</ul>


<p>There are also concepts like &ldquo;stages&rdquo; which frankly were a bit useless in my case.
In the end, after you get all of this wired up, you&rsquo;ll be given a url that looks something like this:</p>

<p><code>https://&lt;random id&gt;.execute-api.&lt;region&gt;.amazonaws.com/&lt;stage name&gt;/&lt;resource name&gt;</code></p>

<p>and that&rsquo;s your webhook url.</p>

<h2>Models</h2>

<p>In the case of SignalFx, a webhook post looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;incidentId&quot;</span><span class="p">:</span><span class="s2">&quot;XXXXXXX&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;detectorUrl&quot;</span><span class="p">:</span><span class="s2">&quot;https://app.signalfx.com/#/detector/XXXXXXX/edit&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;status&quot;</span><span class="p">:</span><span class="s2">&quot;too high&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;rule&quot;</span><span class="p">:</span><span class="s2">&quot;500 errors over 10&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;severity&quot;</span><span class="p">:</span><span class="s2">&quot;Major&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;eventType&quot;</span><span class="p">:</span><span class="s2">&quot;_SF_PLOT_KEY_XXXXXXX_3_17&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;currentValue&quot;</span><span class="p">:</span><span class="s2">&quot;2&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;sources&quot;</span><span class="p">:</span><span class="s2">&quot;chef-external-elb&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;detector&quot;</span><span class="p">:</span><span class="s2">&quot;ELB 5xx Detector&quot;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>which translates to the following model:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;$schema&quot;</span><span class="p">:</span> <span class="s2">&quot;http://json-schema.org/draft-04/schema#&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;title&quot;</span><span class="p">:</span> <span class="s2">&quot;signalfx-webhook-model&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;properties&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;incidentId&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;detectorUrl&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;status&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;rule&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;severity&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;eventType&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;currentValue&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;sources&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;detector&quot;</span><span class="p">:{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;string&quot;</span><span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<h1>The lambda function</h1>

<p>I&rsquo;m still cleaning up the banged out code for the Lambda function to post on github however here&rsquo;s the relevant bits from the <code>opsgenie.js</code> file I added to <a href="https://github.com/ripienaar/lambda_webhook_gwy">R.I.&rsquo;s code</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'>  <span class="nx">sfxNotification</span><span class="o">:</span> <span class="kd">function</span><span class="p">(</span><span class="nx">event</span><span class="p">,</span> <span class="nx">config</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">result</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">apiKey</span><span class="o">:</span> <span class="nx">config</span><span class="p">.</span><span class="nx">api_key</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">alias</span><span class="o">:</span> <span class="nx">event</span><span class="p">.</span><span class="nx">incidentId</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">note</span><span class="o">:</span> <span class="nx">event</span><span class="p">.</span><span class="nx">detectorUrl</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">message</span><span class="o">:</span> <span class="s2">&quot;Alert: &quot;</span><span class="o">+</span><span class="nx">event</span><span class="p">.</span><span class="nx">rule</span><span class="o">+</span><span class="s2">&quot; (&quot;</span><span class="o">+</span><span class="nx">event</span><span class="p">.</span><span class="nx">detector</span><span class="o">+</span><span class="s2">&quot;)&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">description</span><span class="o">:</span> <span class="s2">&quot;Current value: &quot;</span><span class="o">+</span><span class="nx">event</span><span class="p">.</span><span class="nx">currentValue</span><span class="o">+</span><span class="s2">&quot; is &quot;</span><span class="o">+</span><span class="nx">event</span><span class="p">.</span><span class="nx">status</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">details</span><span class="o">:</span> <span class="nx">event</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">tags</span><span class="o">:</span> <span class="nx">event</span><span class="p">.</span><span class="nx">sources</span><span class="p">.</span><span class="nx">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">),</span>
</span><span class='line'>      
</span><span class='line'>  <span class="p">};</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">return</span> <span class="nx">result</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This is the meat of the translation. You can customize this as needed but note how we immediately add a <code>note</code> to the incident with the link to the graph in SignalFX.
When you create a detector in SignalFx, any groupings you create in the signal function are put into the <code>sources</code> key as comma-separated values.
We take these and make them <code>tags</code> in OpsGenie.</p>

<p>We also leverage the OpsGenie <code>alias</code> to create our own ID for the event. Normally you would store this id somewhere for reference later but instead we use the unique id from SignalFX.
This make correlating a previous alert dead simple as you can see below:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">self</span> <span class="o">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">exports</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>  <span class="nx">publish</span><span class="o">:</span> <span class="kd">function</span><span class="p">(</span><span class="nx">handler</span><span class="p">,</span> <span class="nx">event</span><span class="p">,</span> <span class="nx">context</span><span class="p">,</span> <span class="nx">callback</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="kd">var</span> <span class="nx">webhook</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s2">&quot;./webhook_request.js&quot;</span><span class="p">);</span>
</span><span class='line'>    <span class="kd">var</span> <span class="nx">config</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s2">&quot;./config.js&quot;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="p">(</span><span class="nx">event</span><span class="p">.</span><span class="nx">status</span> <span class="o">==</span> <span class="s2">&quot;ok&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">request</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">closeRequest</span><span class="p">(</span><span class="nx">config</span><span class="p">.</span><span class="nx">opsgenie</span><span class="p">);</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">data</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">apiKey</span><span class="o">:</span> <span class="nx">config</span><span class="p">.</span><span class="nx">opsgenie</span><span class="p">.</span><span class="nx">api_key</span><span class="p">,</span>
</span><span class='line'>          <span class="nx">alias</span><span class="o">:</span> <span class="nx">event</span><span class="p">.</span><span class="nx">incidentId</span>
</span><span class='line'>      <span class="p">};</span>
</span><span class='line'>    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">request</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">request</span><span class="p">(</span><span class="nx">config</span><span class="p">.</span><span class="nx">opsgenie</span><span class="p">);</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">data</span> <span class="o">=</span> <span class="nx">self</span><span class="p">[</span><span class="nx">handler</span><span class="p">](</span><span class="nx">event</span><span class="p">,</span> <span class="nx">config</span><span class="p">.</span><span class="nx">opsgenie</span><span class="p">,</span> <span class="nx">context</span><span class="p">);</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="nx">webhook</span><span class="p">.</span><span class="nx">publish</span><span class="p">(</span><span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">data</span><span class="p">),</span> <span class="nx">request</span><span class="p">,</span> <span class="nx">context</span><span class="p">,</span> <span class="nx">callback</span><span class="p">);</span>
</span><span class='line'>  <span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>SignalFX has 4 status it can set for an alert:</p>

<ul>
<li><code>ok</code></li>
<li><code>too high</code></li>
<li><code>too low</code></li>
<li><code>anomalous</code></li>
</ul>


<p>Here we check if the status is <code>ok</code> and if so, we call a different function to generate the request object to a different path (the <code>close</code> alert path).
Since we created the alert using an <code>alias</code> of the SignalFx <code>incidentId</code>, we don&rsquo;t even need to do any more parsing - just <code>POST</code> to the <code>close</code> resource with the <code>alias</code> id.</p>

<p>As you saw above we get more more data and context about the alert and this is all also visible in the OpsGenie mobile app.</p>

<h1>Benefits</h1>

<p>Honestly the biggest benefit is not having to wait on service providers to create native integrations.
Almost every service I&rsquo;ve used over the past several years offers an outgoing webhook capability for their system.
API Gateway solves the part of getting those webhooks and its native Lambda support means I don&rsquo;t need to leave anything &ldquo;running&rdquo; to maintain, upgrade and support just to do something with that webhook.</p>

<p>However one other huge benefit that is also hinted in the other post is that using this model, you get a simple abstraction from your service provider.
If we wanted to move to some other alerting provider, we just change the lambda function to post there instead. No need to redo all the integrations in SignalFx.
My plan is to work towards a model where we try and utilize the API gateway as the webhook endpoint for various services and translate to our other providers from there.</p>

<p>I really enjoyed working with the gateway. Testing wasn&rsquo;t too painful as it has a &ldquo;mock&rdquo; mode as well where it behaves similarly to requestb.in. It supports a couple of different authentication methods well (though sadly they couldn&rsquo;t be leveraged in this case due to provider webhook formats (note if you offer webhook support, let your users define a custom header as part of the webhook!).</p>

<p>Lambda needs to support more languages but python is a happy medium from the mess of javascript and the heft of java.</p>

<h1>Future steps</h1>

<p>I&rsquo;m going to work on first cleaning up this specific Lambda task and making it available on github. After that my next steps are to rewrite the damn thing in Python because I &ldquo;dislike&rdquo; javascript.
We&rsquo;re already using this similar model for another integration as well: Loggly and HipChat.</p>

<p>Loggly has HipChat support already but it posts the messages in HipChat as a raw json dump which is useless:</p>

<p><img src="http://lusis.github.com/images/posts/apigateway-lambda/loggly-json.png" alt="Loggly JSON" /></p>

<p>by migrating it to use API Gateway and Lambda, we get the following instead:</p>

<p><img src="http://lusis.github.com/images/posts/apigateway-lambda/loggly-better.png" alt="Loggly better" />
<em>Obviously that&rsquo;s an MVP</em></p>

<p>We&rsquo;ll probably go down this route farther for other integrations and especially when HipChat Connect goes GA. Then we&rsquo;ll likely start posting richer messages using &ldquo;cards&rdquo; similar to my previous experience with Slack&rsquo;s API.</p>

<h2>ChatOps</h2>

<p>One thing that&rsquo;s really neat is that the api gateway approach can let you make some REALLY simple tools for ChatOps using outgoing webhooks from your chat system.</p>

<p>However it will REALLY open up when Lambda gets VPC support. Imagine a Lambda function that can fire inside your VPC and interact with all your private resources. It&rsquo;s both terrifying and thrilling assuming all the appropriate controls are in place (using api keys, rbac in your outgoing webhook).</p>

<p>Thanks for reading. I hope it was valuable to you.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terraform Modules for Fun and Profit]]></title>
    <link href="http://lusis.github.com/blog/2015/10/12/terraform-modules-for-fun-and-profit/"/>
    <updated>2015-10-12T00:56:04-04:00</updated>
    <id>http://lusis.github.com/blog/2015/10/12/terraform-modules-for-fun-and-profit</id>
    <content type="html"><![CDATA[<p>One of the projects I&rsquo;ve been using more and more recently is <a href="http://terraform.io">terraform</a>.</p>

<!-- more -->


<p>I&rsquo;ve been using it for a while on a smaller scale but a recent project at <a href="http://stormpath.com">work</a> had me putting it through its paces and working out the issues I had. A few of my issues in the past have revolved around:</p>

<ul>
<li>reusability</li>
<li>composability</li>
<li>lifecycle</li>
<li>&ldquo;safety&rdquo; (i.e. don&rsquo;t blow away an entire running stack accidentally)</li>
<li>scope</li>
</ul>


<h1>Scope</h1>

<p>One of the mistakes I made when first using terraform was trying to &ldquo;do too much&rdquo; with it. Like many people, I wanted to see how much I could replace the &ldquo;fragility&rdquo; of existing CM-focused stacks (i.e. <code>knife ec2</code> commands in a NOTES.md file) with something &ldquo;better&rdquo;.</p>

<p>As I used Terraform, I tried to put more and more logic into it. I ended up fighting trying to get things like data about the current launch into userdata. Once I scaled back the scope of what I tried to do with Terraform, it became much easier to use it &ldquo;the right way&rdquo;. Combined with Packer to build images with just a BIT more baked in, things got much easier.</p>

<p>If you can&rsquo;t bake immutable amis for your stack (and I think that most non-trivial cases can&rsquo;t yet), don&rsquo;t try to fit application lifecycle into terraform. I would argue that fitting application lifecycle into terraform is too much for a few reason.</p>

<h1>Safety</h1>

<p>Terraform does its job really well. Too well sometimes. Imagine the following scenario:</p>

<ul>
<li>provision VPC</li>
<li>provision ELB</li>
<li>provision an RDS instance</li>
<li>provision instances with userdata to pull an application and run it</li>
</ul>


<p>You stand up your infrastructure with Terraform using the steps above. All is well. Now you need release a new version of your application.</p>

<ul>
<li>update the userdata to pull the new version of the application</li>
<li><code>terraform apply</code></li>
</ul>


<p>Boom you&rsquo;ve just updated yourself into an outage. Any change you make to your terraform code will cause terraform to destroy that resource and recreate it.
Now terraform has the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lifecycle {
</span><span class='line'>  create_before_destroy = true
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>but it does have limitations and you&rsquo;re forcing everyone to not forget to add that. Honestly that doesn&rsquo;t instill too much confidence that I won&rsquo;t automate myself in the foot.</p>

<p>I realized the trick to terraform safety is to minimize the number of tf files you might have to modify as standard operation. By limiting its scope appropriately and through reuse/composability you can do this. About that reuse thing&hellip;</p>

<h1>Composability/Reuse</h1>

<p>And now we get to the part that will level up your Terraform game - using modules.</p>

<p>The problem is that Terraform modules are currently the least documented part of the Terraform ecosystem either in the official documentation or the community precisely because they are a bit difficult to grok. At the end I&rsquo;m going to point you to some community resources that helped me figure it all out myself.</p>

<h2>Outputs</h2>

<p>The biggest thing that helped me understand modules was finally groking that <a href="https://www.terraform.io/docs/configuration/outputs.html">outputs</a> are NOT just for displaying at the end of a <code>terraform apply</code> or for use as variables in subshells with <code>terraform output</code>.</p>

<p>When using modules, outputs are essentially part of the public API to your module. Once I framed it this way I realized that Terraform modules are libraries except there are no methods to call.</p>

<h2>Modules as Libraries</h2>

<p>A module defines its contract in two ways:</p>

<ul>
<li>Here are the inputs I can take</li>
<li>Here are the outputs I give you</li>
</ul>


<p>Once you start moving some of your Terraform code into modules, you will be forced to create this contract for yourself. Let&rsquo;s take an example from AWS that is highly reusable, VPC creation:</p>

<h3>variables.tf</h3>

<p>First we define our inputs in <code>variables.tf</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>variable "mymod_region" {}
</span><span class='line'>variable "mymod_az" {}
</span><span class='line'>variable "mymod_vpc_cidr" {}
</span><span class='line'>variable "mymod_private_subnet" {}
</span><span class='line'>variable "mymod_public_subnet" {}</span></code></pre></td></tr></table></div></figure>


<p>We&rsquo;re defining 5 inputs for this VPC all prefixed with <code>mymod</code> (more on that in a moment). This module is for creating a VPC that has two subnets - one for instances that get public IPs (like a nat instance) and one for private instances. We could add others here (and our internal vpc module actually provides for a subnet in each AZ of the region used). You can certainly provide sane defaults for these if the module is intended for specific internal use but I would say NOT to do that in the interest of forcing yourself to make it properly reusable.</p>

<h3>outputs.tf</h3>

<p>Now we define some outputs:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>output "vpc_id" { value = "${aws_vpc.default.id}" }
</span><span class='line'>output "private_subnet_id" { value = "${aws_subnet.private.id}" }
</span><span class='line'>output "public_subnet_id" { value = "${aws_subnet.public.id}" }</span></code></pre></td></tr></table></div></figure>


<p>This is where we return things in the format that most people are used to seeing in terraform (<em>resource.name.attribute</em>).
At this point we now have a module that takes information in, does something and spits back out some data for us.</p>

<h2>Using the module</h2>

<p>Now let&rsquo;s say we need to launch an instance into the public subnet and we want to use this VPC module. So we create a new terraform project/repo with the following files:</p>

<ul>
<li><code>main.tf</code>: Holds our module import definitions.</li>
<li><code>instance.tf</code>: Terraform code for the instance resource.</li>
<li><code>outputs.tf</code>: The data we&rsquo;re outputing</li>
<li><code>variables.tf</code>: The variables we&rsquo;re using</li>
</ul>


<h3><code>variables.tf</code></h3>

<p>We&rsquo;ll start here. You&rsquo;ll notice we&rsquo;re going to mix and match our variables for the instance as well as the ones needed by the VPC module. I&rsquo;m not going to include all of the things I might make a variable in the interest of length:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>variable "aws_region" { default = "us-west-2" }
</span><span class='line'>variable "az" { default = "us-west-2a" }
</span><span class='line'>variable "ami" {}
</span><span class='line'>variable "vpc_cidr" {}
</span><span class='line'>variable "private_subnet" {}</span></code></pre></td></tr></table></div></figure>


<h3><code>main.tf</code></h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>provider "aws" { region = "${var.aws_region}" }
</span><span class='line'>module "vpc" {
</span><span class='line'>  source = "git::ssh://git@gitserver/org/my-tf-modules//vpc" // double slash intended. See terraform documentation
</span><span class='line'>  mymod_region = "${var.aws_region}"
</span><span class='line'>  mymod_az = "${var.az}"
</span><span class='line'>  mymod_vpc_cidr = "${var.vpc_cidr}"
</span><span class='line'>  mymod_private_subnet = "${var.private_subnet}"
</span><span class='line'>  mymod_public_subnet = "${var.public_subnet}"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The reason I prefixed the inputs in the VPC module was to help with clarity. This is why it makes it (for me) easier to see where the flow of inputs and outputs is.</p>

<h3><code>instance.tf</code></h3>

<p>I&rsquo;m going to truncate this code a bit only to show where we reuse the various variables:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>resource "aws_instance" "node1" {
</span><span class='line'>  ami = "${var.ami}"
</span><span class='line'>  subnet_id = "${module.vpc.private_subnet_id}"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>As you can see we&rsquo;re using a bit of our local variables (the ami) with the variables from the VPC module. Obviously there are other things that are required here to actually launch that resource but you get the idea.</p>

<h2>Some gotchas with modules</h2>

<p>These aren&rsquo;t hard and fast rules but they will help your sanity</p>

<h3>Don&rsquo;t nest modules</h3>

<p>Looking above you might think that you could convert that into another module (an instance module which uses the VPC module). Don&rsquo;t.
Module variable/output visibility is from the ROOT module only. In the case of nested module usage, the root module is not the one you&rsquo;re writing but the transitive module&rsquo;s caller.</p>

<p>If you were to try and create a module that does these things you would end up needing to create inputs on each outer module to pass down to any modules IT calls as well as outputs to bubble the information all the way back up to the code from where you&rsquo;re using the module.</p>

<p>i.e:</p>

<ul>
<li>instance module will need to define inputs that it accepts to pass to the vpc module</li>
<li>instance module will need to define outputs that duplicate what is returned from the vpc module</li>
</ul>


<p>That becomes VERY cumbersome. Be very shallow with your module usage. Use a single wrapper project to pull in all the modules you need.</p>

<h2>You can duplicate modules under different names</h2>

<p>This is very handy (though cumbersome in terms of duplication of code). The following is from one of our internal &ldquo;wrapper&rdquo; project that launches a 3-node version of our stack for testing</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>resource "aws_key_pair" "deployer" {
</span><span class='line'>  key_name = "${var.orgname}-deployer-key"
</span><span class='line'>  public_key = "${file("${var.orgname}-deployer-key.pub")}"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>module "vpc" {
</span><span class='line'>  source = "git::ssh://git@repo/org/tf-modules.git//vpc"
</span><span class='line'>  spath_orgname = "${var.orgname}"
</span><span class='line'>  spath_nat_ami = "${lookup(var.amis, var.aws_region)}"
</span><span class='line'>  spath_nat_a_private_ip = "${var.nat_a_private_ip}"
</span><span class='line'>  spath_nat_b_private_ip = "${var.nat_b_private_ip}"
</span><span class='line'>  spath_nat_c_private_ip = "${var.nat_c_private_ip}"
</span><span class='line'>  spath_public_subnet_a_cidr = "${var.public_subnet_a_cidr}"
</span><span class='line'>  spath_public_subnet_b_cidr = "${var.public_subnet_b_cidr}"
</span><span class='line'>  spath_public_subnet_c_cidr = "${var.public_subnet_c_cidr}"
</span><span class='line'>  spath_private_subnet_a_cidr = "${var.private_subnet_a_cidr}"
</span><span class='line'>  spath_private_subnet_b_cidr = "${var.private_subnet_b_cidr}"
</span><span class='line'>  spath_private_subnet_c_cidr = "${var.private_subnet_c_cidr}"
</span><span class='line'>  spath_vpc_cidr = "${var.vpc_cidr}"
</span><span class='line'>  spath_keyname = "${var.orgname}-deployer-key"
</span><span class='line'>  aws_region = "${var.aws_region}"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>module "rds" {
</span><span class='line'>  source = "git::ssh://git@repo/org/tf-modules.git//rds"
</span><span class='line'>  spath_orgname = "${var.orgname}"
</span><span class='line'>  iam_rds_user = "${var.iam_rds_user}"
</span><span class='line'>  iam_rds_password = "${var.iam_rds_password}"
</span><span class='line'>  spath_rds_sg_id = "${module.vpc.default_sg_id}"
</span><span class='line'>  spath_rds_private_subnet_a_id = "${module.vpc.private_subnet_a_id}"
</span><span class='line'>  spath_rds_private_subnet_b_id = "${module.vpc.private_subnet_b_id}"
</span><span class='line'>  spath_rds_private_subnet_c_id = "${module.vpc.private_subnet_c_id}"
</span><span class='line'>  aws_region = "${var.aws_region}"
</span><span class='line'>}
</span><span class='line'>module "dockernode_subnet_a" {
</span><span class='line'>  source = "git::ssh://git@repo/org/tf-modules.git//dockerhost"
</span><span class='line'>  spath_orgname = "${var.orgname}"
</span><span class='line'>  dockernode_ami = "${lookup(var.amis, var.aws_region)}"
</span><span class='line'>  dockernode_keypair_name = "${var.orgname}-deployer-key"
</span><span class='line'>  dockernode_subnet_id = "${module.vpc.private_subnet_a_id}"
</span><span class='line'>  dockernode_az = "${module.vpc.private_subnet_a_az}"
</span><span class='line'>  dockernode_sg_id = "${module.vpc.default_sg_id}"
</span><span class='line'>  aws_region = "${var.aws_region}"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>module "dockernode_subnet_b" {
</span><span class='line'>  source = "git::ssh://git@repo/org/tf-modules.git//dockerhost"
</span><span class='line'>  spath_orgname = "${var.orgname}"
</span><span class='line'>  dockernode_ami = "${lookup(var.amis, var.aws_region)}"
</span><span class='line'>  dockernode_keypair_name = "${var.orgname}-deployer-key"
</span><span class='line'>  dockernode_subnet_id = "${module.vpc.private_subnet_b_id}"
</span><span class='line'>  dockernode_az = "${module.vpc.private_subnet_b_az}"
</span><span class='line'>  dockernode_sg_id = "${module.vpc.default_sg_id}"
</span><span class='line'>  aws_region = "${var.aws_region}"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>module "dockernode_subnet_c" {
</span><span class='line'>  source = "git::ssh://git@repo/org/tf-modules.git//dockerhost"
</span><span class='line'>  spath_orgname = "${var.orgname}"
</span><span class='line'>  dockernode_ami = "${lookup(var.amis, var.aws_region)}"
</span><span class='line'>  dockernode_keypair_name = "${var.orgname}-deployer-key"
</span><span class='line'>  dockernode_subnet_id = "${module.vpc.private_subnet_c_id}"
</span><span class='line'>  dockernode_az = "${module.vpc.private_subnet_c_az}"
</span><span class='line'>  dockernode_sg_id = "${module.vpc.default_sg_id}"
</span><span class='line'>  aws_region = "${var.aws_region}"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>You can see we&rsquo;re reusing the &ldquo;dockerhost&rdquo; module three times under different names. This is because we want to launch a host in each AZ in the region. Terraform currently lacks logic for making that sort of balancing easy so we do it this way. Also note that we&rsquo;re refering to other modules in when pass information into another module.</p>

<p>This is okay as this is our root module and it has that visibility.</p>

<h2>Let the graph resolver work for you</h2>

<p>If you can avoid it, don&rsquo;t try to define dependencies yourself (i.e. using <code>depends_on</code>. You can&rsquo;t use it in module blocks anyway). Instead use variable names that give Terraform the information it needs to build the dependency graph itself. By using <code>${module.vpc.private_subnet_c_id}</code> as the <code>subnet_id</code> for <code>dockernode_subnet_c</code>, Terraform can infer the dependency itself and know that it needs to run that module FIRST before touching this one.</p>

<h2>None of this addresses tfstate</h2>

<p>Everything I&rsquo;ve shown so far doesn&rsquo;t address reusing STATE from a terraform run. These modules are great if you want to provide reusable code that people can use for standing up VPCs but what if you want to share the VPC itself.</p>

<p>Recently terraform has added support for <a href="https://www.terraform.io/docs/state/remote.html">&ldquo;remote state&rdquo;</a>. This allows you to pull in the outputs of another terraform run so that it can be used elsewhere.</p>

<p>If you recall earlier I mentioned that minimizing the need to touch terraform files after initial run helps to cut down on the risk of inadvertant destruction of resources. By using the remote state functionality you can provision your main VPC and refer to its IDs in other terraform code without having to touch the base VPC code.</p>

<p>Mind you this does mean that the modifier of the VPC could could accidentally wipe infrastructure but this is a step in the right direction.</p>

<h2>Don&rsquo;t forget about <code>null_resource</code></h2>

<p>While it doesn&rsquo;t participate in the graph fully, the <code>null_resource</code> can come in really handy. It&rsquo;s currently undocumented but you can see an example of usage <a href="https://gist.github.com/lusis/9c0fd50e0de51c3d80b2">here</a>.</p>

<p>The <code>null_resource</code> allows you to attach to run provisioners outside of a formal resource (such as an instance). You&rsquo;ll probably have to be explicit with dependencies when using the null resource but the limited to scope for it should be okay.</p>

<h2>paths, paths and more paths</h2>

<p>You may notice that our vpc module above makes reference to nat nodes. Our model for VPC usage is one Nat instance per AZ. The nat instance creation, however, is scoped inside the VPC module. It also happens to use a <code>remote-exec</code> provisioner (which requires the SSH key that we create as part of the wrapping terraform scripts).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>resource "aws_instance" "nat_a" {
</span><span class='line'>  // elided
</span><span class='line'>  provisioner "remote-exec" {
</span><span class='line'>    inline = [ "printf '${template_file.iptables_config.rendered}' &gt; /tmp/iptables.sav" ]
</span><span class='line'>    connection {
</span><span class='line'>        type = "ssh"
</span><span class='line'>        user = "admin"
</span><span class='line'>        key_file = "${path.cwd}/${var.spath_keyname}"
</span><span class='line'>        agent = false
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Here we define the path to the key file as being in <code>path.cwd</code>. This means we can create the ssh key in a wrapper terraform script, dump it to the current directory and the nat instance creation will know how to find it. There are other path variables availble (from the TF documentation</p>

<blockquote><p>To reference path information, the syntax is path.TYPE. TYPE can be cwd, module, or root. cwd will interpolate the cwd. module will interpolate the path to the current module. root will interpolate the path of the root module. In general, you probably want the path.module variable.</p><footer><strong>terraform</strong> <cite><a href='https://www.terraform.io/docs/configuration/interpolation.html'>www.terraform.io/docs/&hellip;</a></cite></footer></blockquote>


<h2>Modules are vendored</h2>

<p>When you use modules, the first thing you&rsquo;ll have to do is <code>terraform get</code>. This pulls modules into the <code>.terraform</code> directory. Once you do that, unless you do another <code>terraform get -update=true</code>, you&rsquo;ve essentially vendored those modules. This is nice as you know an upstream change won&rsquo;t immediately invalidate and destroy your infra. I would suggest checking those into your wrapper repo.</p>

<h1>Wrap up</h1>

<p>I hope this has helped you with using terraform a bit. One of the best resources I found on module usage that really helped things click was the <a href="http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up/">following awesome blogpost</a> from Tom Doran (@bobtfish) on the subject. He&rsquo;s made some really useful github repos demonstrating a bunch of stuff. Additionally the work done by Brandon Burton <a href="https://github.com/terraform-community-modules">here</a> can give good examples as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Current Status]]></title>
    <link href="http://lusis.github.com/blog/2015/10/12/current-status/"/>
    <updated>2015-10-12T00:06:00-04:00</updated>
    <id>http://lusis.github.com/blog/2015/10/12/current-status</id>
    <content type="html"><![CDATA[<p>I just realized it&rsquo;s been almost a year since I wrote a blog post&hellip;.</p>

<!-- more -->


<p>In June of this year, I left my previous employer and went to work for <a href="http://stormpath.com">Stormpath</a>.
Stormpath holds a special place in my heart as one of the <a href="https://www.linkedin.com/in/lhazlewood">cofounders</a> is a good friend, former boss and former neighbor. I got to help them early on interviewing ops candidates and I&rsquo;ve been bullish on them for a while. Hell I even <a href="https://github.com/lusis/pam_stormpath">learned C the hard way</a> as a fun side project for them.</p>

<p>One of the nice things about Stormpath is that I&rsquo;ve been able to ramp my OSS contributions and involvement back up considerably from the previous employer. I wasn&rsquo;t entirely in a black hole for 2 years but it sure felt like it sometimes. My previous employer was very good to me but it was time to move on.</p>

<h1>Community Involvement</h1>

<p>Outside of employment, my DevOpsDays involvement has sadly suffered quite a bit. This is honestly unrelated to work and more related to family obligations. Both of my kids are now in school and have their own extra curricular activities. I&rsquo;m helping coach my oldest son&rsquo;s football team this year and it&rsquo;s been one of the most rewarding parental life things ever.</p>

<p>I&rsquo;m really hoping to make it to another DevOpsDays soon and <a href="http://www.devopsdays.org/events/2015-charlotte/">Charlotte</a> looks to be the first one.</p>

<h1>Code</h1>

<p>I&rsquo;ve actually written quite a bit of code recently (mostly in Go). Some of the current projects:</p>

<ul>
<li><a href="https://github.com/lusis/go-rundeck">go-rundeck</a>: A handy library and set of utilities for interacting with Rundeck</li>
<li><a href="https://github.com/lusis/go-artifactory">go-artifactory</a>: A handly library and set of utilities for interacting with Artifactory</li>
<li><a href="https://github.com/lusis/logrus_sumologic_hook">logrus_sumologic_hook</a>: Adds support to <a href="https://github.com/Sirupsen/logrus">logrus</a> for sending events to SumoLogic</li>
<li>terraform: I have a few outstanding pull requests to Terraform for adding Artifactory integration and some fixes to the CloudFlare provider</li>
</ul>


<h1>Toys</h1>

<p>I recently purchased a couple of <a href="https://www.yubico.com/products/yubikey-hardware/yubikey-neo/">Yubikey NEO</a> units. What made it worth at this point was the NFC support. At this point I&rsquo;ve got my GPG key stored on there and I&rsquo;m using it as my MFA for LastPass.</p>

<p>Related to the MFA stuff, I&rsquo;ve pretty much switched off Google Authenticator for <a href="https://www.authenticatorplus.com/">Authenticator Plus</a>. What I really like about this is that I can &ldquo;share&rdquo; my MFA across devices since the data store is stored externally. It&rsquo;s essentially the 1Password approach for MFA. Considering I&rsquo;ve had to replace my phone at least 4 or 5 times since I started using it for MFA (kids, ground, boom), this is going to be much less painful when the next &ldquo;whoops&rdquo; comes along.</p>

<p>Before I left the previous employer I took advantage of the employee discount and purchased an <a href="http://www.dell.com/learn/us/en/555/campaigns/xps-linux-laptop">M3800</a> as my daily driver. I&rsquo;ve not had my own laptop in a very long time and this thing has been awesome. I really would have rather had the xps13 but the memory (max 8G) was simply not enough. I used one for the entire time I was at the previous employer and it was constantly a pain point for me.</p>

<h1>Blog itself</h1>

<p>I&rsquo;m hoping to write a few more blog posts coming up. I finally got my octopress setup working again and I&rsquo;m in the process of migrating it to Hugo so I don&rsquo;t have to fight Ruby in the future just to write blog posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Few Things]]></title>
    <link href="http://lusis.github.com/blog/2014/11/21/a-few-things/"/>
    <updated>2014-11-21T11:20:00-05:00</updated>
    <id>http://lusis.github.com/blog/2014/11/21/a-few-things</id>
    <content type="html"><![CDATA[<p>I can&rsquo;t believe I have to even write this</p>

<!-- more -->


<p>but it seems that comprehension is hard and sadly I made the mistake of reading comments when someone posted my systemd redux article on HN.</p>

<h2>CoreOS</h2>

<p>Yes I know CoreOS uses systemd. It seems that ONE comment got taken entirely out of context. If you take it WITH the context of the whole post:</p>

<ul>
<li>If you must use Linux</li>
<li>And we&rsquo;ve agreed that systemd is here to stay in Linux</li>
<li>Use a Linux &ldquo;distro&rdquo; where you don&rsquo;t care that it&rsquo;s running systemd</li>
</ul>


<p>CoreOS is not a traditional general purpose Linux. CoreOS is a firmware for running containers. A bespoke platform for explicitly running containers. You don&rsquo;t manage CoreOS like a traditional distro. You don&rsquo;t use CoreOS like a traditional distro.</p>

<p>And yes I said CoreOS wasn&rsquo;t production ready yet. But it will get there. One of the people I respect most in this industry (and used to work with), Kelsey Hightower , works for CoreOS and has an operations background. If anyone can help address the needs of production usage of a platform, it&rsquo;s Kelsey.</p>

<h2>SystemD/SysV</h2>

<p>As a project, I dislike SystemD. In the specific, there&rsquo;s a shitload of things SystemD gets right. I like unit files. I like running everything contained. I love the security features going in to restrict visibility to processes.</p>

<p>I dislike the SCOPE and I dislike the stuff that extends beyond the init replacement. I dislike the reinvention of things and in the process of bringing in bugs that have been solved for a while.</p>

<p>I also dislike the lockstep systemd/kernel upgrade that&rsquo;s coming.</p>

<h2>Gnome requiring systemd</h2>

<p>You&rsquo;re right, gnome doesn&rsquo;t explicitly require systemd. It requires something that&rsquo;s only provided by systemd currently.</p>

<h2>I&rsquo;m not afraid of change</h2>

<p>I think anyone who&rsquo;s familiar with me and this blog knows that to not be the case. As Chris Webber said earlier, there&rsquo;s a difference between bleeding edge and leading edge.</p>

<h2>Solaris/OmniOS/SmartOS</h2>

<p>I&rsquo;m perfectly aware of the differences between these things. I guess forgetting to add the word &ldquo;derivative&rdquo; after the word &ldquo;Solaris&rdquo; makes me a total idiot.</p>

<h2>I don&rsquo;t &ldquo;hate&rdquo; Poettering or think he&rsquo;s &ldquo;evil&rdquo;</h2>

<p>Really? This is software we&rsquo;re talking about. Just&hellip;wow</p>

<h2>FreeBSD</h2>

<p>No jails are not a 100% replacement for LXC. It is, however, a more mature technology for isolation.</p>

<h2>You&rsquo;re right, I&rsquo;m not a developer</h2>

<p><a href="https://github.com/lusis">I&rsquo;ve never written a line of code in my life.</a></p>

<p>There&rsquo;s probably a whole bunch of other aspersions cast my way in the comments on HN and frankly I can&rsquo;t be bothered to read them. I&rsquo;m actually busy managing a production SaaS and writing code in two different languages at the moment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Systemd-redux]]></title>
    <link href="http://lusis.github.com/blog/2014/11/20/systemd-redux/"/>
    <updated>2014-11-20T00:21:00-05:00</updated>
    <id>http://lusis.github.com/blog/2014/11/20/systemd-redux</id>
    <content type="html"><![CDATA[<p>I figured it was about time for a followup on my systemd post. I&rsquo;ve been meaning to do it for a while but time hasn&rsquo;t allowed.</p>

<!-- more -->


<h1>The end of Linux</h1>

<p>Some people wrongly characterized this as some sort of hyperbole. It was not. Systemd <em>IS</em> changing what we know as Linux today.
It remains to be seen if this is a good or bad thing but Linux is becoming something different than it was.</p>

<h2>Linux is in for a rough few years</h2>

<p>I do honestly believe this will end up being the start of a rocky period for Linux.</p>

<ul>
<li>Lennart has already said that the expectation is that SystemD and the kernel will be upgraded in lockstep</li>
<li>SystemD is consuming more and more of what is currently userspace</li>
<li>SystemD is reinventing existing software stacks from scratch. See the recent systemd-resolvd cache poisoning issue and the journald transaction issues.</li>
</ul>


<p>Additionally, while not Systemd specific but legitimately all inter-related, kdbus is coming and its already got its <a href="https://lkml.org/lkml/2014/10/29/854">fair share of issues in the first implementation</a> including breaking userspace.</p>

<p>We also have distros like SLES adopting btrfs as the default filesystem.</p>

<p>All of these things combined mean that Linux is pushing the bleeding edge of a lot of unbaked technologies. Time will tell if this turns people off or not. I expect that enterprise shops will probably freeze systems at RHEL6 for a good while to come (and not just the standard &ldquo;we&rsquo;re enterprise and we don&rsquo;t like to upgrade&rdquo; time period).</p>

<h2>Systemd isn&rsquo;t going away</h2>

<p>Systemd is here to stay. The only way you will have a system without it is to roll your own. I don&rsquo;t expect many distros to chose to back out. My best hope is that they&rsquo;ll all freeze at the current version. Maybe a few things will get backported here and there for security fixes.</p>

<h2>SystemD components are <em>NOT</em> optional</h2>

<p>I know everyone likes to tout this but, no, the various systemd components while not pid 1 are realistically not optional. Kdbus, single parent hierarchy for namespaces (systemd is taking this one of course), udev changes - the kernel and distros are changing and coallescing around whatever systemd ships. Most distros will probably use systemd-networkd for instance. Look at what happened with Debian just today. The (albeit way late to the game) recommendation to support alternate init systems was rejected. I encourage you STRONGLY to read the systemd-devel mailing list for the kinds of issues you&rsquo;ll possibly have to deal with.</p>

<h1>Options</h1>

<p>To be clear if you&rsquo;re going to stick with Linux, you will have to deal with systemd. It&rsquo;s up to you to decide if that&rsquo;s something you&rsquo;re comfortable with. Systemd is bringing some good things but, like other discussions I&rsquo;ve been involved with, you&rsquo;re going to be stuck with all the other stuff that comes along with it whether you like it or not.</p>

<p>It&rsquo;s worth noting that FreeBSD just got a nice donation from the WhatsApp folks. It also ships with ZFS as part of the kernel and has a jails which is a much more baked technology and implementation than LXC. While you can&rsquo;t use docker now with jails, my understanding is that there is work being done to support NON-LXC operating system level virtualization (such as jails and solaris zones).</p>

<p>Speaking of zones and Solaris, if that&rsquo;s an option for you it&rsquo;s probably the best of breed stack right now. Rich mature OS-level virtualization. SmartOS brings along KVM support for when you HAVE to run Linux but backed by Solaris tech under the hood. There&rsquo;s also OmniOS as a variant as well.</p>

<p>If you absolutely MUST run Linux, my recommendation is to minimize the interaction with the base distro as much as possible. CoreOS (when it&rsquo;s finally baked and production ready) can bring you an LXC based ecosystem. If they were to ever add actual virt support (i.e. KVM), then you could mix and match as needed. If you&rsquo;re working for a startup or a more flexible organization, you can go down this path. If you&rsquo;re working for a more traditional enterprise, your options are pretty limited. At least you&rsquo;ll have the RedHat support contract.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Software Empathy]]></title>
    <link href="http://lusis.github.com/blog/2014/10/19/software-empathy/"/>
    <updated>2014-10-19T22:44:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/10/19/software-empathy</id>
    <content type="html"><![CDATA[<p>Over the past several months, I&rsquo;ve written a lot of blog posts that were critical of various software. If it wasn&rsquo;t clear from these (sometimes) incoherent ramblings, there was a common thread that bound them all together</p>

<!-- more -->


<h2>Why do we write software?</h2>

<p>I write software every now and then. When I do, if possible, I put it up on github for folks to consume. I&rsquo;ve talked about this before. For me, being able to write code and somehow that code helps someone blows my mind.
I haven&rsquo;t always been able to write code (or at least I always told myself I couldn&rsquo;t). I&rsquo;ve never been paid for that task. It&rsquo;s not the career path I chose all those years ago. I like operations. I like managing systems. Most of the code I write is for that purpose in some roundabout way.
Other people write code because they&rsquo;re paid to write code. Sometimes it gets released as open source. Sometimes it doesn&rsquo;t. There are no value judgements in that. It&rsquo;s just a thing that is.</p>

<p>Why do we create anything at all? Ego, altruism, enlightened self-interest - all are justifications given at some point.</p>

<p>Regardless, when it comes to software, we create something that doesn&rsquo;t just exist but exists to be consumed - either personally or professionally - and in something of an interesting twist this thing we create is consumed repeatedly even if we never realized it would be.</p>

<h2>On users</h2>

<p>When you release software, regardless of the motivation, you always have a user. That user can just be you (the creator), it can be customers, it can be a minority community or, if you&rsquo;re really lucky, it can be &ldquo;pretty much everybody&rdquo;.</p>

<p>The best (or worst) part of it is that once it&rsquo;s out there, you have no control over who that user is. Sure you can gate the userbase (think: customers or internal projects) but if you release something as &ldquo;open source&rdquo;, you&rsquo;ve just given up any control over who uses that software.</p>

<p>Yes, you have users and now you have to act accordingly</p>

<h2>Software as an expression of empathy</h2>

<p>We&rsquo;ve all heard of Conway&rsquo;s Law:</p>

<blockquote><p>organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations</p><footer><strong>Melvin Conway</strong> <cite><a href='http://en.wikipedia.org/wiki/Conway%27s_law'>en.wikipedia.org/wiki/&hellip;</a></cite></footer></blockquote>


<p>In that same vein I would argue that the software people create and its lifecycle - how they support it, how it behaves, how it interacts with other software - is a reflection of the empathy of the creator(s) for the user.</p>

<p>I want to use a couple of examples here. Of course these are only a few and they so obviously support my theory that I must be right&hellip;..</p>

<p>Seriously though.</p>

<h3>Logstash</h3>

<p>Logstash is one of those projects that &ldquo;everyone uses&rdquo;. Of course not everyone uses it but really, EVERYONE uses it. Logstash has a motto:</p>

<blockquote><p>Remember: if a new user has a bad time, it's a bug in logstash.</p><footer><strong>Jordan Sissel</strong></footer></blockquote>


<p>If any of you have had the wonderful privilege of knowing Jordan in person, you would understand that this is a DIRECT reflection of him as a person. He has empathy for the users of his software. These days, Jordan is paid to work on logstash and there&rsquo;s a commercial drive to it but this quote has been there well before that.</p>

<p>I&rsquo;m going to come back to this quote in a few.</p>

<h3>Systemd/PulseAudio</h3>

<p>These are a few project from Lennart Poettering. Systemd itself has its own &ldquo;cabal&rdquo; (self-titled). I&rsquo;m not going to get into why I dislike systemd again except to say, I address the empathy issue there as well. Note that I&rsquo;m not calling out Lennart specifically here, though there is history in bug reports and mailing lists from not just him but others in the &ldquo;cabal&rdquo;.</p>

<p>In contrast to logstash, bugs in PulseAudio or problems with systemd are never the fault of the creator but always the user or the user&rsquo;s hardware or the user&rsquo;s inability to move on. I was even told that I just wanted people to &ldquo;get off my lawn&rdquo; because I questioned systemd.</p>

<p>The point is, PulseAudio and systemd as projects have a distinct LACK of empathy from either the original authors or the proponents of it.</p>

<h3>Linux kernel</h3>

<blockquote><p>Mauro, SHUT THE FUCK UP!</p><footer><strong>Linus Torvalds</strong> <cite><a href='https://lkml.org/lkml/2012/12/23/75'>lkml.org/lkml/2012/12/23/75/&hellip;</a></cite></footer></blockquote>


<p>Looking at this you might say &ldquo;Wow Torvalds is a dick. He has no empathy&rdquo;. And if you took his handling of kernel issues and maintainers as is, you&rsquo;d probably be right. In fact, while I&rsquo;ve never met the guy, let&rsquo;s just say &ldquo;Sure he&rsquo;s a dick&rdquo;.</p>

<p>However further down in that same post he says this:</p>

<blockquote><p>WE DO NOT BREAK USERSPACE!</p><footer><strong>Linus Torvalds</strong></footer></blockquote>


<p>In an interesting crossover, you can even read his interactions with Kay Sievers (one of the core developers of systemd) that is a reflection of the same quote:</p>

<blockquote><p>Key, I’m f\*cking tired of the fact that you don’t fix problems in the code *you* write, so that the kernel then has to work around the problems you cause.<br/>.....<br/>But I’m not willing to merge something where the maintainer is known to not care about bugs and regressions and then forces people in other projects to fix their project. Because I am *not* willing to take patches from people who don’t clean up after their problems, and don’t admit that it’s their problem to fix.</p><footer><strong>Linus Torvalds</strong></footer></blockquote>


<h1>How is empathy formed?</h1>

<p>Since we&rsquo;ve not yet defined empathy yet, let&rsquo;s do that:</p>

<p><a href="https://www.google.com/search?client=ubuntu&amp;channel=fs&amp;q=empathy&amp;ie=utf-8&amp;oe=utf-8">the ability to understand and share the feelings of another</a></p>

<p>Google helpfully tells us that the root of the word is from two Greek words:</p>

<ul>
<li>em: &ldquo;in&rdquo;</li>
<li>pathos: &ldquo;feeling&rdquo;</li>
</ul>


<p>Pathos is another interesting word. As technical people, we like to think that we&rsquo;re rational actors and that, by extension, all software and technical issues have a simple rational explaination. This plays to one interpretation of pathos (compared to logos) when we talk about our software.</p>

<p>Obviously if the software doesn&rsquo;t work for the user, there&rsquo;s a reason. Logos for engineers is almost the equivalent of &ldquo;works for me&rdquo; and if I can just explain that to the user, they&rsquo;ll understand. There&rsquo;s no room for emotion here, just logic.</p>

<h2>Empathy as it applies to software</h2>

<p>When we go back to the previous examples, we ask &ldquo;who is the target of the empathy if empathy exists at all?</p>

<p>In the case of Logstash, the target is clearly the user. If the user has a bad time, it&rsquo;s not the user&rsquo;s fault it&rsquo;s logstash&rsquo;s fault.</p>

<p>In the case of Systemd and PulseAudio, I would say that if there is any empathy at all, it&rsquo;s for the people that have to support the software and not the people that have to use it (yes, I said &ldquo;have to&rdquo;).</p>

<p>In the case of the Linux kernel, however coarse, the empathy is for the user but ALSO for the creators and supporters of the software. Just not for one specific creator in this case. You don&rsquo;t break the user experience. You also don&rsquo;t put the people who have to maintain your code in a bind by breaking things.</p>

<h2>The user is not always right</h2>

<p>I said this once on twitter and I want to say it again:</p>

<p><em><em>You don&rsquo;t have to accept that the user is always right but you have to accept that there is a user.</em></em></p>

<p>I want to go back to what the Logstash &ldquo;motto&rdquo; is. Logstash never says &ldquo;the user is always right&rdquo;. If you talk to Jordan about this, the expanded version of that statement works more like this:</p>

<p>&ldquo;If a user has a "bad time&rdquo;, then Logstash is at fault. It may be a legitimate bug. Maybe logstash isn&rsquo;t the right tool for the job in this case or maybe Logstash simply can&rsquo;t do what the user wants. Regardless, that&rsquo;s not the user&rsquo;s fault. It&rsquo;s the fault of the Logstash documentation or the fault of the community in its interactions with the user."</p>

<p>Software has an interesting facet. It&rsquo;s not uncommon in other areas of &ldquo;creation&rdquo; but it&rsquo;s interesting none the less. I want to use my rants on PaaS software as an example.</p>

<p>The most obvious target of PaaS software is the developer who will use the software. There&rsquo;s nothing wrong with this. When we create, we have a target in mind. The goal of PaaS itself is to make the developer experience as simple and awesome as possible.</p>

<p>This works great for Heroku and hosted options because the provider of the software doesn&rsquo;t have to worry about anyone other than the developers.</p>

<p>But when you create a PaaS product that has to run onpremises, you now have ANOTHER userbase that you never thought about - the people who have to OPERATE the software to provide it to the original target market. You don&rsquo;t get to punt on that. You have to address it. You can&rsquo;t hide the ugly bits from the people who have to operate it.</p>

<p>We&rsquo;re currently dealing with this with our OWN product now. We are investing considerable resources in not just the user experience in the traditional sense but also the user experience of the users who have to operate the software.</p>

<h2>Focusing on ALL the users (including the ones who didn&rsquo;t choose to be users)</h2>

<p>Writing software is hard but it&rsquo;s even harder to maintain software. You simply cannot be a software maintainer and not have empathy for the people who use your software. If you don&rsquo;t clearly manage expectations about the scope of your software, then the fault for a user using it incorrectly pretty much falls on your shoulders. This can be as simple as saying in the README:</p>

<p><em>&ldquo;This software has been tested under X, Y and Z scenarios. It was created to do foo and it was created to do foo under these circumstances. I&rsquo;m unable to test the software on bar (or &lsquo;I have no interest in testing on bar as it&rsquo;s not intended to run on bar&rsquo;). I&rsquo;m willing to accept patches to run this software on bar but this is a personal project and the primary focus is doing baz&rdquo;</em></p>

<p>A little communication goes a long way to managing expectations. I throw random crap up on github all the time but I always try to create a README that states things very clearly as to what state the code is in. I&rsquo;m also always willing to turn something over to someone else to maintain rather than have the user have a bad time.</p>

<p>For the record I suck at this. When people email me about problems with something I wrote, I feel terrible and I feel REALLY terrible when I simply don&rsquo;t have the time to even reply to the email.</p>

<p>And really that&rsquo;s the crux of software empathy. We must not only have empathy for the target user or even the unexpected user. We must also go beyond the experience we want them to have with the software but also the scope of the problems they have.</p>

<p>When people open support tickets for OSS, ask a question on IRC or on a mailing list they may be in a very bad situation. They could be experiencing downtime, loss of data or any number of problems. They are stressed, possibly losing money and may even be just coming to grips with a really bad decision in using your software made by someone who doesn&rsquo;t even work for/with them anymore. At this point they just need to stop the bleeding. Again, you can be firm and simply say &ldquo;I&rsquo;m sorry that you&rsquo;re in this position but unfortunately there&rsquo;s not much we can do&rdquo; but that goes a long way to helping them move on to the next phase of troubleshooting.</p>

<h1>Empathy has a place in software</h1>

<p>Empathy is hard enough outside of software. Human beings are messy. We retreat to our machines where things are (largely) logical. If the there&rsquo;s a problem, either it&rsquo;s a logic problem or a bug. It&rsquo;s all solvable. The end-user is clearly defined on a card on a kanban board and we just have to do some user studies.</p>

<p>Human problems don&rsquo;t always have clear solutions and we can&rsquo;t always predict how our software will be used. We just need to accept that and act accordingly when it does happen.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The End of Linux]]></title>
    <link href="http://lusis.github.com/blog/2014/09/23/end-of-linux/"/>
    <updated>2014-09-23T22:53:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/09/23/end-of-linux</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve done a lot of tweeting about systemd lately. My internal conscience constantly reminds me of John Allspaw saying that twitter is just pretty much perfect for snarky comments (paraphrase).</p>

<!-- more -->


<p>Al Tobey asked me a good question:</p>

<blockquote><p>I honestly want to know why you dislike it so much. You clearly know wtf is going on. I haven't heard a specific technical problem.</p></blockquote>


<p>First mistake is thinking I know wtf is going on. However the question was asked. What &ldquo;technical&rdquo; concerns do I have with systemd?</p>

<p>I don&rsquo;t (sort of). Here are my primary ones:</p>

<ul>
<li>journald</li>
<li>architecture</li>
</ul>


<h2><code>journald</code></h2>

<p>Yes <code>journald</code> can be setup to use syslog but by default, it&rsquo;ll use a binary log format. Sure you can use &ldquo;strings&rdquo; on it but is anyone seriously considering that a proper way to get to your system logs? In fairness, <code>journalctl</code> provides some nice mechanisms for targeting specific message types, sources and scope but at the expense of having to use <code>journalctl</code> as the unified interface. Keep this tradeoff in mind when I get to the &ldquo;real problems&rdquo; section.</p>

<h2>Architecture</h2>

<p>On the architecture of systemd, I have a legitimate concern with the scope. Let&rsquo;s use the image from wikipedia:</p>

<p><img src="http://lusis.github.com/images/posts/systemd/systemd-arch.png"></p>

<p>I previously stated that <code>systemd</code> provided a nice juicy attack surface. There are valid arguments that not all these components are &ldquo;core&rdquo; <code>systemd</code>. Regardless, they are still components and there is an implicit trust relationship with &ldquo;core&rdquo; vs &ldquo;components&rdquo;. Yes <code>systemd</code> sticks everything in <code>cgroups</code> (another minor issue I have) but with the coming Dockerpocalypse didn&rsquo;t everyone learn that cgroups were not a security mechanism (nor are containers for that matter)? I still stand by my statement that the &ldquo;big one&rdquo; linux exploit will somehow be tied to systemd.</p>

<p>But back to that architecture for a minute. There are a lot of things in there, that while possibly optional, are things I have zero need for where systemd affects me the most. Telephony? Graphical sessions? I didn&rsquo;t even know what Tizen is before this post (and I think the modified image on wikipedia came from the Tizen wiki). Maybe it&rsquo;s not required. I can&rsquo;t tell. Keep this in mind also.</p>

<p>Probably the best argument against the architecture of systemd is from one of the primary authors <a href="http://0pointer.de/blog/projects/why.html" title="Why Systemd">here</a>. A list of &ldquo;advantages&rdquo; that includes the SCM system in use or the fact that there are &ldquo;Specialized professional consulting and engineering services available&rdquo; is not a valid technical merit. Finally buried deep in the text near the end, we come to understand the biggest architectural problem of all:</p>

<blockquote><p>systemd is in the process of becoming a comprehensive, integrated and modular platform providing everything needed to bootstrap and maintain an operating system's userspace.</p></blockquote>


<p>I also personally think that the <a href="http://0pointer.de/blog/projects/systemd.html"><code>systemd</code> design motivations</a> are &ldquo;flawed&rdquo; at the core:</p>

<blockquote><p>Now, if that's all they are waiting for, if we manage to make those sockets available for connection earlier and only actually wait for that instead of the full daemon start-up, then we can speed up the entire boot and start more processes in parallel. So, how can we do that? Actually quite easily in Unix-like systems: we can create the listening sockets before we actually start the daemon, and then just pass the socket during exec() to it. That way, we can create all sockets for all daemons in one step in the init system, and then in a second step run all daemons at once. If a service needs another, and it is not fully started up, that's completely OK: what will happen is that the connection is queued in the providing service and the client will potentially block on that single request.</p></blockquote>


<p>and</p>

<blockquote><p>Because this is at the core of what is following, let me say this again, with different words and by example: if you start syslog and and various syslog clients at the same time, what will happen in the scheme pointed out above is that the messages of the clients will be added to the /dev/log socket buffer. As long as that buffer doesn't run full, the clients will not have to wait in any way and can immediately proceed with their start-up. As soon as syslog itself finished start-up, it will dequeue all messages and process them. Another example: we start D-Bus and several clients at the same time. If a synchronous bus request is sent and hence a reply expected, what will happen is that the client will have to block, however only that one client and only until D-Bus managed to catch up and process it.</p></blockquote>


<p>I&rsquo;m going to go on record and say that this is quite possibly the worst idea to anyone running a server. The acceptable use cases for this are so narrow that it&rsquo;s hardly justification for everything that followed.</p>

<h1>The real problems</h1>

<p>I know my &ldquo;technical&rdquo; arguments are flimsy. The fact is there some really cool shit in <code>systemd</code> including many of the things listed in the linked post.</p>

<p>The problem with <code>systemd</code> is that it is the single most invasive change to Linux in a long line of changes that ultimately mean that Linux may be headed towards uselessness as a server operating system.</p>

<h2>The invasion of &ldquo;desktop linux&rdquo;</h2>

<p>I&rsquo;m going to state up front (and people are free to disagree with me) that I believe you cannot provide a distribution of Linux that is both designed for the &ldquo;server&rdquo; and the &ldquo;desktop&rdquo; and provide a product that is worth using on either.</p>

<p>We&rsquo;ve see this happening with regularity in other places such as <code>d-bus</code>. Again, these things aren&rsquo;t neccessarily BAD things (and kdbus will enable some REALLY cool shit) but at what cost? I think motivation matters considerably here.</p>

<p>Understand that I exclusively run &ldquo;linux on the desktop&rdquo; and I have for a VERY long time. I have a vested interest in Linux not sucking on the desktop. However I have a GREATER interest in Linux on the server not sucking. My linux desktop doesn&rsquo;t send me pager alerts at 3AM when pulseaudio shits the bed because of some USB interrupt issue with my headset. Pagerduty will, however, call me on the phone and wake up my sleeping partner when there&rsquo;s a kernel panic.</p>

<p>In fact, I&rsquo;ll go even further and say that ANY kernel or distro related change that was driven by &ldquo;the linux desktop&rdquo; is suspect to me.</p>

<p>The problem is I don&rsquo;t have that luxury. I have competing responsiblities. I have to provide a system that runs reliably and can easily be reasoned about and yet I have to build it on distributions created by people who consider how long it takes to get to the fucking GDM login screen and if shutting the laptop lid will cause the system to hibernate properly or not.</p>

<p>I realize that there is overlap in these cases. A power efficient operating system has benefit to me sure but it&rsquo;s not my primary concern.</p>

<h2>Maturity</h2>

<p>This could be classified as technical but it&rsquo;s not just about the project itself. Systemd <em>IS</em> an immature system. Wikipedia puts the initial release as 3/30/2010. Lennart&rsquo;s &ldquo;announcement&rdquo; has a date of 04/30/2010. Let&rsquo;s call it four years among friends.</p>

<p>We have a system that has gone from a blog post to being the &ldquo;comprehensive, integrated and modular platform providing everything needed to bootstrap and maintain an operating system&rsquo;s userspace&rdquo;.</p>

<p>I don&rsquo;t think so.</p>

<p>Let&rsquo;s also not forget that systemd uptake was LARGELY restricted to Fedora up until the point that the Gnome team decided that <code>logind</code> would be a future requirement. I want that to sink in VERY clearly.</p>

<p>Systemd did not get to the place it was in UNTIL it became, by proxy, a requirement for GNOME. What did that give us?</p>

<ul>
<li>Ubuntu 14.04 (the LTS release) is running on a hacked fork of <code>logind</code>.</li>
<li>RHEL/CentOS7 (again LTS releases) are running v208 of systemd that was tagged in October of last year</li>
</ul>


<p>Now I&rsquo;m the first person to complain about distros keeping way old versions of stuff around but this is ridiculous. You cannot tell me that is considered &ldquo;baked&rdquo; by any stretch of the imagination.</p>

<h2>History</h2>

<p>I can&rsquo;t say much here except that my experience with previous projects from Poettering (pusleaudio and avahi) give me very little faith in systemd.</p>

<p>Is that a fair assessment? I think it has relevance. There&rsquo;s a question of what the driving force is behind someone&rsquo;s logic. There&rsquo;s a question of previous quality. Does the person go after the new shiny and abandon some previous project?</p>

<p>The only upshot, I guess, is that distros have a vested interest now as there is no avoiding systemd. This is likely a change that will never get rolled back. We can only hope that the Linux certain people want is the Linux that meets our needs as system administrators.</p>

<h2>Compatibility</h2>

<p>This issue is probably irrelevant to most people but it bothers me greatly.</p>

<p>With the systemd adoption comes the first steps to more applications being &ldquo;Linux&rdquo; only. When the creator of systemd says that we should ignore POSIX compatability and systemd itself relies on Linux-only features like cgroups there&rsquo;s really little hope left. GNOME requiring logind means realistically that GNOME will only ever run on Linux. Will something step up and take its place? Maybe but it will likely see zero adoption and be a niche player in the overall scheme.</p>

<p>Linux is becoming the thing that we adopted Linux to get away from.</p>

<h1>Wrap up</h1>

<p>I am fully willing to concede that systemd is going to enable some really cool stuff. CoreOS has adopted nspawn. Unit files can provide a straight up dead simple mechanism for applications to daemonize themselves. Finally you can say &ldquo;just daemonize this command&rdquo; for me. Cgroups integration is really quite awesome as well. I&rsquo;m totally for Linux adopting some of these awesome new things.</p>

<p>The problems I have are the tradeoffs. This is very similar to a previous discussion I was involved in regarding another tool. In this case you came for cgroups and faster boot times(*) and you got stuck with:</p>

<ul>
<li>binary logging formats</li>
<li>an http server in your init system (oh sure you can use unix domain sockets but have fun with Java talking to those)</li>
<li>QR codes</li>
</ul>


<p>Oddly enough, the thing that is giving more uniformity to Linux is making it less &ldquo;Linux&rdquo; to me.</p>

<h2>Comments</h2>

<p>I am, as always, willing to be corrected in places where I got things wrong. Please be aware that I check in for comments on posts and don&rsquo;t get notified on new ones. If you feel like the discussion doesn&rsquo;t cleanly fit in a comment, feel free to post a gist and I&rsquo;ll respond there and link here.</p>

<p><em>(*) The fastest part of booting my servers is the init. It&rsquo;s the 7 minutes of POST+device enumeration that takes more time. My instances in CloudStack are nearly instant boot as well.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Berks]]></title>
    <link href="http://lusis.github.com/blog/2014/07/15/berks/"/>
    <updated>2014-07-15T22:26:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/07/15/berks</id>
    <content type="html"><![CDATA[<p>Note this post has comments disabled. This is a first for me but if someone wants to ask me anything else, there are multiple personal ways to contact me.</p>

<!-- more -->


<p>Recently I posted a message to the <code>chef-users</code> mailing list. It was a hard post for me to write because I was attempting to be as tactful as possible about the issue. While I think it was the right move and some good came out of it, I now feel like I have to defend myself for some reason.</p>

<p>My concern over Berkshelf came from one single place, what is the implication to Chef (the tool) itself where everything in the ecosystem adopts Berkshelf for dependency solving.</p>

<p>Why do I care? Because EVERY SINGLE EXPERIENCE I&rsquo;ve had with Berkshelf has been crappy. I&rsquo;m never quiet about crappy software. Ask anyone I work with. I&rsquo;m just as matter of fact about it in our own software as well as software I&rsquo;ve had to use.</p>

<p>Understand that my frustration with Berkshelf came out of using it with omnibus early on and the resultant breakage from either vagrant-berkshelf or omnibus. Every attempt I&rsquo;ve made to use Berkshelf internally has ended badly including restoring my local checkout from an internal repo.</p>

<p>You can ask anyone on our operations team but I have said if our team decides we want to use Berkshelf then we&rsquo;ll use it. Multiple attempts have been tried and everyone has come away with a bad experience.</p>

<p>Why is that? Why is something as simple as a dependency solver so painful? Julian Dunn said it best:</p>

<blockquote><p>We grabbed Berkshelf, the first thing that was convenient & met the need. But we also got a lot of things that are kind of superfluous to the goal. The "Berkshelf Way", "Environment Cookbooks", and friends all have little to do with that, and will not be used by most people</p></blockquote>


<p>I hope this makes my concerns about where Berkshelf itself is going in Chef much more clear. No one is arguing that depsolving isn&rsquo;t a problem. The issue is that along with that comes a set of entirely unrelated things. If the entire Chef ecosystem is moving to Berkshelf (the depsolver) plus Berkshelf (the everything else) then, yes, that lessens the value of the ecosystem to me. I stand by my statement that Berkshelf is impossible to avoid these days because it <em>IS</em> integrated into every part of the ecosystem regardless if it&rsquo;s actually a requirement of chef itself.</p>

<p>Understand I am not paid to work on Chef, Chef ecosystem tools or anything else. I am paid to keep our applications and servers running. To do this, we leverage tools like Artifactory Pro, Chef, Jenkins, Rundeck, Omnibus, Vagrant, Nginx and countless others. If any of those tools start causing an impediment to our ability to ship then we have an obligation to revisit if that tool is the right fit. Maybe we&rsquo;ve outgrown it. Maybe the tool (or its ecosystem) has taken a different approach. In the end it doesn&rsquo;t matter. If the tool isn&rsquo;t working, we either have to deal with finding a replacement or fight it until we can find (or create) a replacement. Every tool choice we make has implications on everything from availability to business agility to how often people get woken up at night.</p>

<p>I&rsquo;m truly sorry if people took these criticisms personally. I realize the discussion got heated and others were a bit more stern in the criticism they used. I never asked anyone to &ldquo;come to my defense&rdquo;. I also have never pushed my view on anyone else and told them they shouldn&rsquo;t use Berkshelf. I&rsquo;ve been blunt about my experiences and pain points but never said either &ldquo;You shouldn&rsquo;t use Berkshelf or I don&rsquo;t think you should use Berkshelf&rdquo;.</p>

<p>The people working on these tools are nice people (at least the ones I&rsquo;ve met in person). I realize how hard it is to separate the opinion of something you&rsquo;ve created or maintain from yourself. Your effort in maintaining opensource software is truly appreciated.</p>

<p>The last thing I want to say in my defense is this. If you think that any of this over the past few days is somehow my fault or that I started all this, that&rsquo;s simply wrong. I addressed this in the mailing list post. If you want to be mad at ME that&rsquo;s fine but consider this. You don&rsquo;t always get negative feedback from your users or your customers. At best you get a small chance to salvage the relationship from a public complaint but more often that not, people just go away mad.</p>

<p>The problem is that this carries over in to discussions you never hear about. Joe asks Sally her opinion on X and Sally recounts her personal experience. This shapes Joe&rsquo;s perception of X.</p>

<p>If anyone would like to contact me about this, I will be glad to talk about it as time permits but I&rsquo;m this is last I plan on talking about it in public.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Feedback on 'PaaS Realism' Post]]></title>
    <link href="http://lusis.github.com/blog/2014/06/22/feedback-on-paas-realism/"/>
    <updated>2014-06-22T22:22:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/06/22/feedback-on-paas-realism</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve gotten quite a bit of constructive feedback on the PaaS realism post and I wanted to aggregate it here.</p>

<!-- more -->


<h1>OpenShift</h1>

<p>I was humbled to see a very awesome post from <a href="http://sosiouxme.wordpress.com/2014/06/19/response-to-paas-for-realists-re-openshift/">Luke Meyer who works on the OpenShift team</a>.  I think if you are going to deploy OpenShift, his post is very important to read.</p>

<p>I wanted to respond to a few points specifically he made (and again say thank you for taking the time to respond/educate me)</p>

<h2>State and Sticky Sessions</h2>

<p>Luke is absoltely correct that statefulness and stateful requirements are not a limitation of a PaaS. I&rsquo;ll address this a bit at the bottom a bit more clearly but I would say the issue is <strong>AMPLIFIED</strong> by a PaaS.</p>

<h2>Databases on OpenShift</h2>

<p>Luke points out that database cartridges on OpenShift are not redundant. I was informed that there <a href="http://crunchydatasolutions.com/crunchy-latest/crunchy-announces-high-availability-postgresql-for-openshift-enterprise/"><em>IS</em> an HA cartridge of PGSQL available</a> though it appears to be a commercial offering. The marketing fluff doesn&rsquo;t go into any useful information to clarify how that works (and I&rsquo;ve not yet dug in to see if there&rsquo;s freely available tech information).</p>

<p>And frankly, similarly to the state issue, database usage is pretty app specific. One thing I take issue with is that if you do end up using an external DB, have you really bought yourself that much? Yes your application has a common &lsquo;environment&rsquo; to it now but there&rsquo;s still this non-PaaS component that someone has to use. Unless you&rsquo;re setting up an external development database you now have a tangible difference between production and development that you have to account for. Again, this is not <strong>SPECIFIC</strong> to a PaaS but it is a variable that I think gets glossed over.</p>

<h2>HA Storage</h2>

<p>I think this came across incorrectly in my original post. If your application requires persistant storage, you&rsquo;ve already hit the first strike in my &ldquo;Is your app ready&rdquo; list. If you are using a platform as a service and you require some form of persistent storage, you should go back to the drawing board.</p>

<h2>DynDNS</h2>

<p>I can&rsquo;t go into details but I can tell you this is definitely an issue. A predictable enterprise IT security bullshit issue but an issue none the less. As in a complete non-starter issue.</p>

<h2>MongoDB</h2>

<p>I&rsquo;m going to hold off on this until the end because this ties into an overarching concern/commentary I have.</p>

<p>I think everyone should read the rest of Luke&rsquo;s post that I&rsquo;m not responding to specifically. It&rsquo;s a very well thought out post and he makes very excellent points. In fact at the end he does ask a very legitimate question:</p>

<blockquote><p>So if you think PaaS is right for you, also ask yourself: do you want to be in the building-a-PaaS business or the using-a-PaaS business?</p></blockquote>


<h1>CloudFoundry</h1>

<p>I didn&rsquo;t get much feedback on CF as I&rsquo;d liked but in fairness I didn&rsquo;t give CF the same attention as I did OpenShift. I did, however, get quite a bit of good information from Twitter from both Dave McCrory and James Watters as well as someone I&rsquo;m not comfortable naming considering it was not public communication.</p>

<p>Before I address the CF specifics, however, I think the most critical deciding factor in why <em>NOT</em> to choose CF right now is that it&rsquo;s undergoing a rewrite. I don&rsquo;t know what the upgrade path for that rewrite is. My guess is it&rsquo;s a parallel deploy and then migrate considering the scope of the change but someone PLEASE correct me if I&rsquo;m wrong on that.</p>

<p>Based on what I&rsquo;ve seen, the rewrite <strong>ACTUALLY</strong> sounds like it will make CF a solid choice. The problem here is that standing up a system like CF (or OpenShift or even OpenStack) are not things that should be taken lightly. You are going to be putting your PRODUCTION workloads on these systems. If you stand up CF now and then come back in three to six months telling everyone you have to transition to a new setup (regardless of the migration process), you risk burning a lot of personal capital.</p>

<p>Now on to the feedback</p>

<h2>CF doesn&rsquo;t use containers</h2>

<p>I incorrectly stated that CF used LXC and according to both Dave and James, that was incorrect. I can&rsquo;t find the link that made me think that (poor form on my part). This is currently controlled by Warden (as I grok it).</p>

<h2>CF and application session state</h2>

<p>James pointed me to an interesting blog post about how <a href="http://blog.gopivotal.com/cloud-foundry-pivotal/products/scaling-real-time-apps-on-cloud-foundry-using-node-js-and-redis">CF handles session state</a>. One thing that&rsquo;s interesting about this post is that it honestly isn&rsquo;t even CF specific. However it still doesn&rsquo;t address my concern about databases on a PaaS.</p>

<p>So the demo application is using redis as a session store. Great! We&rsquo;re going to ignore the work that Kyle Kingsbury did on <a href="http://aphyr.com/posts/283-call-me-maybe-redis">Redis with Jepsen</a> for a minute.</p>

<p>What this article <strong>DOESN&rsquo;T</strong> cover is how might I deploy a redis &ldquo;cluster&rdquo; to CF:</p>

<blockquote><p>MemoryStore is simply a Javascript object – it will be in memory as long as the server is up. If the server goes down, all the session information of all users will be lost!</p><p>We need a place to store this outside of our server, but it should also be very fast to retrieve. That’s where Redis as a session store come in.</p></blockquote>


<p>The post goes into terribly awesome detail about how to make your app handle the reconnection while conviently leaving out how one might deal with the loss of the redis instance itself. I mean let&rsquo;s be fair. This post basically just told you how to write reconnect logic. It gives you all the <strong>APP</strong> bits but nothing about the <strong>INFRASTRUCTURE</strong> bits. I&rsquo;ll address this at the end but it&rsquo;s just more of the kind of thing that is driving me crazy about the magical PaaS landscape.</p>

<h2>Go rewrite is about features not bugs</h2>

<p>I&rsquo;m going to disagree with James here. <a href="https://www.youtube.com/watch?v=1OkmVTFhfLY">This video</a> was recently posted from the CloudFoundry conference. While Onsi (who is now my new favorite speaker btw), doesn&rsquo;t call these things &ldquo;bugs&rdquo;, I would consider the issues he brings up and the end results to be bugs. Again this is subjective but it does get to my primary area of concern addressed below.</p>

<h2>CF is good for enterprises</h2>

<p>James pointed me to <a href="http://scn.sap.com/community/cloud/blog/2014/06/22/cloud-foundry-hybris-and-the-hana-cloud-platform-microservices-and-beyond">this blog post</a> about SAP building their nexgen PaaS on CF. While this is interesting and exciting (especially coming from a legacy like SAP) it means very little to me.</p>

<h1>My perspective</h1>

<p>I said this <strong>SEVERAL</strong> times and reiterated it on Twitter but I want to clarify it one more time:</p>

<p>My questions and concerns, while very much focused on the developer experience, are <strong>MORE</strong> focused on the <strong>OPERATIONAL</strong> experience of running any of these products. Yes, there may be &ldquo;hundreds of thousands of production apps&rdquo; running on CloudFoundry (to quote James) but I want to know what the <strong>OPERATIONAL</strong> burden is to running those.</p>

<p>These are the things I care about (outside of the developer experience):</p>

<ul>
<li>What are the failure points?</li>
<li>What is the impact of each failure point?</li>
<li>What are the SINGLE points of failure?</li>
<li>What is my recovery pattern?</li>
<li>What is my upgrade experience?</li>
<li>What is the operational overhead in the applications running ON the product?</li>
<li>What is my DR strategy?</li>
<li>What is my HA strategy?</li>
</ul>


<p>and this is just the SHORT list.</p>

<p>When Luke tells me the operations team running the hosted version of OpenShift says</p>

<blockquote><p>“We had a couple of outages early on that were traced back to mongo bugs but generally we don’t even think about it.  Mongo just keeps ticking and that’s fantastic.</p></blockquote>


<p>That&rsquo;s great but doesn&rsquo;t mean <strong>ANYTHING</strong> to me. What matters to me is MY operational experience with MongoDB. Here I have one data point as a &lsquo;positive&rsquo; and in my own experience and the documented experience of others AS well as the actual math done by <a href="http://aphyr.com/posts/284-call-me-maybe-mongodb">Kyle Kingsbury with Jepsen</a>, MongoDB is not suitable for any production workload I wish to manage.</p>

<p>Luke says &ldquo;Was the use of MongoDB your only criticism here?&rdquo; to which I would say &ldquo;It&rsquo;s not the only concern but it is the single LARGEST concern I have with OpenShift and one that immediately rules it out for me&rdquo;.</p>

<p>Operations is hard to back with science in the same way that doing a Jepsen series is. In the end what we have to go in is best worded by my coworker Tim Freeman:</p>

<blockquote><p>I've run so many things like X that I can roughly tell what running X is going to be like...</p></blockquote>


<p>If the message here isn&rsquo;t clear, the experience of running a platform by the authors of the platform means VERY little to me when it comes to deciding if <em>I</em> or my company should run it. There are plenty of tools I use daily that are SaaS that run on MongoDB (that I&rsquo;m aware of and more that I&rsquo;m not). There&rsquo;s a valid argument about &ldquo;should I trust someone who would wilfully choose MongoDB&rdquo; but that&rsquo;s not something I control. If the service does what I need and I haven&rsquo;t been directly affected by MongoDB&rsquo;s issues then I really don&rsquo;t have any room to complain. If <em>YOUR</em> ops team wants to run product X, that&rsquo;s great. My duty and responsibility is to my ops team, my developers and my business units.</p>

<p>Mark Imbriaco brought up a point that I do agree with (and explicitly state in my previous post) in response to me stating this concern:</p>

<blockquote><p>Still disagree, even with that focus. Supporting PaaS is much less complex than dozens of inconsistent app envs.</p></blockquote>


<p>but there is, again, a very subtle distinction here. An unspoken word and a bit of a mismatch in what I&rsquo;m talking about. Let me restate Mark&rsquo;s tweet with that word and clarify what I think he meant and what I definitely meant (Mark please please correct me if I&rsquo;m wrong in your intention here):</p>

<blockquote><p>Still disagree, even with that focus. Supporting PaaS applications is much less complex than dozens of inconsistent app envs.</p></blockquote>


<p>Mark is 1000% right. Supporting a PaaS-hosted application is infinitely less complex than multiple inconsistent ones.</p>

<p>Supporting a <em>PaaS</em> however, is not infinitely less complex. It is <em>MORE</em> complex. Mark did follow on with this:</p>

<blockquote><p>For smaller envs, your advice to rationalize deployment and runtime envs over PaaS is good advice.</p></blockquote>


<p>Now in fairness (and I love Mark to death and respect him immensely) that&rsquo;s a bit of arrogant way to word it. EVERY environment, regardless of size, should deal with foundational issues such as deployment and runtime before adding another layer of complexity. Another point to keep in mind is that Mark has his background with Heroku. That&rsquo;s not a positive or a negative it&rsquo;s just a very important data point. Mark knows how to run a PaaS. He&rsquo;s run arguably the largest public PaaS out there and dealt with all of its failure points.</p>

<p>Also no one has yet to address my concerns about the centralized management of the PaaS itself or the data <em>IN</em> that PaaS. This is something that Heroku is actually a very POOR model for as Heroku does not concern itself with those things (nor does it need to). While Heroku does <a href="https://devcenter.heroku.com/articles/heroku-postgres-ha">offer and document how an individual might handle an HA or DR scenario</a> <del>note that Heroku doesn&rsquo;t centrally backup every single database hosted on its platform.</del> <em>(It was pointed out to me that I misread the Heroku docs. All PGSQL dbs are backed up but there ARE limits related to the db that vary by tier)</em>. Also worth noting that Heroku&rsquo;s own offering is PGSQL and that the addon providers are responsible that comparible service. When you are running your own PaaS you are not only Heroku but also that addon provider.</p>

<p>So to go back to the quote from Luke:</p>

<blockquote><p>So if you think PaaS is right for you, also ask yourself: do you want to be in the building-a-PaaS business, or the using-a-PaaS business?</p></blockquote>


<p>As I said, this is a good question but it&rsquo;s indicative of the problems with all the solutions to date from the Docker based ones to CloudFoundry to OpenShift. It even extends to the IaaS offerings like OpenStack and CloudStack. Every single time the discussion is framed as &ldquo;you should use this thing. It makes [deploying apps|managing apps|creating infrastructure|autoscaling|blah blah blah] so much easier and better.</p>

<p>And guess what? They&rsquo;re all right. These tools <em>DO</em> make these things easier and in many cases better but they ALL (each and every one of them) ignore (and I would argue INTENTIONALLY) the question staring them in the face.</p>

<p>Who the fuck is going to <em>RUN</em> this shit?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PaaS for Realists]]></title>
    <link href="http://lusis.github.com/blog/2014/06/14/paas-for-realists/"/>
    <updated>2014-06-14T23:12:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/06/14/paas-for-realists</id>
    <content type="html"><![CDATA[<p>I realize I was pretty down on PaaS the past couple of days. Lest I send the wrong message, I figure a clarification is in order</p>

<!-- more -->




<div class="docs-note">I've posted an update to this blog post with clarifications from folks <a href="http://blog.lusis.org/blog/2014/06/22/feedback-on-paas-realism/">here</a>. You should read it after this one</div>


<p>Before we start, we should define some things. It&rsquo;s always important to be on the same page:</p>

<ul>
<li>PaaS: Platform as a Service (no distinction between public or private). When talking about public, it&rsquo;s usually Heroku.</li>
<li>Private PaaS: A PaaS run &ldquo;in-house&rdquo;. I&rsquo;m using &ldquo;in-house&rdquo; loosely here. You could be running this on top of AWS for all I care. You&rsquo;re running it yourself. There are a lot of players in this space but the biggest name is CloudFoundry. There&rsquo;s also OpenShift and then the plethora of docker-based ones like Deis and Flynn.</li>
<li>Affinity: Definable placement policies for where applications run. I use this liberally to refer to both affinity and anti-affinity. Basically &ldquo;I want this to run next to this&rdquo; vs. &ldquo;I don&rsquo;t want this to run next to this&rdquo;</li>
<li>Production: Business critical functions that warrant &ldquo;waking someone up at 2AM&rdquo;</li>
<li>container: linux containers. Nothing else.</li>
<li>docker: a specific container packaging format and ecosystem</li>
</ul>


<p>I also want to be clear that I <em>ONLY</em> care about production workloads. The reason I defined production the way I did is because only the end user can define the business criticality of a service or system function. If you consider an idle bench of engineers unable to work because your build farm is down a bad thing, then your build farm is &ldquo;production&rdquo;.</p>

<p>I also want to point out that I inherently belive that a Private PaaS is probably a really good thing for your business. My argument is largely that most enterprises are not ready for it and are not willing to live with the shift it will require.</p>

<h1>Benefits of a PaaS</h1>

<p>Let&rsquo;s go with the good news first. A PaaS (and more importantly a private PaaS) has a lot of benefits.</p>

<ul>
<li>It can simplify deployment models.</li>
<li>It can unify the workflow of development and production deployments.</li>
<li>It can codify a framework by which you develop your applications</li>
<li>Generally speaking, you also get a more consistent operational surface for your applications</li>
<li>Can create a culture and framework for self-service</li>
</ul>


<h2>Simplified deployment and unified workflow</h2>

<p>Most private PaaS solutions tend to follow the Heroku model of a PaaS. In the Heroku model, You follow a normal development workflow using your VCS as a model for deployment:</p>

<ul>
<li>Develop code</li>
<li>Test code</li>
<li>Push code to Heroku remote for testing</li>
</ul>


<p>That&rsquo;s the sum of the deployment. Your deployment to production is no different.</p>

<h2>Codified framework</h2>

<p>With Heroku, the &ldquo;moving bits&rdquo; of your stack are hidden from you. You don&rsquo;t stand up a database in the traditional sense. You tell Heroku to wire up the database to your application as an add-on. Heroku exposes this add-on to your stack via environment variables. You reference those environment variables in your application code instead of hard-coded settings in property or yaml files. Both CloudFoundry and OpenShift follow the same model:</p>

<ul>
<li>Create a service</li>
<li>Bind the service</li>
<li>Update the application to use the service</li>
</ul>


<p>This is really awesome from a development perspective. You simply define the building blocks of your application, tell the platform to expose them to your application and off you go.</p>

<h2>Operational Surface</h2>

<p>From an operations perspective, a private PaaS can create a consistent operational surface area. You spend less time worrying about individual operating systems. Your &ldquo;host&rdquo; nodes are largely a uniform target. Most of the private PaaS products ship with most common services prebuilt and ready to wire up to applications.</p>

<p>I would even argue that a private PaaS even simplifies the security model a bit as the concept of users are less relevant and much of the PaaS tooling is, by nature, provides proxy access to the things the developers need. Both of the dominant private PaaS solutions leverage kernel cgroups for resource management. Cloudfoundry uses LXC for isolation while OpenShift uses SELinux and MCS currently. I believe, however, that OpenShift is migrating to LXC as well.</p>

<h2>Self-service culture</h2>

<p>Once your private PaaS is up and running, your development team is unleashed to deploy whatever and whenever they want. They aren&rsquo;t waiting around for a full-fledged OS to be provisioned that needs further configuration just to be servicable. Developers are free to experiment with arbitrary components (provided they exist in the PaaS service catalog).</p>

<h1>Rainbows and Unicorn Piss</h1>

<p>However not everything is rosy and bright in the land of the private PaaS. There are downsides as well - some cultural and some technical</p>

<h2>Application Readiness</h2>

<p>Unless your private PaaS is a bespoke solution, you WILL have to change your application model. You cannot simply forklift an application into a PaaS. Your application must be designed not only to work in a PaaS environment but also to work in a specific PaaS.</p>

<p>Most traditional &ldquo;enterprise&rdquo; applications are not ready for a PaaS solution. Many will have to be significantly rewritten. The most common model for this is the <a href="http://12factor.net/">12 Factor Application</a>.</p>

<p>I should state up front that I disagree with quite a few bits of the 12 Factor model. It&rsquo;s important to remember that, imho, the 12 Factor model was designed as a business strategy for Heroku. The steps follow exactly what makes an app work best on Heroku, not what is best for an application.</p>

<p>Regardless, as the private PaaS solutions are largely modeled on Heroku you might as well state that 12 Factor is the way you&rsquo;ll need to design your application.</p>

<h2>Magical autoscaling</h2>

<p>As I said in my previous post, this really doesn&rsquo;t exist. Your application has to be DESIGNED to scale this way. As Adrian Cockcroft pointed out in the comments to my previous post Netflix &ldquo;overallocated&rdquo; on the dependency side up front to minimize the need and impact of things like rebalancing data and load balancer scaling. It&rsquo;s also worth noting that Netflix did NOT use a PaaS (though arguably the model for how they used AWS was PaaS-ish).</p>

<p>Most &ldquo;enterprise&rdquo; applications I&rsquo;ve dealt with never scaled cleanly. They needed things like sticky sessions and made assumptions about data access paths. Quite frankly they also were not designed for this level of deployment volatility. I would go even further and say that if you have a release cycle measured in months, don&rsquo;t bother.</p>

<h2>Magical Autorecover</h2>

<p>Just like autoscaling, this is also not what you think it is. Unless your application maintains exactly ZERO state, then you will never see this benefit. Do you write files to critical files to disk in your application? Yep those are gone when you &ldquo;magically autorecover&rdquo;. The autorecovery that was promised you? It redeploys your application. Your state is lost and no you don&rsquo;t have NFS or shared storage or anything to fall back to. Get used to shoving your blobs in your database. Oh but what if your database fails?</p>

<p>This is where it gets interesting. I&rsquo;m still sussing out the recovery models for the two primary players in this space but most likely you will LOSE that data and have to restore from a backup. I&rsquo;m sure someone will call me on this and I&rsquo;m willing to listen but I do know for a fact that the autofailover model of things like your MySQL instance depend on migratable or shared storage (at least from my reading of the docs).</p>

<p>This all of course leads me to the next part</p>

<h1>Technical Requirements</h1>

<p>I alluded to this earlier but there are technical requirements that most companies are simply not ready for.</p>

<h2>Distributed Systems</h2>

<p>All applications are inherently distributed systems even if you don&rsquo;t want to admit it. However a PaaS is more so than most shops are ready for. Let&rsquo;s run down the components for the current version of <a href="http://docs.cloudfoundry.org/concepts/architecture/">CloudFoundry</a>. I count 11 distinct components. If we move over to <a href="http://openshift.github.io/documentation/oo_system_architecture_guide.html">OpenShift</a> I count 4 components.</p>

<p>Both of these applications use a service router, a message bus, a data store and <code>n</code> number of actual nodes running the deployed applications. In both cases, the documentation for these components requires you to already know how to scale and maintain these components. There are any number of places where these stacks can fall apart and break and you will need to be an expert in all of them.</p>

<p>Also one of the more hilarious bits I&rsquo;ve found is the situation with DNS. I can&rsquo;t count the number of shops where DNS changes where things like wildcard DNS were verboten. Good luck with the PaaS dyndns model!</p>

<h2>Operational Immaturity</h2>

<p>To be clear while I feel that most organizations aren&rsquo;t ready for the operational challenges of maintaining a PaaS, the job is made harder by the PaaS software. In both cases, the operational maturity of the products themselves simply isn&rsquo;t there.</p>

<p>Look at the &ldquo;operators&rdquo; documentation <a href="http://docs.gopivotal.com/pivotalcf/concepts/high-availability.html">here</a> for CloudFoundry HA. I can sum it up for you pretty easily:</p>

<blockquote><p>GL;HF</p></blockquote>


<p>Basically they punt everything over to you as if to say &ldquo;Fuck if we know. Use that thing you sysadmin types use to make shit redundant.</p>

<p>And lest you think OpenShift is any better, OpenShift uses MongoDB with this nice bit of information:</p>

<blockquote><p>"All persistent state is kept in a fast and reliable MongoDB cluster."</p></blockquote>


<p>What I&rsquo;m about to say I stand behind 100%. Any company that tells you that MongoDB is &ldquo;reliable&rdquo; is basically saying:</p>

<ul>
<li>We have no idea what we&rsquo;re talking about</li>
<li>We know f-all about operations</li>
<li>We hate you</li>
</ul>


<p>Any tool that uses MongoDB as its persistent datastore is a tool that is not worth even getting started with. You can call me out on this. You can tell me I have an irrational dislike of MongoDB. I don&rsquo;t care. Having wasted too much time fighting MongoDB in even the most trivial of production scenarios I refuse to ever run it again. My life is too short and my time too valuable.</p>

<p>Additionally I&rsquo;ve found next to zero documentation on how a seasoned professional (say a MySQL expert) is expected to tune the provisioned MySQL services. The best I can gather is that you are largely stuck with what the PaaS software ships in its service catalog. In the case of OpenShift you&rsquo;re generally stuck with whatever ships with RHEL.</p>

<p>Another sign of operational immaturity I noticed in OpenShift is that for pushing a new catalog item you actually have to RESTART a service before it&rsquo;s available.</p>

<h2>Disaster Recovery</h2>

<p>After going over all the documentation for both tools and even throwing out some questions on twitter, disaster recovery in both tools basically boils down to another round of &ldquo;good luck; have fun&rdquo;.</p>

<p>Let&rsquo;s assume your PaaS installation is a roaring success. You&rsquo;ve got every developer in your org pushing random applications out to production. Self-service is the way of life. We&rsquo;ve got databases flying all over the place.</p>

<p>How do you back them all up? Well this is a PaaS, Bob. It&rsquo;s all about self-service. The developers should be backing them up.</p>

<p>WAT.</p>

<p>Again based on the research I&rsquo;ve done (which isn&rsquo;t 1000% exhaustive to be fair), I found zero documentation about how the administrator of the PaaS would back up all the data locked away in that PaaS from a unified central place. If your solution is to tell me that Susan&rsquo;s laptop is where the backups of our production database lives, I&rsquo;m going to laugh at you.</p>

<h2>Affinity</h2>

<p>Affinity issues make the DR scenario even MORE scary. I have no way of saying &ldquo;don&rsquo;t run the MySQL database on the same node as my application&rdquo;. This makes the risk surface area even more large. Combine that with the fact that a single host could be running multiple business critical applications. I realize that these tools have algos that are supposed to handle this for you but I&rsquo;ve not seen any sort of policy enforcement mechanism for that in the documentation.</p>

<h1>So what&rsquo;s the answer?</h1>

<p>I don&rsquo;t think ANY of the current private PaaS solutions are a fit right now. OpenShift is, imho, built on unsound ground. CloudFoundry in its current Ruby form is a mess of moving parts. In fairness CloudFoundry is going through a rewrite with some firm leadership behind it that I have quite a bit of faith in when it comes to operational concerns.</p>

<p>Additionally both tools are embracing containers and docker packaging to increase security but none of the tools offer, as far as I can tell, anything resembling a hybrid model. I don&rsquo;t trust docker storage containers yet personally.</p>

<p>And I want to be clear. I&rsquo;m not trying to be a BOFH here with all my talk of &ldquo;placement policy&rdquo; and &ldquo;disaster recovery&rdquo;. I fully embrace the idea of a private PaaS. I simply don&rsquo;t embrace it in any of the current ecosystem. Even a modicum of due diligence should rule them both out until they address what are basic business sanity checks. These platforms require real operations to run and maintain. If you&rsquo;re still throwing things over the wall to your operations team to deploy into your PaaS then you really haven&rsquo;t gained anything. Unless your engineering organization is willing to step up to the shared responsibility inherent in a PaaS, then you definitely aren&rsquo;t ready. Until then, your time and money is better spent optimizing and standardzing your development workflow and operational tooling to build your own psuedo-PaaS.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[There's No Konami Code for Operations]]></title>
    <link href="http://lusis.github.com/blog/2014/06/13/no-konami-for-operations/"/>
    <updated>2014-06-13T23:45:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/06/13/no-konami-for-operations</id>
    <content type="html"><![CDATA[<p>up up down down left right left right b a select start</p>

<!-- more -->


<p>I went on a bit of a rip today about all sorts of technology. I figured I should at least clarify some of it in long form.</p>

<h2>Vmotion/live migration technologies</h2>

<p>Vmotion is a scam. I have frequently said that only trivial workloads are safe for vmotion. Here&rsquo;s the reasoning:</p>

<ul>
<li>live migration realistically requires the workload on the vm being migrated to be quiesced.</li>
<li>any workload that can be quiesced with no impact is most likely a trivial workload</li>
<li>trivial workloads don&rsquo;t NEED the benefit of live migration</li>
<li>to that end, you&rsquo;re basically paying a lot of money to allow a system to copy its files from one node to another</li>
</ul>


<p>Let&rsquo;s also not forget that live migration claims to have accomplished a lot of things such as time travel. You may also know live motion technology by its other names like:</p>

<ul>
<li>Dude where&rsquo;s my clock?</li>
<li>I paused your workload for you but you didn&rsquo;t notic&hellip;hey why did I just get a network partition in my cluster?</li>
</ul>


<p>But hey it demos really well when you can keep watching that streaming video while your vm is moved from host to another.</p>

<h2>Autoscaling</h2>

<p>Autoscaling is a myth. My reasoning behind this has similarities with vmotion/live migration.
Again we have a set of things we need to clarify:</p>

<ul>
<li>horizontal vs vertical autoscaling</li>
<li>triviality of the workload being autoscaled</li>
<li>workload support for autoscaling</li>
<li>scale up vs scale down</li>
</ul>


<p>I am not concerns with trivial workloads. Trivial workloads are&hellip;well&hellip;trivial. The largely cached static marketing website takes no effort whatsoever to scale. Oh look I just brought up a new server with the same static content! Instant capacity!</p>

<p>Let&rsquo;s take a standard architecture here:</p>

<ul>
<li>caching (memcache)</li>
<li>frontend</li>
<li>load balancer</li>
<li>database (this applies to traditional RDBMS as well as NoSQL)</li>
</ul>


<p>When I &ldquo;autoscale&rdquo; my caching layer, I now have to concern myself with the following things:</p>

<ul>
<li>hashing algos</li>
<li>cache miss increase</li>
</ul>


<p>So sure, feel free to autoscale that group of memcache servers but your performance just went to through the floor. Now you&rsquo;ve had a downstream affect on your database as you&rsquo;re having to go to origin due to cache misses. Oh and by the way not all servers talking to the caching tier saw the same topology so now you&rsquo;ve got possibly incorrect data you&rsquo;re serving from the cache</p>

<p>When I autoscale my frontend, I&rsquo;ve now add <code>n</code> number of connections to the database. How&rsquo;s that network looking? Oh wait did you just autoscale to the point of starving the database of resources? Have you possible shot yourself in the foot because now the stuff that was working before is getting rejected because of connection counts?</p>

<p>Autoscaling load balancers is also a problem as you now have the issue of topology mismatch of your backends as well as dealing with session injection that was PREVIOUSLY handled by only one LB.</p>

<p>Finally let&rsquo;s get to autoscaling our database. Vertical or horizontal, relational or &ldquo;NoSQL&rdquo; it doesn&rsquo;t matter. If you vertically scale your DB, do you have to restart the process with larger memory allocations? What about rebalancing of data when you scale horizontally?</p>

<p>And we&rsquo;ve not even gotten to if your application is actually ABLE to be autoscaled (are you entirely stateless? 12 factor friendly?).</p>

<p>Combining these things along with unmentioned downstream impacts and transitive dependencies, means that in most cases when you need to autoscale you won&rsquo;t be able to respond to the workload for some time. It&rsquo;s possible that AFTER that time has passed, the workload may be gone.</p>

<p>And let&rsquo;s not even talk about trying to unwind all that madness via scale down.</p>

<h1>private PaaS</h1>

<p>This is almost the most egregious of them all. It ranks right up there with &ldquo;private cloud&rdquo; in the bullshit-o-meter department. Resources are not infinite.
I have a bit of a guidepost I use when thinking about &ldquo;new&rdquo; functionality in applications.</p>

<ul>
<li>what functionality am I adding?</li>
<li>is it my core competency</li>
<li>what does the landscape of existing ecosystem look like for that functionality</li>
<li>how successful is that landscape for that functionality</li>
</ul>


<p>In most cases, the landscape is either saturated with business whose core focus and expertise is on that concept/functionality. In worst cases the majority of the businesses in that space are failing. Think long and hard about if you have the expertise to do this thing.</p>

<p>PaaS and IaaS are in the same boat. I&rsquo;m a bit more harsh on the IaaS front as I truly believe that if an operations team had been able to deliver on the promise of virtualization but couldn&rsquo;t (for any reason) then sticking ANOTHER layer on top isn&rsquo;t going to magically make it work. The platform still has real hardware under the covers that has the same limitations it had before - bandwidth capcity, io, patching of hypervisors. This stuff doesn&rsquo;t just disappear. In many cases you can actually hit a wall VERY early on for capacity issues.</p>

<p><a href="http://www.slideshare.net/TimMackey/hypervisor-31754727">This slide deck about Cloudstack Hypervisor choices</a> is an amazing read for understanding limitations. Some of these are actually imposed by the hypervisor:</p>

<ul>
<li>Max VLANs</li>
<li>Max Storage</li>
<li>Max vms per hypervisor</li>
<li>Max hypervisors per pool</li>
</ul>


<p>There&rsquo;s no cheat code for this shit, folks. Very few PaaS and IaaS products tackle operational issues at all. Time to first success is important but not if it comes at the expense of cost to operate over time. Yes, I can sudocurlbash your product on to my system but that&rsquo;s trivial. How do I deal with:</p>

<ul>
<li>Backups</li>
<li>Affinity/Anti-affinity issues</li>
<li>Upgrades</li>
<li>Supportability</li>
</ul>


<p>IaaS actualy isn&rsquo;t as bad as a PaaS in some of those but they both have issues. PaaS is mainly worse because you have adopt into an ecosystem and philosophy (not that 12 factor isn&rsquo;t entire good but it&rsquo;s a start) if you want to have any real success.</p>

<p>You will <em>NOT</em> forklift a workload into one of these models and be successful in the long term. In the short term you will simply have given someone else a lot of your money.</p>

<p>I drove a forklift for two years. I know forklifts.</p>

<h1>End rant</h1>

<p>I&rsquo;m not saying that people can&rsquo;t be successful at these things. Clearly they are to some degree. But that&rsquo;s only the public face. How much shit did they have to wade through to make this work? Where are the bits of baling wire, duct tape and a healthy belief in a higher power that keep it from just failling over the edge of the abyss?</p>

<p>Anyway that&rsquo;s just a short list of things from a very tired and worn out person with less hair than he started the day with. I know I shouldn&rsquo;t get mad about this stuff but it&rsquo;s hard when the people trying to smokescreen you ARE you in a sense (professionally speaking). Frankly I&rsquo;m just tired of people thinking that the operational aspects of this stuff are irrelevant because somebody promised them &ldquo;autoscaling selfhealing magical rainbow-colored unicorn piss in a bottle&rdquo; where they didn&rsquo;t have to interact with operations folks ever again.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Omnibus-redux]]></title>
    <link href="http://lusis.github.com/blog/2014/04/13/omnibus-redux/"/>
    <updated>2014-04-13T23:01:00-04:00</updated>
    <id>http://lusis.github.com/blog/2014/04/13/omnibus-redux</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been a pretty big proponent of omnibus. I still think it&rsquo;s the right way to go but recent changes have removed the primary reason for recommending it</p>

<!-- more -->


<p>I did a lot of evangelism for <a href="https://github.com/opscode/omnibus-ruby">omnibus</a> last year. Presentations, blogposts, a sysadvent article. It is/was a great tool however it no longer fits the primary usecase.</p>

<h1>Original workflow</h1>

<p>Originally the biggest benefit to omnibus (outside of the core of what it did) was the Vagrantfile it generated. Because of this Vagrantfile, I could generate a project and publish the repo for anyone to use. That person didn&rsquo;t have to have any ruby tooling installed. They just needed vagrant and two plugins (<code>vagrant-omnibus</code> and <code>vagrant-berkshelf</code>).</p>

<p>They could check out the repository and just run <code>vagrant up</code> and the packages would be nice and neat dropped off in the <code>pkg</code> directory locally. I didn&rsquo;t see this as a problem workflow because I didn&rsquo;t listen to my own advise.</p>

<p>This is from the original generated README of a fresh omnibus project:</p>

<p><img src="http://s3itch.lusis.org/1wzxjV.png" alt="original README" /></p>

<p>This was the part of the workflow I was bullish on. In fact we went whole hog internally with this. Anyone could contribute because they could test locally with only the exising tools that we already had installed.</p>

<p>But that seems to have all changed with Omnibus version 3. Now omnibus requires a full ruby development environment just to do what it previously did with a <code>Vagrantfile</code> alone.
The reason for this is that instead of Vagrant, now omnibus uses <a href="https://github.com/test-kitchen/test-kitchen">test-kitchen</a>. Additionally it seems to ALSO require <code>Berkshelf</code> locally now.</p>

<p>This is where it gets really ugly.</p>

<p>It doesn&rsquo;t just require Berkshelf. It requires an UNRELEASED version of Berkshelf.</p>

<h2>Sidebar on tooling</h2>

<p>I wanted to take a minute to talk a little bit about Chef tooling.</p>

<p>There is evidently a shift going on in the Chef community and I apparently haven&rsquo;t been keeping up. The Chef community flocked to Berkshelf for reasons I don&rsquo;t understand. It evidently solved a problem I didn&rsquo;t have. You see I used chef-client and knife (with several plugins). I work with a lot of folks who are NOT ruby or chef people. For us, Chef is a means to an end. It&rsquo;s a tool just like Maven or Artifactory. We use chef-solo as the installer for our platform, for instance. We are not ruby developers or users. All of our tooling is either in Python or Java and our application code base is in Java.</p>

<p>Opscode (or rather Chef) has previously made a big push to make being a Chef user mean not being a ruby expert. There seems to have been not only a shift in that thinking but also in how the tools are to be used.</p>

<p>A good example is chef-metal. This originally confused me because this was the chef model:</p>

<ul>
<li><code>knife</code> is for your workstation</li>
<li><code>chef-client</code> is for your servers</li>
</ul>


<p>With <code>chef-metal</code> that changes a bit because the understanding is that where you might use <code>knife rackspace &lt;blah&gt;</code> you&rsquo;ll now run <code>chef-client recipe[rackspace_servers]</code>.</p>

<p>So back to berkshelf and other tools&hellip;</p>

<p>Before these were optional. Slowly but surely they&rsquo;re becoming NOT optional. The problem with this is as I described above. No longer is the workflow:</p>

<ul>
<li>install knife</li>
<li>checkout our chef repo</li>
<li>edit/upload cookbooks</li>
</ul>


<p>It&rsquo;s now become a ruby developer workflow of somekind. I don&rsquo;t have a cookbook directory. All of my cookbooks are somewhere in <code>~/.berkshelf</code> and I&rsquo;m expected to have every cookbook be its own repo or something. I still don&rsquo;t fully understand what&rsquo;s going on here and frankly I don&rsquo;t have the time. I have chef novices on my team and I don&rsquo;t have any official documentation to point them at because this is something that exists outside of the official chef documentation.</p>

<p>I&rsquo;m not trying to slander ANY of these tools. I&rsquo;m sure they&rsquo;re all wonderful. <code>test-kitchen</code> looks great for being able to break away from tying the provisoner to vagrant but again that&rsquo;s not the workflow that works for us (or frankly anyone who just wants to use chef). My argument is simply that if these things are going to be the defacto model then they should be rolled into Chef somehow and be documented on the official documentation.</p>

<h1>So back to omnibus</h1>

<p>So we have two issues here that make omnibus not a fit anymore:</p>

<ul>
<li>Requires a full blown ruby environment to <strong>BUILD</strong> a project whereas before it only required a full blown ruby environment to <strong>CREATE</strong> a project.</li>
<li>the 3.x RELEASE of Omnibus had a dependency on an <strong>UNRELEASED</strong> artifact</li>
</ul>


<p>That second one is really painful to swallow. Quite frankly it&rsquo;s just poor form to do that. You can argue about version numbers being meaningless or &ldquo;it&rsquo;s stable just still in beta&rdquo; but when you&rsquo;re asking someone to use and depend on your tools it&rsquo;s just not right. If your dependencies aren&rsquo;t released yet then you don&rsquo;t get to release. Let&rsquo;s also not forget that EVERY anicllary add-on in the Chef world seems to have its own dependency on Berkshelf now.</p>

<h1>How did this all come to a head</h1>

<p>When Heartbleed hit, I needed to rebuild our two big omnibus packages. Recently I had switched over to a new laptop and didn&rsquo;t yet have anything checked out. This was fine because I had the omnibus projects checked out on my desktop. It was running a 1.4 release of vagrant and was where I did most of my builds before. So we generated new packages and were happy.</p>

<p>We also have new team members on our ops team. I was using this as an opportunity to show them the omnibus packages and let them build them as well. So I tell them to check out the repos, make sure they have the plugins installed and run <code>vagrant up</code>. This didn&rsquo;t work and it turns out somebody had vagrant 1.5 installed. No big deal I think. We&rsquo;ll punt on that one and just make a note that we&rsquo;ll need a new <code>vagrant-berkshelf</code> plugin when it&rsquo;s released.</p>

<p>But yesterday I went to work on a massive refactor of our omnibus packages since we&rsquo;re cleaning up a bunch of extras and changing things around. I knew that omnibus 3 had several things to make the whole build process go faster. It also allowed me greater control in build determinism. So I upgrade and generate a new project to see what the new layout looks like and test the builds. When I realize there&rsquo;s no Vagrantfile, I&rsquo;m really confused. The readme says a Vagrantfile would be generated.</p>

<p>That set off the things I tweeted and posted on the mailing list. In the end it came down to me evidently relying on something I was never supposed to rely on and being told I should learn to RTFM.</p>

<h1>So where does that leave things?</h1>

<p>Right now I have to take back everything I said about omnibus. It&rsquo;s not that I don&rsquo;t think it&rsquo;s a great tool and I certainly don&rsquo;t give two shits about getting subtweeted. I still think it&rsquo;s a great tool and I think the idea is the right one.</p>

<p>However the main reason I recommended omnibus and bothered to integrate it is gone. It&rsquo;s simply not the straightforward process it was and the removal of the vagrant build lab puts too much on the non-ruby-ecosystem user. This is where I didn&rsquo;t listen to my own advice. I frequently warned people that omnibus exists first and foremost for the needs of Chef. We got lucky that it worked this long and it was an awesome ride.</p>

<p>I&rsquo;m still trying to figure out the best way to get BACK to the vagrant build-lab but it&rsquo;s not working out so well. I&rsquo;m attempting to rebuild my <a href="https://github.com/lusis/omnibus-omnibus-omnibus">omnibus-omnibus-omnibus</a> project to ship everything needed but now I&rsquo;m back to stepping outside the community framework and making people who wanted to just create packages needing something extra I created.</p>

<h1>One last thing</h1>

<p>I&rsquo;m not posting this expecting anyone to change anything in Omnibus and I&rsquo;m not trying to be passive-aggressive. I&rsquo;m not entitled to anything from anyone. This is more about providing something I can link to for users of the omnibus builds I&rsquo;ve already published since they will no longer work out-of-the-box.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stop Fighting Distros - Part 2]]></title>
    <link href="http://lusis.github.com/blog/2013/09/23/stop-fighting-distros-part-2/"/>
    <updated>2013-09-23T23:27:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/09/23/stop-fighting-distros-part-2</id>
    <content type="html"><![CDATA[<p>I&rsquo;m a pretty harsh person. My wife and I have this discussion all the time about how things I say get interpreted. As the communicator, this responsibility lies squarely on my shoulders.</p>

<p>So before I start, I don&rsquo;t &ldquo;hate&rdquo; distributions or the packaging format they use or the people doing the work. To this even of you who toil to track countless security reports or maintain some software package in an upstream repository because of your love for that software, here&rsquo;s to you.</p>

<!-- more -->


<h2>Packaging policies</h2>

<p>In general I hate very few things. When I say I hate something usually hate is even too strong of a word.</p>

<p>I <strong>really</strong> hate stupid policies. I hate stupid policies that are predicated on the shape of a reality that either no longer exists or never existed in the first place. I hate policies that never evolve to the reality of the world. I hate policies that throw pragmatism and common sense out the window. I also really hate ego-driven policies.</p>

<p>What kinds of things fit that bill?</p>

<ul>
<li><em>most</em> corporate security policies</li>
<li>Various parts of PEP8</li>
<li>a oddly large amount of government regulation and legislation</li>
<li>FHS</li>
</ul>


<p>And yes, distro packaging policies (mostly) fit that bill.</p>

<p>I&rsquo;ve said this a lot recently (and I stand by it):</p>

<blockquote><p>Distribution packaging policies are not designed for people who package software</p></blockquote>


<p>Packaging policies exist first and foremost for the benefit of the distribution. This isn&rsquo;t a bad thing or a good thing - it&rsquo;s just a thing.</p>

<p>Let&rsquo;s compare a few packaging policies from different distros. I spent most of my valuable time this evening between fighting a 3 year old who didn&rsquo;t want to go to sleep reading these.</p>

<ul>
<li><a href="http://www.debian.org/doc/debian-policy/">Debian</a></li>
<li><a href="https://fedoraproject.org/wiki/Packaging:Guidelines?rd=Packaging/Guidelines">Fedora</a></li>
<li><a href="https://wiki.archlinux.org/index.php/Arch_Packaging_Standards">Arch</a></li>
<li><a href="https://devmanual.gentoo.org/general-concepts/index.html">Gentoo</a></li>
</ul>


<p>Ignoring for a minute the sheer verbosity of some of these guidelines (I think Arch wins at simplicity and pragmatism), they all share a few common key themes:</p>

<ul>
<li>Break shit up into multiple packages</li>
<li>It needs to be able to be built from source</li>
<li>put X here, Y here, Z here</li>
<li>ONLY 1 VERSION IS ALLOWED</li>
<li><em>These guidelines exist to make maintaining the distribution easier</em></li>
</ul>


<p>Note that of all the distributions Debian is quite possibly the most offensive at this. Fedora follows a close second.</p>

<p>I want to address a few of these though I might have done so in the previous version of this post. I&rsquo;m also going to skip the &ldquo;build from source&rdquo; point because I have no real problem with that on a general level.</p>

<h3>Break shit into multiple packages</h3>

<p>There are two arguments here that tend to crop up in support of this:</p>

<ul>
<li>security</li>
<li>disk space</li>
</ul>


<p>Disk space concerns are entirely subjective. I&rsquo;m not hurting for disk space and I haven&rsquo;t been for quite some time. I&rsquo;m anal about which packages I install, not because of disk space, but because I want to have to worry about as few security announcements as possible. Quite honestly this argument is the distro equivalent of PEP8&rsquo;s column width. It is not my concern how many ISOs the distribution has to span across. I either install from a private mirror or netboot off the internet. Regardless I haven&rsquo;t put an actual cd in a system in a VERY long time.</p>

<p>But the one that gets me is the &ldquo;security&rdquo; argument. The basis for this is that you can supposedly immediately get the benefit of a fix to, say openssl, in all packages that link against openssl.</p>

<p>This is bullshit.</p>

<p>For that to REALLY be true, you have to turn up the white noise reality distortion field pretty high.</p>

<p>Let&rsquo;s take a vulnerability in a library that everything in the system links to like openssl. There have been <a href="http://www.openssl.org/news/vulnerabilities.html">three CVEs</a> against openssl this year (2 of which affect versions that most distributions use today).</p>

<p>For a distribution to get a fix in place for openssl, they should ostensibly be testing EVERY SINGLE PACKAGE that links against openssl. In reality, you can get by with lesser testing of that fix assuming the ABI doesn&rsquo;t change but that&rsquo;s still a pretty big risk to take. I&rsquo;m honestly not sure (and would love feedback) from distros about exactly what the test cycle is for these cases.</p>

<p>But let&rsquo;s assume they do test that. If the vulnerability was &ldquo;responsibly disclosed&rdquo; it&rsquo;s possible they&rsquo;ve had time to do that but if not, you have a case where every single package on your system that links against openssl could be vulnerable.</p>

<p>So at most, what you have is a wash. I would actually argue that a package that brings its own deps is better off in this case.</p>

<p>Let&rsquo;s also not ignore the case that <strong>RedHat 5 STILL USES RUBY 1.8.4 WHICH DOES NOT GET ANY FIXES UPSTREAM WHATSOEVER.</strong> I don&rsquo;t think ANY ruby 1.8 version is getting fixes anymore. You are ENTIRELY dependent on Redhat to not only backport fixes from different versions of Ruby but also make sure they don&rsquo;t break any other Ruby packages the distro ships. In some cases, Redhat will actually have to figure out how to patch that version of Ruby.</p>

<h3>File placement</h3>

<p>The whole &ldquo;put X here, Y here, Z here&rdquo; is really about consistency more than anything. I don&rsquo;t entirely disagree with it but it really is a matter of preference. I personally like my configs to live close to my application. I hate bouncing around the filesystem.</p>

<h3>Version restrictions</h3>

<p>This is where it gets ugly and when combined with the &ldquo;use existing libraries&rdquo; model that the whole policy is downright combative to people who write software.</p>

<p>I&rsquo;ve said previously that various language communities have coalesced around their own toolchains. One thing that&rsquo;s pretty common, however, is that those communities allow for multiple concurrent versions of a given package. How they isolate and define which version to use is specific to the language but it exists.</p>

<p>Shoehorning that into the vendor policy around versioning simply does not work.</p>

<p>Let&rsquo;s take a java application. I need version 42 of <code>commons-dingleberry</code>. The vendor has packaged version 39. If this was debian, they would tell me I need to make my application work with version 39. What is more likely the case is that my application simply won&rsquo;t ever be included in upstream because I&rsquo;m not about to spend my free time that I contribute to an opensource project trying to make my software work with a version of a library that may simply not have the functionality I need.</p>

<p>This is really quite hilarious when you take something like Logstash which shades all the java deps it has in to its own jar. These packaging guidelines (and from what was communicated by someone attempting to get it into Fedora) requires someone to actually CHANGE Logstash such that the ElasticSearch version is its own package. Each version of Logstash is actually dependent on a specific version of ElasticSearch (due to the ability of Logstash to run ES embedded or as a non-data cluster member). Giving a user the &ldquo;freedom&rdquo; to switch ES versions actually creates a shitty experience for the user.</p>

<h2>And that&rsquo;s really the crux of it</h2>

<p>Jordan has asked people repeatedly to NOT try and get Logstash into upstream repositories. Here&rsquo;s why:</p>

<blockquote class="twitter-tweet"><p><a href="https://twitter.com/lusis">@lusis</a> <a href="https://twitter.com/robynbergeron">@robynbergeron</a> my fear is having the next 10 years spent telling rhel users &quot;sorry, logstash 1.2 is X years old, please upgrade&quot;</p>&mdash; @jordansissel (@jordansissel) <a href="https://twitter.com/jordansissel/statuses/382251492775710720">September 23, 2013</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Jordan makes a valid point. And it&rsquo;s not just Logstash. The software that people are attempting to run today (yes even in the stodgiest of enterprises) changes too often to ever be valid as packaged in upstream. No software I have ever cared about running outside of the core OS was ever present in a distro&rsquo;s upstream repository. Running CentOS or RHEL? EPEL is the FIRST thing I have to add to the system.</p>

<h1>Remediation</h1>

<p>Nothing kills the relevance of a distribution faster than how much effort someone has to go to for it to be usable. Why did Ubuntu start to &ldquo;win&rdquo;? I see two things:</p>

<ul>
<li>2 year LTS cycle</li>
<li>The addition of PPAs</li>
</ul>


<p>Two years, while still an eternity, is frequent enough to be able to work in relevant changes in the industry. PPAs provide an outlet for someone to provide packages in a way that is largely well integrated into the distribution workflow. However even PPAs are not keeping up with the pace these days. I&rsquo;m curious if an omnibus-style package would ever work in a PPA (assuming you would go to the effort of converting an omnibus project into a source deb.</p>

<h1>Software authors share the responsibility</h1>

<p>Using the vendor&rsquo;s package format as an excuse not to provide packages doesn&rsquo;t fly anymore. FPM has eliminated that. You don&rsquo;t have to know a single goddamn thing about the RPM spec format or  deb control files.</p>

<p>And beyond that, with omnibus, you don&rsquo;t even need to worry about dependencies on the system. By default and omnibus project will build packages for every current LTS release of Ubuntu and RHEL/RHEL clones. There&rsquo;s not even a need to create a proper apt or yum repo. With an omnibus package you don&rsquo;t have any external dependencies.</p>

<h1>What can distribution vendors do?</h1>

<p>I&rsquo;ve talked at length about this issue with a few folks but none so passionate about it as <a href="https://twitter.com/samkottler">Sam Kottler</a> and <a href="https://twitter.com/robynbergeron">Robyn Bergeron</a>. Robyn actively sought me out at PuppetConf a few years ago. I think we chatted for over an hour on distribution relevance.</p>

<p>In general I think what would help most at this point is for distributions to provide an avenue for authors to provide packages that don&rsquo;t meet strict guidelines. If Opscode wants to provide omnibus&rsquo;d chef clients in /opt/chef without the user having to curlbash the package, that would awesome.</p>

<p>There is the concern to the user but my crusade in technology has been to not treat the user like a 3 year old. Give users enough clear information about risk and let them make the decision to install a package. Communication goes a long way.</p>

<h1>More to come</h1>

<p>I&rsquo;m going to have more to write about this topic. It&rsquo;s a never-ending source of fuel really. Thanks for reading.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Historical Compendium of Useless Shit]]></title>
    <link href="http://lusis.github.com/blog/2013/09/17/a-historical-compendium-of-useless-shit/"/>
    <updated>2013-09-17T22:20:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/09/17/a-historical-compendium-of-useless-shit</id>
    <content type="html"><![CDATA[<p>Yes it started on Twitter and ended up here. Circle of life and all that</p>

<!-- more -->


<p><em>Fair warning: I&rsquo;m not doing ANY research for this post. Normally I do but I&rsquo;m doing this off-the-cuff so to speak.
If I fuck up some specific thing, please feel free to correct. This is basically a personal perspective/history
And it&rsquo;s probably full of typos so there&rsquo;s that</em></p>

<p>So a discussion on twitter started because I brought up Mesos, Docker and OSv. I commented that none of these things were &ldquo;new&rdquo; technology really.</p>

<p>To be clear, it is a scientifically provable fact (for some values of &lsquo;scientifically provable fact&rsquo;) that when containers are brought up FreeBSD jails and Solaris Zones will work its way into the conversation. This is actually perfectly legitimate as the fit the same &ldquo;model&rdquo; as containers - lightweight &lsquo;virtualization&rsquo;. And whenever virtualization is brought up, someone will bring up IBM and LPARs. Again, perfectly legit.</p>

<p>In rough order of quick wikipedia'ing and personal memory, it goes something like this:</p>

<ul>
<li>IBM virt (z, i and later p - can&rsquo;t recall if RS6k AIX did LPARS)</li>
<li>FreeBSD jails (wikipedia says first in 4.0 so around 2000?)</li>
<li>Solaris Zones (again the big interbrain says 2004)</li>
<li>Xen (2003)</li>
<li>VServer (???)</li>
<li>OpenVZ (remember that shit? 2005ish)</li>
<li>UML (????)</li>
<li>VMware (late 90s iirc)</li>
</ul>


<p>Also slot QEMU and KVM in here somewhere. These are all forms of &lsquo;virtualization&rsquo;. I&rsquo;m sure there are some pedants are drafting comments now but let&rsquo;s call it what everyone who isn&rsquo;t a pedant calls it - virtualization (this little bit is important). Each one is better or worse for different reasons.</p>

<p>All I&rsquo;m trying to say is that everyone has a claim at some point in the stack. I&rsquo;ve always argued that really IBM is the progenitor of this whole space but I&rsquo;m sure someone will point out something I missed.</p>

<h1>So about <code>*</code>BSD</h1>

<p>The point was made that everyone seems to forget about FreeBSD jails. I don&rsquo;t think that&rsquo;s the case at all. Here&rsquo;s my little take on why FreeBSD didn&rsquo;t &ldquo;take off&rdquo; (and the same applies to others as well). I&rsquo;m probably wrong but whatever. Not like that&rsquo;s a first.</p>

<p>Everyone involved in this twitter convo has been in the industry a while. We remember when Linux wasn&rsquo;t the defacto operating system you would use. AIX, Solaris, HPUX - THOSE were the things REAL businesses used.</p>

<p>So why is Linux suddenly the defacto choice and why did technically superior solutions fall by the wayside? As I said, that wasn&rsquo;t always the case. We used to actually have to HIDE the fact that the print server wasn&rsquo;t running WindowsNT. Linux and FreeBSD were what we used at home. The others were what we used at work. I got my start with the whole Linux world around 95 with Slackware just as Yggdrasil was going away.</p>

<p>Anyway here are a few things I think that &ldquo;hurt&rdquo; FreeBSD a bit:</p>

<h2>Licensing</h2>

<p>Licensing wars were almost as prolific as editor wars in on Slashdot. Regardless of your position on WHICH license is more &lsquo;liberal&rsquo;, this had an impact on the success. IIRC, the original BSD license was incompatible with the GPL. This meant there was little cross-pollination between the communities.</p>

<p>Regardless, what you ended up seeing was more &ldquo;corporate&rdquo; adoption of BSD into closed hardware products (firewalls, load balancers) and that never made it back into the community because it didn&rsquo;t have to.</p>

<p>On the Linux side, however, the GPL sort of promoted a community because of the forced pollination.</p>

<h2>Speciation</h2>

<p>Probably not the best way to describe it but within the BSD community the 3 major derivitives ended up being pigeonholed</p>

<ul>
<li>FreeBSD was your server OS</li>
<li>NetBSD was for weird-ass random hardware</li>
<li>OpenBSD was for your firewalls (&lsquo;cause Theo is mad crazy about security yo)</li>
</ul>


<p>Again, whether or not this characterization was correct that was the general thought process most people I knew had.</p>

<p>Meanwhile, Linux was Linux. Yes there were distros but they largely set themselves apart by what they layered on top in userland. This is somewhat critical.</p>

<h2>Desktop</h2>

<p>As I said above, FreeBSD was never really positioned as being for desktops among the commoners. Meanwhile Linux distributions were catering to that crowd. The thing is Linux was acceptable as a server OS as well. So if we were running Linux at home on our desktops, there was very little cognative disconnect to running it on our servers. Just don&rsquo;t install X and it&rsquo;s a server, right?</p>

<p>FWIW this is one of the things that I think helped Microsoft as well - knowledge portability.</p>

<h2>IBM</h2>

<p>Quite honestly, IBM was probably the biggest thing to help Linux get that final push. Yes we&rsquo;d been running Linux at home, on our personal web servers and in small corners of the office but now here&rsquo;s a name our bosses trust saying that Linux is a thing. That bald little boy talking about sharing shit and all that. Holy fuck! We made it. Then the COTS started coming. I remember getting that copy of Oracle 6 (or was it 7?) at Atlanta Linux Expo. Holy shit, Oracle runs on Linux!</p>

<p>So this IBM thing is a pretty big hurdle for a BSD to overcome. Marketing took over. Nobody cared that ufs was superior to ext2 or which is better ipfilter (then pf) vs. ipchains. IBM is backing Linux to the tune of 1 Billion dollars. Who gives a fuck about which license is more &ldquo;free&rdquo;?</p>

<p>Oh and IBM is doing it again though arguably it means fuckall at this point since Linux is, again, the defacto standard.</p>

<h1>Wrap up</h1>

<p>Mind you this is mostly personal opinion/perspective. Is FreeBSD technically superior? Probably. Did it do certain things first and better? Yep (though as I brought up someone else did it before FreeBSD in some cases).</p>

<p>Let&rsquo;s face it, even as Linux fans you have to admit that Linux has been adopting shit from everyone else for a while and arguably as a shittier implementation. Don&rsquo;t get me wrong, I love Linux and I owe it a lot. This is one reason I try and give back as much as I can. I wasn&rsquo;t able to before.</p>

<p>I think FreeBSD has a chance for a comeback at this point (some of these have been going on for a while):</p>

<ul>
<li>REAL AWS support (not some hackish method). People start in AWS for better or worse.</li>
<li>Having a native JDK by way of OpenJDK (as opposed to the ABI bullshit you used to have to do), will help.</li>
<li>The general industry move AWAY from COTS. This has been going on for a while but nobody really fucking cares about if you can run Websphere.</li>
<li>General embracing of alternative languages which have no problem running on BSD</li>
<li>General frustration with Linux as a server OS. See CoreOS and talk to anyone at Fastly.</li>
</ul>


<h1>Some things I find &lsquo;funny&rsquo;</h1>

<ul>
<li>For all the bitching we used to do about GPL vs BSD license, everybody I know these days is going with Apache over either of them</li>
<li>After all this fucking time, &ldquo;Linux on the desktop&rdquo; is still a joke (and I&rsquo;ve run Linux on my desktop for the past 15 years)</li>
<li>Linux volume management STILL sucks compared to like&hellip;.everywhere else</li>
<li>People hated the BSD source-based model and yet Gentoo was a thing&hellip;</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Go for System Administrators]]></title>
    <link href="http://lusis.github.com/blog/2013/08/11/go-for-system-administrators/"/>
    <updated>2013-08-11T15:44:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/08/11/go-for-system-administrators</id>
    <content type="html"><![CDATA[<blockquote><p>If I never directly touch a Go concurrency primitive, I'm convinced I'm going to write all my cli apps with it just for ease of deployment.</p></blockquote>


<!-- more -->


<p>This is something I said the other day. I figured it deserved a more detailed blog post.</p>

<h2>NKOTB</h2>

<p>Most people who know me professionally know two things about me:</p>

<ul>
<li>I&rsquo;m fairly pragmatic and somewhat conservative about technology decisions</li>
<li>I&rsquo;m a language tourist</li>
</ul>


<p>This second one is something Bryan Berry attributed to me in an early FoodFight episode. What&rsquo;s interesting is the two things seemingly conflict.</p>

<p>I love learning new programming languages. This comes as a pretty big shock to me on a regular basis because I&rsquo;m not a professional programmer. I didn&rsquo;t go to college for programming (I actually didn&rsquo;t go to college at all). My career in IT has been pretty much 100% focused on the area of operations. Anything I&rsquo;ve ever touched - qa, dba, dev - has always been from that lens and to satisfy some need operationally.</p>

<p>So it&rsquo;s weird that I find myself 18 years later having a working knowledge of ruby, python, perl, java and a few other languages to a lesser degree. Mainly I come to new languages to scratch an itch.</p>

<p>This leads me to picking up Go.</p>

<p>If you haven&rsquo;t heard of Go, there are countless articles, blog posts and a shitload of new tooling written in it. The latest batch of hotness around linux containers and new deployment models (docker) is based on Go. There are also quite a few other &ldquo;big&rdquo; name projects built in Go as well - packer, etcd. Mozilla is doing all new internal tooling in Go (as I understand it) and quite a few folks are switching to it.</p>

<p>Mind you I don&rsquo;t pick up languages based on popularity. I don&rsquo;t care for JavaScript and Node at all, for instance. Originally I had no interest in Go either. I figured it was another Google experiment that was more academic than anything else. Besides, if I had to get a handle on a c-like language, why not just learn C and be done with it?</p>

<p>I actually attempted that route working on a PAM module for StormPath. While it was somewhat satisfying, it was ultimately VERY frustrating.</p>

<h2>So why Go now?</h2>

<p>One of the reasons I decided to give Go another shot was that it appeared to be around for the long haul after all. That made at least a contender for me.
But then some of the tooling I was using operationally was being built in Go. Since I wanted to be able to fix issues in those tools (especially considering they were new projects which would surely need fixes) I really needed to pick up on the language.</p>

<p>However one tool really pushed me that last step - <a href="https://github.com/coreos/etcd">etcd</a>.</p>

<p>You can read up on etcd yourself but if you know my history with <a href="https://github.com/lusis/Noah">Noah</a>, you realize WHY I have such an interest in this.</p>

<p>What surprised me was when I decided that I&rsquo;d probably be writing a lot of tooling myself in Go.</p>

<h2>On Pragmatism</h2>

<p>All the internal tooling my team develops at Dell Enstratius is written in Python. This was a pragmatic choice for us:</p>

<ul>
<li>It&rsquo;s the least common denominator on platforms our product supports (So it will always be on customer systems)</li>
<li>It&rsquo;s rigid in the right ways for new programmers (of which we had quite a few on our team)</li>
<li>Regardless of skill level with Python, you can usually look at someone else&rsquo;s code and follow it thanks to the previous item</li>
</ul>


<p>Why didn&rsquo;t we go with Ruby considering I was personally much stronger at Ruby and we had some Ruby experience via Chef internally?</p>

<ul>
<li>Have you seen the state of Ruby on distros?</li>
<li>We didn&rsquo;t want to conflict with any possible customer tooling using Ruby</li>
<li>Not enough rigidity for the new folks</li>
<li>As comfortable as I am at Ruby, because of the flexibility of the language and metaprogramming, it can be downright impossible to navigate someone else&rsquo;s code</li>
</ul>


<p>Our team weighed all the options here and we all agreed on Python. I set out to write a library for accessing our API. This would give us a foundation for our tooling as well as serve as a reference project - with tests, project structure, bin scripts and the like - for new tooling.</p>

<p>Things are/were going great up until a recent situation with a customer. We try and minimize dependencies in our tooling for obvious reasons. However there are a few libraries that just make things SO much easier - <a href="https://github.com/kennethreitz/requests">requests</a>, <a href="https://github.com/kennethreitz/envoy">envoy</a>. We also like to use <a href="http://fabfile.org">Fabric</a> to wrap some things up.</p>

<p>However we ran into a situation where a customer refused to let us pull packages in externally. So while we could &ldquo;sneakernet&rdquo; the bulk of our tools over, some things wouldn&rsquo;t work. Tracking down all the transitive deps and vendoring everything was a pain in the ass.</p>

<p>This is what lead to my statement above.</p>

<h2>Tooling in Go</h2>

<p>Go, while not as tight a feedback loop as Python, is still pretty tight. Compilations happen fast and you can test fairly quickly. But the dependency issue is really the killer. It simply doesn&rsquo;t exist. I can take that binary I compiled and move it around with no problem without needing the runtime installed. There are lots of batteries included in the stdlib as well.</p>

<p>I can also compile that same code on osx, windows or linux with no modification. This bit us in Python with some of our deps as well.</p>

<p>As I said, while the tooling I&rsquo;m currently writing has no need for any of the advanced concurrency stuff in go, it&rsquo;s nice that&rsquo;s it there out of the box should I want to use it.</p>

<h2>This isn&rsquo;t a switching story</h2>

<p>We&rsquo;re not switching to Go for our tooling but I probably will. I&rsquo;m already working on writing a wrapper for our API in Go so I can duplicate some of the tools. This will be really handy when I&rsquo;m on a system where dependencies are limited. That&rsquo;s really what this post is about.</p>

<p>If you&rsquo;re in operations, there is no reason you shouldn&rsquo;t learn Go.</p>

<p>The syntax is easy. The stuff that made C painful is largely hidden from you. Meanwhile you don&rsquo;t need to worry about what version of Python or Ruby is installed on your systems. It&rsquo;s a great language to use for bootstrap tools where you don&rsquo;t yet have your deps installed. It&rsquo;ll also help should you start adopting tools like docker, packer or etcd.</p>

<p>Give it a shot.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DevOps - the Title Match]]></title>
    <link href="http://lusis.github.com/blog/2013/06/04/devops-the-title-match/"/>
    <updated>2013-06-04T21:38:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/06/04/devops-the-title-match</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve noticed an annoying trend recently. I was content to ignore it for a while but now it&rsquo;s getting almost stupid. That trend is the job title of &ldquo;DevOps&rdquo;.</p>

<!-- more -->


<p>I came across an article tonight that confused the hell out of me. It was an interview. The outlet wasn&rsquo;t a technical one per se but it was a technical interview none the less.</p>

<p>This part is what confused me (<em>highlights mine</em>):</p>

<blockquote><p>To do this requires a well-integrated platform that is full of capabilities for the **developer**; contemplates the needs of **DevOps**; provides command, control and visibility for **operations**; and is ....</p></blockquote>


<p>To quote an internet-famous person:</p>

<p><strong>WAT</strong></p>

<h2>Stepping back</h2>

<p>I don&rsquo;t like the idea of <em>devops</em> as a job title in the first place. I don&rsquo;t like it as a role either. It makes no sense. You don&rsquo;t call someone an <em>agile</em> so why would you call them a <em>devop</em>?</p>

<p>My problem lies in the fact that it implies that there&rsquo;s something wrong with being a sysadmin, operations person, developer or whatnot. Not only that but the idea behind devops is the elimination of silos, not the creation of new ones.</p>

<p>I have, however, made a bit of peace with the fact that <em>devops</em> has become a replacement title of sorts for <em>sysadmin</em> or <em>developer</em>.</p>

<p>I get the problem domain. Companies want to be able to qualify the types of people they want. The phrase <em>devops</em> carries a certain meaning with it. People are trying to leverage that. In other cases, it&rsquo;s become a codeword for &ldquo;generalist&rdquo; or &ldquo;technologist&rdquo;. And, yes, even in some cases it&rsquo;s become a code word for &ldquo;doing both development and ops work&rdquo;</p>

<h2>What does devops mean though</h2>

<p>Here&rsquo;s the secret. I&rsquo;ll tell you EXACTLY what devops means.</p>

<p>Devops means giving a shit about your job enough to not pass the buck. Devops means giving a shit about your job enough to want to learn all the parts and not just your little world.</p>

<p>Developers need to understand infrastructure. Operations people need to understand code. People need to fucking work with each other and not just occupy space next to each other.</p>

<p>I worked at a company several years ago. We created a dedicated devops team. The rationale was solid - the company had a monolithic idea of roles and titles. We also had a large group on both sides that were only interested in doing their little bit and going home. By creating this title/team, it was easier at a company level to justify them working on non-standard projects.</p>

<p>So a &ldquo;devops&rdquo; team was created. This was a small team of what essentially boiled down to &ldquo;super sysadmins&rdquo;. We wrote puppet manifests, worked with the developers to automate build processes&hellip;shit like that.</p>

<p>What ended up happening was that the devops team was seen as elitist by the operations team, nosy and invasive by the developers and everyone just passed the blame on to them - &ldquo;Devops did that. Not us&rdquo;</p>

<h2>So back to that quote</h2>

<p>Having said all that, what about the quote?</p>

<p>This is indicative of the problem I described above. I think I&rsquo;ve finally figured out the question I want to ask people who think this way:</p>

<p>If devops is a distinct role/title apart from development and operations, then what the fuck does a &ldquo;devop&rdquo; do?</p>

<p>Let&rsquo;s look at that quote again. It implies that:</p>

<ul>
<li>&ldquo;command, control and visibility&rdquo; is something developers have no need for</li>
<li>operations won&rsquo;t need the same capabilities as developers</li>
<li>There&rsquo;s a third group that has an entirely different set of needs</li>
</ul>


<p>What is that third group? What possible aspect outside of development and operations of the IT needs of a company do those two groups not think affects them in some way and thus have a vested interest in being involved? <em>(Yes I&rsquo;m aware companies have dbas, security and what not - those are shitty silos too)</em></p>

<p>This topic comes up all the time on various mailing lists and it never seems to really reach any sort of consensus. So I&rsquo;m asking you. If a &ldquo;devops&rdquo; is something different than someone in operations or development, someone different than a sysadmin or developer&hellip;.</p>

<p>What the fuck is it?</p>

<h2>Disclaimer</h2>

<p>This post was writting on an airplane with an annoying passenger in front of me, quite a bit of rum in me and a lack of sleep. I&rsquo;m guessing it really doesn&rsquo;t look any different than any other post though does it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Smart Clients]]></title>
    <link href="http://lusis.github.com/blog/2013/05/13/smart-clients/"/>
    <updated>2013-05-13T21:17:00-04:00</updated>
    <id>http://lusis.github.com/blog/2013/05/13/smart-clients</id>
    <content type="html"><![CDATA[<p>Currently <a href="http://ricon.io/east.html">RICON|EAST</a> is going on in NYC. <a href="https://twitter.com/tsantero">Tom Santero</a> and the whole Basho crew is doing an awesome job if the content available via the live stream and twitters is to be believed.</p>

<!--more-->


<p><em>(please note this is my first blog entry post-D. As such and because I&rsquo;ve yet to talk to any legal folks, I should state that this does not represent any opinion or policy of Dell)</em></p>

<p>One thing that caught my eye/ear when I could listen/watch was an excellent presentation by <a href="https://twitter.com/seancribbs">Sean Cribbs</a>. Sean holds a special place in my hero worship pantheon for a few reasons:</p>

<ul>
<li>I first heard about Riak on one of the first episodes of the ChangeLog show (along with <a href="https://twitter.com/argv0">Andy Gross</a>). It and the whole NoSQL thing made sense then</li>
<li>Sean is a pretty fucking down to earth person. He graciously drove down to one of our local meetups. Uber friendly and an awesome advocate for Basho and Riak.</li>
<li>He&rsquo;s a really awesome presenter and if you&rsquo;ve never had the priviledge of seeing him (live stream or in person), he rocks the mic.</li>
</ul>


<p>So in Sean&rsquo;s presentation he&rsquo;s talking about some changes to the Ruby client library for Riak. Many of the changes make the Ruby library a proper smart client. Read <a href="https://github.com/basho/riak-ruby-client/wiki/Connecting-to-Riak">this wiki</a> wiki entry under <strong>Connecting to Clusters</strong> for some of the features. It&rsquo;s awesome (especially the transport-related failure handling).</p>

<h1>Client libraries in general</h1>

<p>I want to say a bit about client libraries. Regardless of what they talk to (though I&rsquo;ll be talking specifically about database client libraries), this is something many companies get wrong.</p>

<p>Everyone knows I&rsquo;m not the biggest MongoDB/10Gen fan in the world. I won&rsquo;t go into detail about the technical reasons behind that. Many others have done a much more eloquent dive into that topic.
As much as it&rsquo;s easy to make fun of MongoDB as being an marketing-driven database, they did get one thing right. They owned their client driver availability. Not only did they own and maintain all the drivers but they largely had the same API across the various languages.</p>

<p>Then again, they had to. Other databases/applications offer a REST-ish interface over HTTP (or plain-text interface like Redis) so they can punt a bit. Got a libcurl port for your language? You&rsquo;re set. MongoDB has its own protocol and that god-forsaken BSON shitshow.</p>

<p>One of the benefits, however, of a plain-text or HTTP-based protocol is that it&rsquo;s a pattern we can grok as operators and developers. We load balance webservers. We speak to third-party APIs. It&rsquo;s not the most EFFICIENT but it&rsquo;s a known quantity. It&rsquo;s also, as I said, REALLY fucking easy to add support to your language of choice. No need to FFI some c library or make a binary extension. Any language worth its salt has http client support in stdlib (even if it&rsquo;s as big a pile of dog squeeze as net/http). Again, most languages also have libcurl support for something better.</p>

<h1>Back to smart clients</h1>

<p>I largely dislike applications that require smart clients to get the full benefit. As an operations person, I&rsquo;m USUALLY using a dynamic language like python, ruby or perl to access the system as opposed to directly from the application. This was my biggest gripe with ZooKeeper (as I&rsquo;ve said many times in the past). It&rsquo;s also been one of my points of contention with Datomic. If you aren&rsquo;t on a JVM language, you&rsquo;re shit out of luck for now. Yes, JRuby makes this billions of times easier for those of us using Ruby but Jython is still not where it needs to be for modern Python.</p>

<p>I also had this problem with Voldemort. Disclaimer this is 2-3 year old data from running Voldemort in production. AFAIK, it&rsquo;s still the case. For the sake of this discussion, we&rsquo;re going to ignore data opacity. At the time, the only way to fully access the data in and maintain a Voldemort cluster was from the JVM. I ended up writing quite a bit of JRuby wrapper around StoreClient just to see the data we had in Voldemort.</p>

<p>Riak (and as another example, ElasticSearch) is nice in this regard. It&rsquo;s HTTP. I can curl it from a shell script. I can use the Ruby library Basho is maintaining. If I&rsquo;m using a language without &lsquo;official&rsquo; support, I can write my own. All the metadata is largely attached to http headers and even monitoring is done via the <code>/ping</code> and <code>/stats</code> urls. Something I didn&rsquo;t realize until today (thanks to <a href="https://twitter.com/b6n">Benjamin Black</a>) is that the stats interface actually exposes stuff I had previously glossed over including cluster topology. This is where the meat of the discussion on twitter today happened.</p>

<h1>Operational Happiness</h1>

<p>My original statements on this discussion related to using haproxy in front of your Riak cluster. There are several reasons I prefer this but a quick sidebar</p>

<h2>Seed Nodes</h2>

<p>I&rsquo;ve had some minor operational experience with Cassandra (nothing to write home about) but one of the things that always bothered me was the idea of &lsquo;seed nodes&rsquo;. Let me be clear that Cassandra and Riak are pretty much the only two datastores I&rsquo;d feel comfortable using these days (with the nod going to Riak) in any sort of scalable environment. Postgres has earned its way back to my into my graces but MySQL can <em>insert Louis C.K. euphemism here</em>.</p>

<p>My problem with seed nodes is the idea that I have special nodes. These nodes have to be hard-coded in a config file somewhere or discovered by some other method. I could store them as DNS lookups but now I&rsquo;ve got to deal with TTLs on DNS. And I&rsquo;ve got to deal with the fact that DNS doesn&rsquo;t actually care if the host I&rsquo;ve been given is actually alive or not.</p>

<p>I could store this information in ZooKeeper but what if I don&rsquo;t actually have native ZK support in that database? I&rsquo;ve got to write something that populates ZK when a new node is available and it&rsquo;s not actually a live check. I still have to test that host first. Yes you should do that anyway but it&rsquo;s a valid point.</p>

<p>So if I&rsquo;m storing seed nodes as DNS names in a config file, I can never change those names without either rolling out new code or configs. That might require a restart somewhere. If I&rsquo;m clever, I could probably make that an administrative hook in my application (think JMX) where I can fiddle the seed list. I can poll a config file for changes. I can do a lot of thing but none of them are &ldquo;optimal&rdquo; to me.</p>

<h2>Back to haproxy</h2>

<p>My prefered method of using Riak is to stick everything behind haproxy. There are several reasons for this but here are a few reasons (note we&rsquo;re going to assume use of CM at this point):</p>

<ul>
<li>Operationally, haproxy nodes are easier to manage than application configs (depending on the application).</li>
<li>Many times, at different companies, we&rsquo;ve had to roll our own layer on top of a client library (or even write our own due to licensing issues). Load-balancing is not neccessarily a core application developer competency (and it&rsquo;s bitten me in the ass before).</li>
<li>From an operational perspective, I can bring nodes in and out of service in haproxy for maintenance without needing to inform the client or have it waste cycles with stale node detection. It simply will never talk to an out of service backend.</li>
</ul>


<p>Basically the haproxy crew has already done all the work in load balancing HTTP connections intelligently and they&rsquo;re pretty damn good at it. I love my developers and I know the folks writing the various libraries I use are smart people but again it goes back to core competency. Note that Basho gets a nod here, however, in that I&rsquo;m pretty sure that, what with writing webmachine, they grok http pretty well.</p>

<p>I&rsquo;ve even gone the approach of using haproxy on every system that needs to connect to some backend locally. This totally eliminates the idea of fixed points in the network for connections (at the expense of having to deal with a bit of drift between CM runs). If I wanted to, I could even make multiple riak clusters transparent behind haproxy (though I can only think of a few REALLY specific use cases for that).</p>

<h2>Trade offs</h2>

<p>Yes there are trade-offs to this approach. As Ben pointed out, the Riak stats interface is really powerful from a topology perspective. A REALLY smart riak client can discovery data layout and make deeply intelligent decisions on that.</p>

<p>With other applications like ElasticSearch I can actually become a full fledged cluster member as a non-data node and actually offload some of the work of the cluster for scatter/gather type operations using the Java library.</p>

<p>With haproxy, I don&rsquo;t get those types of benefits.</p>

<h2>What I do get</h2>

<p>Outside of the maintainability of haproxy (which is, again, subjective) I get one benefit I <strong>CAN&rsquo;T</strong> get with smart clients that is, unfortunately, neccessary with many customers - a more narrow network allowance.</p>

<p>Enterprises are interesting beasts and still think in terms of traditional tiered application stacks. Nothing wrong with that and it does have SOME security benefits but many deployments I&rsquo;ve been involved with have official policies that &lsquo;data tiers&rsquo; (whatever the hell those are) must be protected in a dedicated network behind an additional firewall. So here&rsquo;s Riak, a &lsquo;data tier&rsquo;, that we have to have with as small an ingress as possible. That rules out smart clients of the toplogy-aware variety. So we stick haproxy in the mix. We tell them to use a load balancer. Some folks use an internal F5 HA pair with some sort of VRRP. We&rsquo;ll set up haproxy + keepalived or some other combination depending.</p>

<h1>TMTOWTDI</h1>

<p>One of the things that Riak allows is this level of flexibility. I can use HTTP and haproxy. I can use HTTP and smart client. I can use protobufs and haproxy or a smart client. It&rsquo;s really that flexible. I happen to prefer the haproxy approach for reasons I&rsquo;ve already mentioned but I totally grok that some folks want a more intelligent client approach. Some folks would argue that there&rsquo;s a right way and a wrong way but I don&rsquo;t see it like that. What I see is a datastore that, just like it letting me control the consistency levels I want, let&rsquo;s me control HOW I access that data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Future of Noah]]></title>
    <link href="http://lusis.github.com/blog/2013/01/20/future-of-noah/"/>
    <updated>2013-01-20T21:15:00-05:00</updated>
    <id>http://lusis.github.com/blog/2013/01/20/future-of-noah</id>
    <content type="html"><![CDATA[<p>This is probably the most difficult blog post I&rsquo;ve had to write. What&rsquo;s worse is I&rsquo;ve been sitting on it for months.</p>

<!-- more -->


<p>When I started Noah a few years ago, I had a head full of steam. I had some grand ideas but was trying to keep things realistic. I simply wanted a simple REST-ish interface for stashing nuggets of information between systems and a flexible way to notify interested parties when that information changed.</p>

<p>It started as a <a href="https://raw.github.com/lusis/Noah/8a2e193c043ab30cce17d7ada25ef33b72baa73e/doc/noah-mindmap-original.png">mindmap</a> laying in bed one night. It was my first serious project and I had no idea what I was getting in to. If you&rsquo;re curious, you can read quite a bit of my initial braindumps on the <a href="https://github.com/lusis/Noah/wiki">wiki under &lsquo;General Thoughts&rsquo;</a>. I watched every day as more and more people started following the project.</p>

<p>It was a game changer for me in many ways. Working on Noah was fun and it was rewarding in more ways than one. But real life gets in the way sometimes.</p>

<h1>On stewardship</h1>

<p>One of the things I&rsquo;ve learned over the past few years is that for opensource to REALLY thrive, it can&rsquo;t be a one-person show. I&rsquo;ve been involved with opensource for most of my 17+ year career. You think I would have learned that lesson before now.</p>

<p>Stewardship is a hard thing. Our arrogance and pride makes us want to keep things close to our chest.</p>

<ul>
<li>&ldquo;I just want to get to a 1.0 release&rdquo;</li>
<li>&ldquo;Things are too in flux right now. It wouldn&rsquo;t be fair to bring others in&rdquo;</li>
<li>&ldquo;I don&rsquo;t quite trust anyone else with it yet&rdquo;</li>
<li>&ldquo;Let me just get this ONE part of the API in place first..&rdquo;</li>
</ul>


<p>These are all things I said to myself.</p>

<p>What really changed my mind was a few things. Being involved in the Padrino project. Seeing the Fog community grow after Wesley started allowing committers. Seeing Jordan trust me enough to make me a logstash committer before his daughter was born. The biggest trigger was actually one of my own projects - the chef logstash cookbook.</p>

<p>Bryan Berry (FSM bless him) pestered the hell out of me about getting some changes merged in. He was making neccessary changes and fixes. He was evolving it to make it more flexible beyond my own use case. I don&rsquo;t recall if he asked to be a committer but I gave it to him. The pull request queue drained and he added more than I ever had time for. Not long after, I added Chris Lundquist. Those two have been running it since then really.</p>

<p>I think back to when I got added to the committers for Padrino. It was a rush. It was amazing and scary. Above all it was the encouragement I needed. How dare I deny someone else that same opportunity.</p>

<p>Making that first pull request is hard. To have it accepted is a feeling I&rsquo;ll keep with me for a long time. I can only hope that some project I create some day will give someone that same confidence and feeling.</p>

<h1>So what about Noah</h1>

<p>Noah is in the same place Logstash was. I&rsquo;m not using it and that&rsquo;s really hurting it more than anything. It&rsquo;s time to let someone who IS using it take control. I care too much about it to watch it die on the vine. I still believe in what it was designed to do and every single day I get emails asking me if it&rsquo;s still alive because it&rsquo;s a perfect fit for what someone needs. The same stuff is STILL coming up on various mailing lists and Noah is a perfect fit. There are companies actively using it even it the current unloved state. Those folks have a vested interest in it.</p>

<p>When I added Chris and Bryan to the cookbook, I sent them an email with what my vision was for the cookbook. I can&rsquo;t find that email now but I recall only had two real requirements:</p>

<ul>
<li>Out of the box, it would work on a single system with no additional configuration (i.e. add the cookbook to a run_list and logstash would work automatically)</li>
<li>A user never had to modify the cookbook to change anything related to roles (i.e. allow the attributes to drive search for discovering your indexer - hence all the role stuff in the attrs now)</li>
</ul>


<p>I need to do the same thing for Noah and see where it leads.</p>

<h1>Dat list</h1>

<p>This list isn&rsquo;t comprehensive but I think it hits the key points.</p>

<h2>Simple</h2>

<p>Noah should be simple to interact with. It was born out of frustration with trying to interact with ZooKeeper. Nothing is more simple than being able to use <code>curl</code> IMHO. I can use Noah in shell scripts and I can use it in Java (we had a Spring Configurator at VA that talked to Noah. It was awesome). You should always be able to use <code>curl</code> to interact with Noah. I wish I could find it now but someone once brought up Noah on the ZK mailing list. This led to various rants about how it didn&rsquo;t do consensus and a bunch of other stuff that ZK did. One of the Yahoo guys (I wish I could remember who) said something in favor of Noah that stuck with me:</p>

<p><em>Interfaces matter</em></p>

<p>I know I&rsquo;m on the right track here because Rackspace just built a product that provides an HTTP interface to ZK. Oh and it does callbacks.</p>

<h2>Friendly to both sysadmins and developers</h2>

<p>Simplicity plays into this but I wanted Noah to be the tool that solved some friction between the people who write the code and the people who run the code. Configuration is all over the place in any modern stack. Configuration management has come into its own. People are using it but you still see disconnects. Where should this config be maintained? What&rsquo;s the best way to have puppet track changes to application configuration? I can&rsquo;t get my developers to update the ERB templates in the Chef cookbook. All of these things are where Noah is helpful.</p>

<p>I still stand by the statement that <a href="http://lusislog.blogspot.com/2011/03/ad-hoc-configuration-coordination-and.html">not all configuration is equal</a>. Volatility is a thing and it doesn&rsquo;t have to mean the end of all the effort in moving to a CM tool. I wanted to remove that friction point.</p>

<p>I was also immensely inspired by Kelsey Hightower here. I&rsquo;ve told the story several times of how Kelsey got so frustrated that the developers wouldn&rsquo;t cooperate with us on Puppet and config files for our applications that he learned enough Java to write a library for looking up information in Cobbler. Cobbler has an XMLRPC api and that was simple enough that he could port his python skills to java and write the fucking library himself. I wanted Noah to be friendly enough that a sysadmin could do what Kelsey did.</p>

<h2>Watches and Callbacks</h2>

<p>I&rsquo;ve said this before but one of the most awesome things that ZK has is watches. They have pitfalls (reregister your watches after they fire for instance) but they&rsquo;re awesome. Noah&rsquo;s callback system is the thing that needs the most love (it works but the plugin API was never finalized). It&rsquo;s also one of the most powerful parts that meets the needs of folks that I see posting on various mailing lists.</p>

<p>The idea is simple. When something changes in Noah, you should be able to fire off a message however the end-user wants to get it. I think this is one of the reasons I love working on Logstash so much. Writing plugins is so simple and it&rsquo;s the gateway drug to anyone who wants to contribute to logstash.</p>

<h1>Things I don&rsquo;t care about</h1>

<p>What don&rsquo;t I care about?</p>

<h2>Language</h2>

<p>I don&rsquo;t care about the language it&rsquo;s written in. If someone wants to take it and convert it to Python or Erlang or Clojure, be my guest. I just want the ideas to live on somehwere. In fact, I&rsquo;ve rewritten various parts of Noah over the last year privately. Not just experimenting with moving from EM to Celluloid but as a Cherry.py app, in Clojure and I even started an Erlang attempt (except that I know almost NO Erlang so it didn&rsquo;t get very far).</p>

<h2>Name</h2>

<p>Honestly I don&rsquo;t even care about the name. Yeah it&rsquo;s witty and fits with the idea of ZooKeeper but I have no qualms about adding a link to your project from the Noah readme and recommending people use it instead.</p>

<h2>Paxos/ZAB</h2>

<p>This was never a requirement for Noah. Noah was specifically designed for certain types of information. If you need that, use the right tool.</p>

<h2>Persistence</h2>

<p>Let&rsquo;s be honest. From a simplicity standpoint, it doesn&rsquo;t get much simpler than Redis. It&rsquo;s one of the reasons we changed the default logstash tutorial to use Redis instead of RabbitMQ. I know Redis reinvents a lot of wheels that have already been solved but it, along with ElasticSearch, are one of the lowest friction bits of software I&rsquo;ve dealt with in a long time. Not having external dependencies is a godsend for getting started.</p>

<p>However I&rsquo;ve also got small experiments privately where I used ZMQ internally and sqlite. I&rsquo;ve written a git-based persistence for it too.</p>

<p>Riak is also a great fit for Noah and takes care of the availability issue on the persistence side. More on Riak in a sec.</p>

<h1>So that&rsquo;s it</h1>

<p>That&rsquo;s really all that matters. If you want to take ownership of the project, contact me. Let me know and we&rsquo;ll talk. Who knows. Maybe I&rsquo;m overestimating the level of interest. Maybe ZK isn&rsquo;t as unapproachable to people anymore. The language bindings have certainly gotten much better. I just want the project to be useful to folks and I&rsquo;m getting in the way of that.</p>

<h1>What are the other options?</h1>

<p>I don&rsquo;t know of many other options out there. Doozer is picking up steam again as I understand it and it has a much smaller footprint than ZK does. There was a python project that did a subset of Noah but I can&rsquo;t find it now.</p>

<p>One thing that is worth considering is a project that I found earlier today - <a href="https://github.com/cocagne/zpax">zpax</a>. While this is just a framework experiment of sorts, it could inspire you to add your own frontend to it. The same author is also working on DTLS on top of ZMQ.</p>

<p>I&rsquo;ve thought about ways I could actually do this with Logstash plugins. It&rsquo;s doable but not really feasible without making Logstash do something it isn&rsquo;t shaped for.</p>

<p>Another idea that I&rsquo;m actually toying around with is simply using Riak plus a ZeroMQ post-commit hook so that plugins could be written in a simpler way. <a href="https://github.com/seancribbs/riak_zmq">Sean Cribbs already took the idea and made a POC 2 years ago</a> based on a gist from Cody Soyland. You wouldn&rsquo;t have the same API up front as Noah but you could stub that out in some framework and also have it be the recipient of the ZMQ publishes.</p>

<p>Finally you could just use ZooKeeper. Yes it has MUCH greater overhead but you DO get a lot more bang for the buck. There really isn&rsquo;t anything in the opensource world right now that compares. It also provides additional features that I never really cared about or needed in Noah.</p>

<h1>Wrap up</h1>

<p>I&rsquo;m not done in this space. I don&rsquo;t know where I&rsquo;m going next with it. Maybe I&rsquo;ll start from scratch with a much simpler API. Maybe I&rsquo;ll just run with the Riak idea.</p>

<p>I just want to give a shoutout to the countless people who helped me evangelize Noah over the last few years. It was recommended on mailing lists, twitter and many other places. It meant a lot to me and I only hope that someone will take up the mantle and make it something you would recommend again.</p>

<p>For those of you still using Noah, I hope we can find a home for it so that it can continue to provide value to you.</p>

<p>Thanks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How We Vagrant]]></title>
    <link href="http://lusis.github.com/blog/2012/12/17/how-we-vagrant/"/>
    <updated>2012-12-17T22:10:00-05:00</updated>
    <id>http://lusis.github.com/blog/2012/12/17/how-we-vagrant</id>
    <content type="html"><![CDATA[<p>People may or may not have noticed but I&rsquo;ve been largely offline for the past 4 weeks or so. This is because I&rsquo;ve been in the middle of a pretty heavy redesign of a few key parts of our application stack. This also required me to learn Java so I&rsquo;ve been doubly slammed.</p>

<p>As part of that redesign, I worked on what we lovingly refer to internally as the &ldquo;solo installer&rdquo;. I gave a bit of background on this in a post to the Chef mailing list at one point but I&rsquo;ll go over it again as part of this post.</p>

<!-- more -->


<h1>Beginnings</h1>

<p>To understand why this is something of a departure for us, it&rsquo;s worth understanding from whence we came. enStratus, like most hosted solutions, has experienced largely organic growth. One of the nice things about a SaaS product is that you have the freedom to experiment to some degree. You might be in the middle of a MySQL to Riak migration and still need two data stores for the time being. You might be in the process of changing how some background process works so you&rsquo;ve got an older designed system running along side the newer system which is only doing a subset of the work.</p>

<p>With a hosted platform these kinds of things are hidden from the end-user to some degree. They don&rsquo;t know what&rsquo;s going on behind the scenes and they really don&rsquo;t care as long as you&rsquo;re doing X, Y and Z that you&rsquo;re being paid to do.</p>

<p>Now for those of you running/developing/managing some sort of SaaS/Hosted solution I want you to take a journey with me. Imagine that tomorrow someone walked into your office and said:</p>

<blockquote><p>We need to take our production environment and make it so that a customer can run it on-premise. Oh and it has to be able to run entirely isolated and can't always talk back to any external service.</p></blockquote>


<p>That&rsquo;s pretty much the place enStratus found itself. enStratus is a cloud management platform. Not all clouds are public. Maybe someone wants to use your service but has regulatory needs that prevent them from using it as is. Maybe it needs to run entirely isolated from the rest of the world. There are valid reasons for all of these despite my general attitude towards security theater in the enterprise.</p>

<p>Now you&rsquo;ve got an interesting laundry list in front of you:</p>

<ul>
<li>How do you teach someone to manage this organic &ldquo;thing&rdquo; you&rsquo;ve built?</li>
<li>How do you take not one application but an entire company and its stack and shove it in a box?</li>
<li>Do you wrap up all the external components (monitoring, file servers, access control) and deal with those?</li>
</ul>


<p>We aren&rsquo;t the first company to do this and we won&rsquo;t be the last. Take a look at Github. They offer a private version of Github. But we&rsquo;re not just talking about one part of Github - e.g. Gist. We&rsquo;re talking about the entire stack the company runs to provide Github as we know it.</p>

<p>Unless you design with this in mind, you can&rsquo;t really begin to understand how difficult of a task this can be. As I understand it, Github finally went the appliance route and offer prefab vms with some setup glue. Please correct me if I&rsquo;m wrong here.</p>

<h1>Early iterations</h1>

<p>Obviously you can see that this was/is a daunting task. Original versions of our install process were based around a collection of shell scripts. Because of certain details of the stack (such as encryption keys for our key management system), we had to maintain state between each component of the stack when it was installed. Currently there are roughly 7 core components/services that make up enStratus:</p>

<ul>
<li>The console</li>
<li>The api endpoint</li>
<li>The key management subsystem</li>
<li>The provisioning subsystem</li>
<li>The directory integration service</li>
<li>The &ldquo;worker&rdquo; system</li>
<li>The &ldquo;monitor&rdquo; system</li>
</ul>


<p>and those are just the enStratus components. You also need RabbitMQ, MySQL and Riak (as we&rsquo;re currently transitioning from MySQL to Riak). All of these things largely talk to each other over some web service or via RabbitMQ. With one or two exceptions, they can all be loadbalanced in an active/active type of configuration and scaled horizontally simply by adding an additional &ldquo;whatever&rdquo; node.</p>

<p>So the original installation process was a set of shell scripts that persisted some state and this &ldquo;state&rdquo; file had to be copied between systems. Yes, we could use some sort of external configuration store but that&rsquo;s another component that we would have to install just to do the installation.</p>

<h1>Phase two</h1>

<p>One of my coworkers, <a href="https://twitter.com/zomgreg">Greg Moselle</a> was sort of &ldquo;sysadmin number one&rdquo; at enStratus. This was in addition to his duties as managing all customer installs. So he did what most of us would do and brute forced a workable solution with the original shell scripts. As enStratus started to offer Chef and Puppet support in the product, Greg gets this wild hair up his ass and thinks:</p>

<blockquote><p>I wonder if I can rewrite these shell scripts into something a bit more cross-platform and idempotent using chef-solo.</p></blockquote>


<p>You might be thinking the same thing I originally did that this was largely a bad idea. In my mind we had a workable solution for the interim in the existing shell scripts that had the install of enStratus down to a day or so. Pragmatism right? It&rsquo;s also worth noting that this was how he wanted to learn Chef&hellip;</p>

<p>So off he goes and does what I recommend any new Puppet or Chef user does - exec resources all over the fucking place. Wrap your shell scripts in <code>exec</code>. Hardcode all the fucking things.</p>

<p>Once he did this, then I started working with him on some basic attributes to make them a bit more flexible. Before too long we had a stack of roles matching different components and we had moved everything from <code>cookbook_file</code> to <code>remote_file</code>. It was still a mess of execs but it worked.</p>

<p>But we still had this &ldquo;state&rdquo; we had to maintain between runs. This is not going away anytime soon. In production we store this state in attributes and use chef-server. We didn&rsquo;t have that luxury here.</p>

<p>Then <a href="https://twitter.com/jimsander">Jim Sander</a> drops in and writes a small setup script that maintains some of that state for us. Basically a wrapper around raw <code>chef-solo</code>. Side note, if you ever need someone to drop some shell scripting knowledge on your ass, Jim&rsquo;s the man to see. Ask him about his Tivoli days to really piss him off.</p>

<p>At this point, I start working on cleaning up the recipes as a sort of tutorial for folks. I&rsquo;d pick a particular recipe and refactor it to all native resources and make it data driven. I&rsquo;d commit these in small chunks so folks could easily see what the differences were easily - stuff like &ldquo;instead of execing to call rpm, we&rsquo;ll use the yum provider&rdquo;.</p>

<p>At this point we&rsquo;ve got something pretty far evolved from where we were. Now that we&rsquo;ve got this workable chef-solo repository, I decide to hack out a quick Vagrantfile. The problem was it wasn&rsquo;t entirely idempotent and we still had some manual steps that had to be dealt with. In addition to finishing up the recipes and ended up rewriting large chunks of the setup script. Now that I had something largely repeatable and localized, we suddenly had a Vagrant setup that folks could use for development. It wasn&rsquo;t fully automated but it worked. We also still had this shared state thing.</p>

<p>So I set out to refactor the setup script a bit more. What&rsquo;s important to keep in mind is that the primary use-case for this chef-solo repository wasn&rsquo;t for Vagrant. This is our &ldquo;installer&rdquo;. The interesting part to me is that the improvements to how we do on-premise installs are coming as a direct result of making this work better with Vagrant. There&rsquo;s a lot of wrapper work tied up in the setup script that wouldn&rsquo;t need to be done if we used a base box that had more stuff baked in. However not baking stuff in actually gives us a more real-world scenario for installation.</p>

<p>Additionally we needed to be able to somehow pass user-specific configuration settings into the <code>vagrant up</code> process and get those into <code>chef-solo</code> by way of the setup wrapper. We have things like license keys, hostnames and my personal hated favorite - database credentials - that need to be handled in a way that we can make it so a developer can just type <code>vagrant up</code> and be running. If I have to require someone to edit a json file or anything else, the whole thing will fall flat on its face.</p>

<p>So any time we needed something like that, we added support to the setup wrapper and then used environment variables to pass that information in to vagrant.</p>

<h1>So how do we vagrant?</h1>

<p>We leverage environment variables pretty heavily in our Vagrantfile. If it&rsquo;s something that someone might need to tune for whatever reason, it&rsquo;s an environment variable that triggers an option to our setup script.</p>

<h2>Current list of tunables</h2>

<p>This is just a subset of the tunables we control via environment variables. The majority of these map directly to options for the setup script:</p>

<ul>
<li><code>ES_DLPASS</code> and <code>ES_LICENSE</code>: the basic set of credentials needed to fetch our assets and your personal license key.</li>
<li><code>ES_MEM</code>: this is the result of some of our front end developers having less memory than others.</li>
<li><code>ES_CACHE</code>: We have an office in New Zealand and bandwidth there is &ldquo;challenging&rdquo;. This allows us to cache as much as possible between calls to <code>vagrant up</code>. This not only triggers caching of system packages downloaded but also triggers the <code>prefetch</code> option in our setup script that predownloads all the assets. These assets are all stored in the <code>cache</code> directory of the repository which is not coincidently the value of <code>file_cache_path</code> in chef-solo. Remember that we may not always have external network access during installation so we offer a way to warm the <code>cache</code> directory with as many assets as possible.</li>
<li><code>ES_BOX</code>: let&rsquo;s you specify an alternate base box to use. This is how we test the installer on different distros.</li>
<li><code>ES_DEVDIR</code>: shares an additional directory from the host to the vagrant image. This is how development is done (at least for me). I map this to the root of all of my git repository checkouts.</li>
<li><code>ES_VAGRANT_NW</code>: Allows you to configure bridged networking in addition to the host-only network we use.</li>
<li><code>ES_PROFILE</code>: This directly maps to an option in our setup script for persisting state between runs.</li>
</ul>


<p>There are other options that are specific to the enStratus product as well but you get the idea.</p>

<h2>The setup script</h2>

<p>I can&rsquo;t post the full thing here but I can give you a general idea of how it works and some of the options it supports. This is a &ldquo;sanitized&rdquo; truncated version of the help output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: setup.sh [-h] [-e] [-f] -p &lt;download password&gt; -l &lt;license key&gt; [-s savename] [-c &lt;console hostname&gt;] [-n &lt;number of nodes&gt;] [-m &lt;mapping string&gt;] [-a &lt;optional sourceCidr string&gt;]
</span><span class='line'>-------------------------------------------------------------------------
</span><span class='line'>-p: The password for downloading enStratus
</span><span class='line'>-l: The license key for enStratus
</span><span class='line'>
</span><span class='line'>For most single node installations, specify the download password and license key.
</span><span class='line'>
</span><span class='line'>optional arguments
</span><span class='line'>------------------
</span><span class='line'>-h: This text
</span><span class='line'>-e: extended help
</span><span class='line'>-f: fetch-only mode. Downloads and caches *MOST* assets. Requires download password and *WILL* install chef
</span><span class='line'>-c: Alternate hostname to use for the console. [e.g. cloud.mycompany.com] (default: fqdn of console node)
</span><span class='line'>-a: Alternate string to use for the sourceCidr entry. You know if you need this.
</span><span class='line'>-s: A name to identify this installation
</span><span class='line'>-n: Number of nodes in installation [1,2,4] (default: 1)
</span><span class='line'>-m: Mapping string [e.g. frontend:192.168.1.1,backend:backend.mydomain.com]
</span><span class='line'>
</span><span class='line'>About savename:
</span><span class='line'>---------------
</span><span class='line'>Savename is a way to persist settings between runs of enStratus.
</span><span class='line'>If you specify a save name, a directory will be created under local_settings
</span><span class='line'>will be created. It will contain a YAML file with your settings as well 4 JSON files.
</span><span class='line'>
</span><span class='line'>The YAML file is the source of truth for the named installation. The JSON files MAY
</span><span class='line'>be recreated if the contents of the YAML file change. They exist to migrate between systems.
</span><span class='line'>If a save file is found, no other arguments are honored. If you need to change the 
</span><span class='line'>download password or license key, please update the YAML file itself
</span><span class='line'>
</span><span class='line'>If you lose this YAML file you will not be able to recover this enStratus installation.
</span><span class='line'>You should save it somewhere secure and optionally version it.</span></code></pre></td></tr></table></div></figure>


<h3>Persisting settings</h3>

<p>One of the &ldquo;gotchas&rdquo; we have is how do we basically build a node JSON file for chef-solo to use with any information we need to persist. Since we don&rsquo;t know the state of all the systems involved when we go in, we have to &ldquo;punt&rdquo; on a few things. What we end up doing is something we call the <code>savename</code>. If you use this option, the settings you define will be persisted to a directory that git ignores called <code>local_settings</code>. This directory will contain directories named after the above <code>savename</code> parameter. The setup script (written for now in bash) will create a yaml file (easy to do in bash with HEREDOC as opposed to JSON) and also a copy of the generated encryption keys in a plain text file for the customer to store.</p>

<p>The only thing we can count on being on the system up front is the Chef omnibus install (since that&rsquo;s a requirement). Instead of complicating things with ruby at this point (and chicken/egg issues since the setup script actually installs chef omnibus), we use the <code>erubis</code> binary that gets installed with omnibus to pass the yaml to to a JSON erb template. That generated JSON is the node json with attribute overrides. We actually support multi-node installation in the setup script if you provide a mapping of where certain components are running when calling setup. If you rerun setup using an existing <code>savename</code> parameter, the yaml file is updated (only certain values) and then regenerate the JSON file.</p>

<h1>The upshot</h1>

<p>The best part of all of this is that we can now say the same process is used when installing enStratus locally in Vagrant, in our dev, staging and production environments (though production uses chef-server) as well as what we install on the customer&rsquo;s site. We version this repository around static points in our release cycle. We branch for a new release and create tags at given points in the branch based on either a patch release for enStratus itself in that release OR a patch to the installer itself.</p>

<p>It&rsquo;s not all unicorns pooping rainbows. The process is much more complicated than it needs to be but it&rsquo;s almost a world of difference from where it was when I started and it was entirely a team effort. This setup allowed us to do full testing to switch entirely off the SunJDK (and the need to manually download the JCE during customer installs) onto OpenJDK. We were able to migrate from Tomcat to Jetty and refactor our build process using this method. I was able to do this work without affecting anyone else. All I had to do when we were ready for full testing was tell everyone to switch branches, run <code>vagrant up</code> and test away.</p>

<h1>Special thanks</h1>

<p>I want to give a serious shout-out to Mitchell Hashimoto and John Bender for the work they did with Vagrant. Last year I said that no two software products impacted my career more than ElasticSearch and ZeroMQ. This year, without a doubt, Vagrant is at the top of that list.</p>

<h1>Addendum</h1>

<p>What follows is the sanitized version of our <code>Vagrantfile</code>. If anyone has any suggestions, I&rsquo;m all ears:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="no">Vagrant</span><span class="o">::</span><span class="no">Config</span><span class="o">.</span><span class="n">run</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_BOX&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span> <span class="o">=</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_BOX&#39;</span><span class="o">]</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span> <span class="o">=</span> <span class="s2">&quot;es-dev&quot;</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box_url</span> <span class="o">=</span> <span class="s2">&quot;https://opscode-vm.s3.amazonaws.com/vagrant/boxes/opscode-ubuntu-12.04.box&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_VAGRANT_NW&#39;</span><span class="o">]</span> <span class="o">==</span> <span class="s2">&quot;bridged&quot;</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">network</span> <span class="ss">:bridged</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="c1"># If you change this address, the conditional logic</span>
</span><span class='line'>    <span class="c1"># in console.rb will break</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">network</span> <span class="ss">:hostonly</span><span class="p">,</span> <span class="s2">&quot;172.16.129.19&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1"># These entries allow you to run code locally and talk to a</span>
</span><span class='line'>  <span class="c1"># &quot;working set&quot; of data services</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">15000</span><span class="p">,</span> <span class="mi">15000</span>   <span class="c1"># api</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">3302</span><span class="p">,</span> <span class="mi">3302</span>     <span class="c1"># dispatcher</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">2013</span><span class="p">,</span> <span class="mi">2013</span>     <span class="c1"># km</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">5672</span><span class="p">,</span> <span class="mi">5672</span>     <span class="c1"># RabbitMQ (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">8098</span><span class="p">,</span> <span class="mi">8098</span>     <span class="c1"># Riak HTTP (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">8097</span><span class="p">,</span> <span class="mi">8097</span>     <span class="c1"># Riak protobuf (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">3306</span><span class="p">,</span> <span class="mi">3306</span>     <span class="c1"># MySQL (autostarts)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">forward_port</span> <span class="mi">55672</span><span class="p">,</span> <span class="mi">55672</span>   <span class="c1"># RabbitMQ management interface</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_MEM&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">customize</span> <span class="o">[</span><span class="s2">&quot;modifyvm&quot;</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">&quot;--memory&quot;</span><span class="p">,</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_MEM&#39;</span><span class="o">]]</span>
</span><span class='line'>  <span class="k">else</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">customize</span>  <span class="o">[</span><span class="s2">&quot;modifyvm&quot;</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">&quot;--memory&quot;</span><span class="p">,</span> <span class="mi">8192</span><span class="o">]</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DEVDIR&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">share_folder</span> <span class="s2">&quot;es-dev-data&quot;</span><span class="p">,</span> <span class="s2">&quot;/es_dev_data&quot;</span><span class="p">,</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DEVDIR&#39;</span><span class="o">]</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_CACHE&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="nb">puts</span> <span class="s2">&quot;Shared cache enabled&quot;</span>
</span><span class='line'>    <span class="no">FileUtils</span><span class="o">.</span><span class="n">mkdir_p</span><span class="p">(</span><span class="no">File</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span><span class="s2">&quot;apt&quot;</span><span class="p">,</span><span class="s2">&quot;partial&quot;</span><span class="p">))</span> <span class="k">unless</span> <span class="no">Dir</span><span class="o">.</span><span class="n">exists?</span><span class="p">(</span><span class="no">File</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span><span class="s2">&quot;apt&quot;</span><span class="p">,</span> <span class="s2">&quot;partial&quot;</span><span class="p">))</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">share_folder</span><span class="p">(</span><span class="s2">&quot;apt&quot;</span><span class="p">,</span> <span class="s2">&quot;/var/cache/apt/archives&quot;</span><span class="p">,</span> <span class="s2">&quot;cache/apt&quot;</span><span class="p">)</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">provision</span> <span class="ss">:shell</span> <span class="k">do</span> <span class="o">|</span><span class="n">shell</span><span class="o">|</span>
</span><span class='line'>    <span class="no">ES_LICENSE</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_LICENSE&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_DLPASS</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DLPASS&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_PROFILE</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_PROFILE&#39;</span><span class="o">]</span> <span class="o">||</span> <span class="s2">&quot;vagrant-</span><span class="si">#{</span><span class="no">Time</span><span class="o">.</span><span class="n">now</span><span class="o">.</span><span class="n">to_i</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="no">ES_LICENSE</span><span class="o">.</span><span class="n">nil?</span> <span class="ow">or</span> <span class="no">ES_DLPASS</span><span class="o">.</span><span class="n">nil?</span>
</span><span class='line'>      <span class="nb">puts</span> <span class="s2">&quot;You must set the environment variables: ES_LICENSE and ES_DLPASS!&quot;</span>
</span><span class='line'>      <span class="nb">exit</span> <span class="mi">1</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>    <span class="no">ES_CLOUD</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_CLOUD&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_CIDR</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_CIDR&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="no">ES_DEBUG</span><span class="o">=</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_DEBUG&#39;</span><span class="o">]</span> <span class="o">||</span> <span class="kp">false</span>
</span><span class='line'>    <span class="n">setup_opts</span> <span class="o">=</span> <span class="s2">&quot;-l </span><span class="si">#{</span><span class="no">ES_LICENSE</span><span class="si">}</span><span class="s2"> -p </span><span class="si">#{</span><span class="no">ES_DLPASS</span><span class="si">}</span><span class="s2"> -s </span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2"> &quot;</span>
</span><span class='line'>    <span class="n">setup_opts</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;-c </span><span class="si">#{</span><span class="no">ES_CLOUD</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="no">ES_CLOUD</span>
</span><span class='line'>    <span class="n">setup_opts</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;-a </span><span class="si">#{</span><span class="no">ES_CIDR</span><span class="si">}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="no">ES_CIDR</span>
</span><span class='line'>    <span class="no">ES_DEBUG</span> <span class="p">?</span> <span class="n">chef_opts</span><span class="o">=</span><span class="s2">&quot;-l debug -L local_settings/</span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2">/chef-run.log&quot;</span> <span class="p">:</span> <span class="s2">&quot;&quot;</span>
</span><span class='line'>    <span class="n">shell</span><span class="o">.</span><span class="n">inline</span> <span class="o">=</span> <span class="s2">&quot;cd /vagrant; ./setup.sh </span><span class="si">#{</span><span class="n">setup_opts</span><span class="si">}</span><span class="s2">; chef-solo -j local_settings/</span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2">/single_node.json -c solo.rb </span><span class="si">#{</span><span class="n">chef_opts</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>  <span class="k">if</span> <span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_POSTRUN&#39;</span><span class="o">]</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">provision</span> <span class="ss">:shell</span> <span class="k">do</span> <span class="o">|</span><span class="n">shell</span><span class="o">|</span>
</span><span class='line'>      <span class="n">shell</span><span class="o">.</span><span class="n">inline</span> <span class="o">=</span> <span class="s2">&quot;chef-solo -j /vagrant/local_settings/</span><span class="si">#{</span><span class="no">ES_PROFILE</span><span class="si">}</span><span class="s2">/single_node.json -c /vagrant/solo.rb -o </span><span class="se">\&quot;</span><span class="si">#{</span><span class="no">ENV</span><span class="o">[</span><span class="s1">&#39;ES_POSTRUN&#39;</span><span class="o">]</span><span class="si">}</span><span class="se">\&quot;</span><span class="s2">&quot;</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
</feed>
