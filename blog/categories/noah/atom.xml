<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: noah | blog dot lusis]]></title>
  <link href="http://lusis.github.com/blog/categories/noah/atom.xml" rel="self"/>
  <link href="http://lusis.github.com/"/>
  <updated>2014-06-14T00:52:22-04:00</updated>
  <id>http://lusis.github.com/</id>
  <author>
    <name><![CDATA[John E. Vincent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Future of Noah]]></title>
    <link href="http://lusis.github.com/blog/2013/01/20/future-of-noah/"/>
    <updated>2013-01-20T21:15:00-05:00</updated>
    <id>http://lusis.github.com/blog/2013/01/20/future-of-noah</id>
    <content type="html"><![CDATA[<p>This is probably the most difficult blog post I've had to write. What's worse is I've been sitting on it for months.</p>

<!-- more -->


<p>When I started Noah a few years ago, I had a head full of steam. I had some grand ideas but was trying to keep things realistic. I simply wanted a simple REST-ish interface for stashing nuggets of information between systems and a flexible way to notify interested parties when that information changed.</p>

<p>It started as a <a href="https://raw.github.com/lusis/Noah/8a2e193c043ab30cce17d7ada25ef33b72baa73e/doc/noah-mindmap-original.png">mindmap</a> laying in bed one night. It was my first serious project and I had no idea what I was getting in to. If you're curious, you can read quite a bit of my initial braindumps on the <a href="https://github.com/lusis/Noah/wiki">wiki under 'General Thoughts'</a>. I watched every day as more and more people started following the project.</p>

<p>It was a game changer for me in many ways. Working on Noah was fun and it was rewarding in more ways than one. But real life gets in the way sometimes.</p>

<h1>On stewardship</h1>

<p>One of the things I've learned over the past few years is that for opensource to REALLY thrive, it can't be a one-person show. I've been involved with opensource for most of my 17+ year career. You think I would have learned that lesson before now.</p>

<p>Stewardship is a hard thing. Our arrogance and pride makes us want to keep things close to our chest.</p>

<ul>
<li>"I just want to get to a 1.0 release"</li>
<li>"Things are too in flux right now. It wouldn't be fair to bring others in"</li>
<li>"I don't quite trust anyone else with it yet"</li>
<li>"Let me just get this ONE part of the API in place first.."</li>
</ul>


<p>These are all things I said to myself.</p>

<p>What really changed my mind was a few things. Being involved in the Padrino project. Seeing the Fog community grow after Wesley started allowing committers. Seeing Jordan trust me enough to make me a logstash committer before his daughter was born. The biggest trigger was actually one of my own projects - the chef logstash cookbook.</p>

<p>Bryan Berry (FSM bless him) pestered the hell out of me about getting some changes merged in. He was making neccessary changes and fixes. He was evolving it to make it more flexible beyond my own use case. I don't recall if he asked to be a committer but I gave it to him. The pull request queue drained and he added more than I ever had time for. Not long after, I added Chris Lundquist. Those two have been running it since then really.</p>

<p>I think back to when I got added to the committers for Padrino. It was a rush. It was amazing and scary. Above all it was the encouragement I needed. How dare I deny someone else that same opportunity.</p>

<p>Making that first pull request is hard. To have it accepted is a feeling I'll keep with me for a long time. I can only hope that some project I create some day will give someone that same confidence and feeling.</p>

<h1>So what about Noah</h1>

<p>Noah is in the same place Logstash was. I'm not using it and that's really hurting it more than anything. It's time to let someone who IS using it take control. I care too much about it to watch it die on the vine. I still believe in what it was designed to do and every single day I get emails asking me if it's still alive because it's a perfect fit for what someone needs. The same stuff is STILL coming up on various mailing lists and Noah is a perfect fit. There are companies actively using it even it the current unloved state. Those folks have a vested interest in it.</p>

<p>When I added Chris and Bryan to the cookbook, I sent them an email with what my vision was for the cookbook. I can't find that email now but I recall only had two real requirements:</p>

<ul>
<li>Out of the box, it would work on a single system with no additional configuration (i.e. add the cookbook to a run_list and logstash would work automatically)</li>
<li>A user never had to modify the cookbook to change anything related to roles (i.e. allow the attributes to drive search for discovering your indexer - hence all the role stuff in the attrs now)</li>
</ul>


<p>I need to do the same thing for Noah and see where it leads.</p>

<h1>Dat list</h1>

<p>This list isn't comprehensive but I think it hits the key points.</p>

<h2>Simple</h2>

<p>Noah should be simple to interact with. It was born out of frustration with trying to interact with ZooKeeper. Nothing is more simple than being able to use <code>curl</code> IMHO. I can use Noah in shell scripts and I can use it in Java (we had a Spring Configurator at VA that talked to Noah. It was awesome). You should always be able to use <code>curl</code> to interact with Noah. I wish I could find it now but someone once brought up Noah on the ZK mailing list. This led to various rants about how it didn't do consensus and a bunch of other stuff that ZK did. One of the Yahoo guys (I wish I could remember who) said something in favor of Noah that stuck with me:</p>

<p><em>Interfaces matter</em></p>

<p>I know I'm on the right track here because Rackspace just built a product that provides an HTTP interface to ZK. Oh and it does callbacks.</p>

<h2>Friendly to both sysadmins and developers</h2>

<p>Simplicity plays into this but I wanted Noah to be the tool that solved some friction between the people who write the code and the people who run the code. Configuration is all over the place in any modern stack. Configuration management has come into its own. People are using it but you still see disconnects. Where should this config be maintained? What's the best way to have puppet track changes to application configuration? I can't get my developers to update the ERB templates in the Chef cookbook. All of these things are where Noah is helpful.</p>

<p>I still stand by the statement that <a href="http://lusislog.blogspot.com/2011/03/ad-hoc-configuration-coordination-and.html">not all configuration is equal</a>. Volatility is a thing and it doesn't have to mean the end of all the effort in moving to a CM tool. I wanted to remove that friction point.</p>

<p>I was also immensely inspired by Kelsey Hightower here. I've told the story several times of how Kelsey got so frustrated that the developers wouldn't cooperate with us on Puppet and config files for our applications that he learned enough Java to write a library for looking up information in Cobbler. Cobbler has an XMLRPC api and that was simple enough that he could port his python skills to java and write the fucking library himself. I wanted Noah to be friendly enough that a sysadmin could do what Kelsey did.</p>

<h2>Watches and Callbacks</h2>

<p>I've said this before but one of the most awesome things that ZK has is watches. They have pitfalls (reregister your watches after they fire for instance) but they're awesome. Noah's callback system is the thing that needs the most love (it works but the plugin API was never finalized). It's also one of the most powerful parts that meets the needs of folks that I see posting on various mailing lists.</p>

<p>The idea is simple. When something changes in Noah, you should be able to fire off a message however the end-user wants to get it. I think this is one of the reasons I love working on Logstash so much. Writing plugins is so simple and it's the gateway drug to anyone who wants to contribute to logstash.</p>

<h1>Things I don't care about</h1>

<p>What don't I care about?</p>

<h2>Language</h2>

<p>I don't care about the language it's written in. If someone wants to take it and convert it to Python or Erlang or Clojure, be my guest. I just want the ideas to live on somehwere. In fact, I've rewritten various parts of Noah over the last year privately. Not just experimenting with moving from EM to Celluloid but as a Cherry.py app, in Clojure and I even started an Erlang attempt (except that I know almost NO Erlang so it didn't get very far).</p>

<h2>Name</h2>

<p>Honestly I don't even care about the name. Yeah it's witty and fits with the idea of ZooKeeper but I have no qualms about adding a link to your project from the Noah readme and recommending people use it instead.</p>

<h2>Paxos/ZAB</h2>

<p>This was never a requirement for Noah. Noah was specifically designed for certain types of information. If you need that, use the right tool.</p>

<h2>Persistence</h2>

<p>Let's be honest. From a simplicity standpoint, it doesn't get much simpler than Redis. It's one of the reasons we changed the default logstash tutorial to use Redis instead of RabbitMQ. I know Redis reinvents a lot of wheels that have already been solved but it, along with ElasticSearch, are one of the lowest friction bits of software I've dealt with in a long time. Not having external dependencies is a godsend for getting started.</p>

<p>However I've also got small experiments privately where I used ZMQ internally and sqlite. I've written a git-based persistence for it too.</p>

<p>Riak is also a great fit for Noah and takes care of the availability issue on the persistence side. More on Riak in a sec.</p>

<h1>So that's it</h1>

<p>That's really all that matters. If you want to take ownership of the project, contact me. Let me know and we'll talk. Who knows. Maybe I'm overestimating the level of interest. Maybe ZK isn't as unapproachable to people anymore. The language bindings have certainly gotten much better. I just want the project to be useful to folks and I'm getting in the way of that.</p>

<h1>What are the other options?</h1>

<p>I don't know of many other options out there. Doozer is picking up steam again as I understand it and it has a much smaller footprint than ZK does. There was a python project that did a subset of Noah but I can't find it now.</p>

<p>One thing that is worth considering is a project that I found earlier today - <a href="https://github.com/cocagne/zpax">zpax</a>. While this is just a framework experiment of sorts, it could inspire you to add your own frontend to it. The same author is also working on DTLS on top of ZMQ.</p>

<p>I've thought about ways I could actually do this with Logstash plugins. It's doable but not really feasible without making Logstash do something it isn't shaped for.</p>

<p>Another idea that I'm actually toying around with is simply using Riak plus a ZeroMQ post-commit hook so that plugins could be written in a simpler way. <a href="https://github.com/seancribbs/riak_zmq">Sean Cribbs already took the idea and made a POC 2 years ago</a> based on a gist from Cody Soyland. You wouldn't have the same API up front as Noah but you could stub that out in some framework and also have it be the recipient of the ZMQ publishes.</p>

<p>Finally you could just use ZooKeeper. Yes it has MUCH greater overhead but you DO get a lot more bang for the buck. There really isn't anything in the opensource world right now that compares. It also provides additional features that I never really cared about or needed in Noah.</p>

<h1>Wrap up</h1>

<p>I'm not done in this space. I don't know where I'm going next with it. Maybe I'll start from scratch with a much simpler API. Maybe I'll just run with the Riak idea.</p>

<p>I just want to give a shoutout to the countless people who helped me evangelize Noah over the last few years. It was recommended on mailing lists, twitter and many other places. It meant a lot to me and I only hope that someone will take up the mantle and make it something you would recommend again.</p>

<p>For those of you still using Noah, I hope we can find a home for it so that it can continue to provide value to you.</p>

<p>Thanks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fun With Celluloid]]></title>
    <link href="http://lusis.github.com/blog/2011/08/13/fun-with-celluloid/"/>
    <updated>2011-08-13T00:30:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/08/13/fun-with-celluloid</id>
    <content type="html"><![CDATA[<p><em>warning! This is a really long post</em></p>

<p>In the course of rewriting the <a href="https://github.com/lusis/Noah">Noah</a> callback daemon, I started to get really frustrated with EventMachine. This is nothing against EventMachine by any stretch of the imagination. I really like it.</p>

<p>What I was having issues with is making the plugin framework as dirt simple as possible. By using EM, I had no choice but to require folks to understand how EM works. This primarily meant not blocking the reactor. Additionally, through no fault of EM, I was starting to get mired in callback spaghetti.</p>

<h1>Actors</h1>

<p>I've mentioned several times before that I love the actor model. It makes sense to me. The idea of mailboxes and message passing is really simple to understand. For a while, there was project that implemented actors on top of EM called Revactor but it stalled. I started following the author (Tony Arcieri) on GitHub to see if he would ever update it. He did not but I caught wind of his new project and it was pretty much exactly what I was looking for.</p>

<p>Actors have a proven track record in Erlang and the Akka framework for Scala and Java uses them as well.</p>

<h1>Celluloid</h1>

<!--more-->


<p><a href="https://github.com/tarcieri/celluloid">Celluloid</a> is an implementation of Actors on Ruby. At this point, it lacks some of the more advanced features of the Akka and Erlang implementations. However Tony is very bullish about Celluloid and is pretty awesome in general.</p>

<p>I'm not going to go over Celluloid basics in too much detail. Tony does an awesome job in the <a href="http://celluloid.github.com/">README</a> for the project. What I want to talk more about is how I want to use it for Noah and what capabilities it has/is missing for that use case.</p>

<h1>Noah callbacks</h1>

<p>I won't bore you with a rehash of Noah. I've written a ton of blog posts (and plan to write more). However for this discussion, it's important to understand what Noah callbacks need to do.</p>

<h2>Quick recap</h2>

<p>Any object in Noah can be "watched". This is directly inspired by ZooKeeper. Because Noah is stateless, however, watches need to work a little differently. The primary difference is that Noah's watches are asynch. As a side-effect of that, we get some really cool additional functionality. So what does a Noah watch consist of?</p>

<ul>
<li>An absolute or partial path to and endpoint in the system</li>
<li>A URI-style location for notification of changes to that path</li>
</ul>


<p>Let's say you had a small sinatra application running on all your servers. Its only job was to be a listener for messages from Noah. This daemon will be responsible for rewriting your <code>hosts</code> file with any hosts that are created, modified or deleted on your network.</p>

<p>In this case, you might register your watch with a path of <code>/hosts/</code> and an endpoint of <code>http://machinename:port/update_hosts</code>. Any time a host object is created, updated or deleted Noah will send the JSON representation of that object state along with the operation performed to that endpoint. Let's say you also want to know about some configuration setting that has changed which lives at <code>/configurations/my_config_file.ini</code>. Let's put a kink in that. You want that watch to drop its message onto a RabbitMQ exchange.</p>

<p>So now we have the following information that we need to act on:</p>

<ul>
<li><code>{:endpoint =&gt; 'http://machine:port/update_hosts', :pattern =&gt; '//noah/hosts'}</code></li>
<li><code>{:endpoint =&gt; 'amqp://host:port/exchange?durable=true', :pattern =&gt; '//noah/configurations/my_config_file.ini'}</code></li>
</ul>


<p>Not so hard right? But we also have some additional moving parts. Something needs to monitor Redis for these various CRUD messages. We need to compare them against a list of endpoints that want notification about those messages. We also need to intercept any messages from Redis that are new endpoints being registered. Oh and we also need to know about failed endpoints so we can track and eventually evict them. Obviously we don't want to stop http messages from going out because AMQP is slow. Imagine if we implemented FTP endpoint support! Essentially we need high concurrency not only on each 'class' of endpoint (http, amqp, ftp whatever) but also within each class of endpoint. If any individual endpoint attempt crashes for any reason, we need to take some action (eviction for instance) and not impact anyone else.</p>

<h1>Doing it with Celluloid</h1>

<p>So thinking about how we would do this with actors, I came up with the following basic actors:</p>

<ul>
<li>RedisActor <em>watches the Redis pubsub backend</em></li>
<li>HTTPActor <em>handles HTTP endpoints - a 'worker'</em></li>
<li>AMQPActor <em>handles AMQP endpoints - a 'worker'</em></li>
<li>BrokerActor <em>responsible for intercepting endpoint CRUD operations and also determining which actors to send messages to for processing</em></li>
</ul>


<p>As I said previously, we also need to ensure that if any worker crashes, that it gets replaced. Otherwise we would eventually lose all of our workers.</p>

<p>With this information, we can start to build a tree that looks something like this:</p>

<pre><code>- Master process
    |_Redis
    |_Broker
    |_HTTPPool
    |    |_Worker
    |    |_Worker
    |_AMQPPool
        |_Worker
        |_Worker
</code></pre>

<p>The master process is responsible for handling the Redis, Broker and Pool actors. Each pool actor is responsible for its workers. Not really visible in the ASCII above is how messages flow:</p>

<ul>
<li>Master process spawns Redis, Broker, HTTPPool and AMQPPool as supervised processes.</li>
<li>Each pool type spins up a set of supervised workers.</li>
<li>Master process makes an HTTP request to the Noah server for all existing watches (synchronous)</li>
<li>It sends a message with those watches to the Broker so it can build its initial list.(synchronous)</li>
<li>Redis actor watches pubsub.</li>
<li>Watch messages are sent to a mailbox on the Broker. (synchronous)</li>
<li>The rest to a different mailbox on the broker.</li>
<li>The broker performs some filtering to determine if any registered watches care about the message. If so, those are sent to the appropriate pool. (async)</li>
<li>Each Pool selects a worker and tells him the endpoint and the message</li>
<li>The worker delivers the message</li>
</ul>


<p>Where this became a slight problem with Celluloid is that it lacks two bits of functionality currently:</p>

<ul>
<li>Supervision trees</li>
<li>Pool primitives</li>
</ul>


<p>Right now in Celluloid, there is no way to build "pools" of supervised processes. The supervised part is important. If a process is supervised, crashes will be trapped and the process will be restarted.</p>

<p>So how did we "fake" this with the existing functionality?</p>

<p>The generic tree was fairly easy. The main Ruby process creates supervised processes for each actor:</p>

<p>``` ruby</p>

<pre><code>class RedisActor
  include Celluloid::Actor
  def initialize(name)
    @name = name
    log.info "starting redis actor"
  end

  def start
   # start watching redis
  end
end
class BrokerActor
  include Celluloid::Actor
  # constructor
  def process_watch(msg)
    #...
  end
  def do_work(msg)
    #...
  end
end

class HTTPPool
  # you get the idea
end

@http_pool = HTTPPool.supervise_as :http_pool, "http_pool"
@broker_actor = BrokerActor.supervise_as :broker_actor, "broker"
@redis_actor = RedisActor.supervise_as :redis_actor, "redis"
</code></pre>

<p>```</p>

<p>The workers were a bit more complicated. What I ended up doing was something like this:</p>

<p>``` ruby</p>

<pre><code>class HTTPWorker
  include Celluloid::Actor

  attr_reader :name

  def initialize(name)
    @name = name
  end
  def do_work(ep, msg)
    # Work to send the message
  end
end

class HTTPPool
  include Celluloid::Actor
  WORKERS = 10

  attr_reader :workers

  def initialize(name)
    @name = name
    @workers = []
    WORKERS.times do |id|
      @workers[id] = HTTPWorker.supervise_as "http_worker_#{id}".to_sym, "http_worker_#{id}"
    end
  end
  def do_work
    @workers.sample.actor.do_work "msg"
  end
end
</code></pre>

<p>```</p>

<p>The problem as it stands is that we can't really have "anonymous" supervised processes. Each actor that's created goes into Celluloid's registry. We need a programatic way to look those up so we use <code>supervise\_as</code> to give them a name.</p>

<p>This gives us our worker pool. Now Redis can shovel messages to the broker who filters them. He sends a unit of work to a Pool which then selects a random worker to do the REAL work. Should any actor crash, he will be restarted. Because each actor is isolated, A crash in talking to redis, doesn't cause our existing workers to stop sending messages.</p>

<p>Obviously this a fairly naive implementation. We're missing some really important functionality here.</p>

<ul>
<li>detecting busy workers</li>
<li>detecting dead workers (yes we still need to do this)</li>
<li>alternate worker selection mechanisms (cyclical for instance)</li>
<li>crash handling</li>
<li>backlog handling</li>
</ul>


<p>You might wonder why we care if a worker is dead or not? Currently Celluloid buffers messages in each actor until the can be dealt with. In the case of our Pool, it will select a worker and buffer any messages if the worker is blocked. If our worker crashes on its current unit of work, it returns control to the pool. The pool then attempts to send the worker the next message but the worker is dead and hasn't respawned yet.</p>

<h1>Some code to play with</h1>

<p>Yes, we've finally made it to the end.</p>

<p>I've created a fun little sinatra application to make it easier for me to test my pools. It consists of a generic Pool class that can be subclassed and take a the name of a worker class as an argument. When a worker gets a message of "die", it will raise an exception thus simulating a crash. Additionally, the "message processing" logic includes sleep to simulate long running work.</p>

<p>The reason Sinatra is in the mix is to provide an easy way for me to fire off simulated requests to the pool so I can experiment with different approaches. Eventually, Celluloid will have a proper Pool construct. I plan on using this as the basis for a pull request. You can see it here. Please feel free to fork and experiment with me. It's really fun.</p>

<p><div><script src='https://gist.github.com/1143369.js?file='></script>
<noscript><pre><code>require 'celluloid'
require 'logger'
require 'uuid'
require 'sinatra/base'

# This is just a simple demo of a possible Pool implementation for Celluloid
# The sinatra interface exists just to do some testing of crashing workers and the like

# TODO
# Create a busy worker registry of some kind
# Implement a small stats page

LOGGER = Logger.new(STDOUT)
LOGGER.progname = &quot;noah-agent&quot;
Celluloid.logger = LOGGER

class WorkerError &lt; Exception; end

class Pool
  include Celluloid::Actor
  #trap_exit :worker_exception_handler

  attr_reader :workers, :busy_workers

  def initialize(name, opts = {:num_workers =&gt; 10, :worker_class =&gt; Worker})
    @name = name
    @workers = []
    @busy_workers = []
    LOGGER.info(&quot;Pool #{name} starting up&quot;)
    opts[:num_workers].times do |worker|
      start_worker(opts[:worker_class])
    end
  end

  def start_worker(klass)
    worker_id = gen_worker_id
    LOGGER.info(&quot;Pool #{@name} is starting a #{klass.to_s} worker&quot;)
    wkr = klass.supervise_as &quot;#{@name}_worker_#{worker_id}&quot;.to_sym, &quot;#{@name}_worker_#{worker_id}&quot;
    @workers &lt;&lt; wkr
  end

  def notify_worker(msg)
    worker = self.get_worker
    @busy_workers &lt;&lt; worker.name
    worker.work msg
    @busy_workers.delete worker.name
  end

  def worker_exception_handler(actor, reason)
    LOGGER.debug(&quot;Worker #{actor.name} crashed because #{reason}. You should see a doctor about that&quot;)
  end

  
  protected
  def gen_worker_id
    Digest::SHA1.hexdigest(UUID.generate)
  end

  def get_worker
    worker = @workers.sample.actor
    LOGGER.info(&quot;Found Worker: #{worker.name} in the pool&quot;)
    if worker.alive?
      worker
    else
      LOGGER.error &quot;Worker #{worker.name} was dead. Retrying!&quot;
      self.get_worker
    end
  end

end

class MyWorker
  include Celluloid::Actor
  attr_reader :name

  def initialize(name)
    @name = name
  end

  def work(msg)
    LOGGER.info(&quot;Message for you sir! #{msg}&quot;)
    case msg
    when &quot;die&quot;
      # Simulate some long-running work that crashes
      sleep 15
      raise WorkerError, &quot;Boo got shot!&quot;
    else
      # Simulate some long-running work here
      sleep 30
      LOGGER.debug(&quot;Hey there camper! #{@name} is doing some work for you&quot;)
    end
  end

end

class TestApp &lt; Sinatra::Base
  @pool = Pool.supervise_as :my_cool_pool, &quot;MyCoolPool&quot;, {:num_workers =&gt; 30, :worker_class =&gt; MyWorker}
  configure do
    set :app_file, __FILE__
    set :logging, false
    set :dump_errors, false
    set :run, false
    set :server, &quot;thin&quot;
    set :pool, @pool
  end

  put '/scale' do
    settings.pool.actor.start_worker(MyWorker)
    &quot;Added a worker&quot;
  end

  get '/stats' do
    &quot;Worker count: #{settings.pool.actor.workers.size}\n Busy workers: #{settings.pool.actor.busy_workers.size}&quot;
  end

  put '/die' do
    settings.pool.actor.notify_worker! &quot;die&quot;
  end

  put '/send' do
    settings.pool.actor.notify_worker! request.body.read
  end
end

app = TestApp
app.run!</code></pre></noscript></div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 4]]></title>
    <link href="http://lusis.github.com/blog/2011/05/19/on-noah-part-4/"/>
    <updated>2011-05-19T22:01:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/19/on-noah-part-4</id>
    <content type="html"><![CDATA[<p><em>This is the fourth part in a series on Noah. <a href="http://goo.gl/l3Mgt">Part 1</a>, <a href="http://goo.gl/Nj2TN">Part 2</a> and <a href="http://goo.gl/RsZtZ">Part 3</a> are available as well</em></p>

<p>In Part 1 and 2 of this series I covered background on Zookeeper and
discussed the similarities and differences between it and Noah. Part 3
was about the components underneath Noah that make it tick.</p>

<p>This post is about the "future" of Noah. Since I'm a fan of Fourcast
podcast, I thought it would be nice to do an immediate, medium and long
term set of goals.</p>

<!--more-->


<h1>Immediate Future - the road to 1.0</h1>

<p>In the most immediate future there are a few things that need to happen.
These are in no specific order.</p>

<ul>
<li><p>General</p>

<ul>
<li>Better test coverage ESPECIALLY around the watch subsystem</li>
<li>Full code comment coverage</li>
<li>Chef cookbooks/Puppet manifests for doing a full install</li>
<li>"fatty" installers for a standalone server</li>
<li>Documentation around operational best practices</li>
<li>Documentation around clustering, redundancy and hadr</li>
<li>Documentation around integration best practices</li>
<li>Performance testing</li>
</ul>
</li>
<li><p>Noah Server</p>

<ul>
<li>Expiry flags and reaping for Ephemerals</li>
<li>Convert mime-type in Configurations to make sense</li>
<li>Untag and Unlink support</li>
<li>Refactor how you specify Redis connection information</li>
<li>Integrated metrics for monitoring (failed callbacks, expired
ephemeral count, that kind of stuff)</li>
</ul>
</li>
<li><p>Watcher callback daemon</p>

<ul>
<li>Make the HTTP callback plugin more flexible</li>
<li>Finish binscript for the watcher daemon</li>
</ul>
</li>
<li><p>Other</p>

<ul>
<li>Finish <a href="http://goo.gl/B65aL">Boat</a></li>
<li>Finish NoahLite LWRP for Chef (using Boat)</li>
<li>A few more HTTP-based callback plugins (Rundeck, Jenkins)</li>
</ul>
</li>
</ul>


<p>Now that doesn't look like a very cool list but it's a lot of work for
one person. I don't blame anyone for not getting excited about it. The
goal now is to get a functional and stable application out the door that
people can start using. Mind you I think it's usable now (and I'm
already using it in "production").</p>

<p>Obviously if anyone has something else they'd like to see on the list,
let me know.</p>

<h1>Medium Rare</h1>

<p>So beyond that 1.0 release, what's on tap? Most of the work will
probably occur around the watcher subsystem and the callback daemon.
However there are a few key server changes I need to implement.</p>

<ul>
<li><p>Server</p>

<ul>
<li>Full ACL support on every object at every level</li>
<li>Token-based and SSH key based credentialing</li>
<li>Optional versioning on every object at every level</li>
<li>Accountability/Audit trail</li>
<li>Implement a long-polling interface for inband watchers</li>
</ul>
</li>
<li><p>Watcher callback daemon</p>

<ul>
<li>Decouple the callback daemon from the Ruby API of the server.
Instead the daemon itself needs to be a full REST client of the
Noah server</li>
<li>Break out the "official" callback daemon into a distinct package</li>
</ul>
</li>
<li><p>Clients</p>

<ul>
<li>Sinatra Helper</li>
</ul>
</li>
</ul>


<p>Also during this period, I want to spend time building up the ecosystem
as a whole. You can see a general mindmap of that
<a href="https://github.com/lusis/Noah/wiki/Ecosystem">here</a>.</p>

<p>Going into a bit more detail...</p>

<h2>Tokens and keys</h2>

<p>It's plainly clear that something which has the ability to make runtime
environment changes needs to be secure. The first thing to roll off the
line post-1.0 will be that functionality. Full ACL support for all
entries will be enabled and in can be set at any level in the namespace
just the same as Watches.</p>

<h2>Versioning and Auditing</h2>

<p>Again for all entires and levels in the namespace, versioning and
auditing will be allowed. The intention is that the number of revisions
and audit entries are configurable as well - not just an enable/disable
bit.</p>

<h2>In-band watches</h2>

<p>While I've lamented the fact that watches were in-band only in
Zookeeper, there's a real world need for that model. The idea of
long-polling functionality is something I'd actually like to have by 1.0
but likely won't happen. The intent is simply that when you call say
<code>/some/path/watch</code>, you can pass an optional flag in the message stating
that you want to watch that endpoint for a fixed amount of time for any
changes. Optionally a way to subscribe to all changes over long-polling
for a fixed amount of time is cool too.</p>

<h2>Agent changes</h2>

<p>These two are pretty high on my list. As I said, there's a workable
solution with minimal tech debt going into the 1.0 release but long
term, this needs to be a distinct package. A few other ideas I'm kicking
around are allowing configurable filtering on WHICH callback types an
agent will handle. The idea is that you can specify that this invocation
only handle http callbacks while this other one handles AMQP.</p>

<h2>Sinatra Helper</h2>

<p>One idea I'd REALLY like to come to fruition is the Sinatra Helper. I
envision it working something like this:</p>

<p>``` ruby</p>

<pre><code>require 'sinatra/base'

class MyApp &lt; Sinatra::Base
  register Noah::Sinatra

  noah_server "http://localhost:5678"
  noah_node_name "myself"
  noah_app_name "MyApp"
  noah_token "somerandomlongstring"
  dynamic_get :database_server
  dynamic_set :some_other_variable, "foobar"
  watch :this_other_node
end
</code></pre>

<p>```</p>

<p>The idea is that the helper allows you to register your application very
easily with Noah for other components in your environment to be know. As
a byproduct, you get the ability to get/set certain configuration
parameters entirely in Noah. The watch setting is kind of cool as well.
What will happen is if you decide to <code>watch</code> something this way, the
helper will create a random (and yes, secure) route in your application
that watch events can notify. In this way, your Sinatra application can
be notified of any changes and will automatically "reconfigure" itself.</p>

<p>Obviously I'd love to see other implementations of this idea for other
languages and frameworks.</p>

<h1>Long term changes</h1>

<p>There aren't so much specific list items here as general themes and
ideas. While I list these as long term, I've already gotten an offer to
help with some of them so they might actually get out sooner.</p>

<h2>Making Noah itself distributed</h2>

<p>This is something I'm VERY keen on getting accomplished and would really
consider it the fruition of what Noah itself does. The idea is simply
that multiple Noah servers themselves are clients of other Noah servers.
I've got several ideas about how to accomplish this but I got an
interesting follow up from someone on Github the other day. He asked
what my plans were in this area and we had several lengthy emails back
and forth including an offer to work on this particular issue.</p>

<p>Obviously there are a whole host of issues to consider. Race conditions
in ordered delivery of Watch callbacks (getting a status "down" after a
status "up" when it's supposed to be the other way around..) and
eventual consistency spring to mind first.</p>

<p>The general architecture idea that was offered up is to use
<a href="https://github.com/derekcollison/nats">NATS</a> as the mechanism for
accomplishing this. In the same way that there would be AMQP callback
support, there would be NATS support. Additional Noah servers would only
need to know one other member to bootstrap and everything else happens
using the natural flows within Noah.</p>

<p>The other part of that is how to handle the Redis part. The natural
inclination is to use the upcoming Redis clustering but that's not
something I want to do. I want each Noah server to actually include its
OWN Redis instance "embedded" and not need to rely on any external
mechanism for replication of the data. Again, the biggest validation of
what Noah is designed to do is using only Noah itself to do it.</p>

<h2>Move off Redis/Swappable persistence</h2>

<p>If NATS says anything to me, it says "Why do you even need Redis?". If
you recall, I went with Redis because it solved multiple problems. If I
can find a persistence mechanism that I can use without any external
service running, I'd love to use it.</p>

<h2>ZeroMQ</h2>

<p>If I were to end up moving off Redis, I'd need a cross platform and
cross language way to handle the pubsub component. NATS would be the
first idea but NATS is Ruby only (unless I've missed something). ZeroMQ
appears to have broad language and platform support so writing custom
agents in the same vein as the Redis PUBSUB method should be feasible.</p>

<h2>Nanite-style agents</h2>

<p>This is more of a command-and-control topic but a set of
high-performance specialized agents on systems that can watch the PUBSUB
backend or listen for callbacks would be awesome. This would allow you
really integrate Noah into your infrastructure beyond the application
level. Use it to trigger a puppet or chef run, reboot instances or do
whatever. This is really about bringing what I wanted to accomplish with
Vogeler into Noah.</p>

<h2>The PAXOS question</h2>

<p>A lot of people have asked me about this. I'll state right now that I
can only make it through about 20-30% of any reading about Paxos before
my brain starts to melt. However in the interest of proving myself the
fool, I think it would be possible to implement some Paxos like
functionality on top of Noah. Remember that Noah is fundamentally about
fully disconnected nodes. What better example of a network of unreliable
processors than ones that never actually talk to each other. The problem
is that the use case for doing it in Noah is fairly limited so as not to
be worth it.</p>

<p>The grand scheme is that Noah helps enable the construction of systems
where you can say "This component is free to go off and operate in this
way secure in the knowledge that if something it needs to know changes,
someone will tell it". I did say "grand" didn't I? At some point, I may
hit the limit of what I can do using only Ruby. Who knows.</p>

<h1>Wrap up - Part 4</h1>

<p>Again with the recap</p>

<ul>
<li>Get to 1.0 with a stable and fixed set of functionality</li>
<li>Nurture the Noah ecosystem</li>
<li>Make it easy for people to integrate Noah into thier applications</li>
<li>Get all meta and make Noah itself distributed using Noah</li>
<li>Minimize the dependencies even more</li>
<li>Build skynet</li>
</ul>


<p><em>I'm not kidding on that last one. Ask me about Parrot AR drones and
Noah sometime</em></p>

<p>If you made it this far, I want to say thank you to anyone who read any
or all of the parts. Please don't hesitate to contact me with any
questions about the project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 3]]></title>
    <link href="http://lusis.github.com/blog/2011/05/18/on-noah-part-3/"/>
    <updated>2011-05-18T18:14:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/18/on-noah-part-3</id>
    <content type="html"><![CDATA[<p><em>This is the third part in a series on Noah. <a href="http://goo.gl/l3Mgt">Part 1</a> and <a href="http://goo.gl/Nj2TN">Part 2</a> are available as well</em></p>

<p>In Part 1 and 2 of this series I covered background on Zookeeper and
discussed the similarities and differences between it and Noah. This
post is discussing the technology stack under Noah and the reasoning for
it.</p>

<h1>A little back story</h1>

<p>I've told a few people this but my original intention was to use Noah as
a way to learn Erlang. However this did not work out. I needed to get a
proof of concept out much quicker than the ramp up time it would take to
<a href="http://learnyousomeerlang.com/">learn me some Erlang</a>. I had this
grandiose idea to slap mnesia, riak_core and webmachine into a tasty
ball of Zookeeper clonage.</p>

<!--more-->


<p>I am not a developer by trade. I don't have any formal education in
computer science (or anything for that matter). The reason I mention
this is to say that programming is hard work for me. This has two side
effects:</p>

<ul>
<li>It takes me considerably longer than a working developer to code
what's in my head</li>
<li>I can only really learn a new language when I have an itch to
scratch. A real world problem to model.</li>
</ul>


<p>So in the interest of time, I fell back to a language I'm most
comfortable with right now, Ruby.</p>

<h1>Sinatra and Ruby</h1>

<p>Noah isn't so much a web application as it is this 'api thing'. There's
no proper front end and honestly, you guys don't want to see what my
design deficient mind would create. I like to joke that in the world of
MVC, I stick to the M and C. Sure, APIs have views but not in the "click
the pretty button sense".</p>

<p>I had been doing quite a bit of glue code at the office using
<a href="http://www.sinatrarb.com">Sinatra</a> (and EventMachine) so I went with
that. Sinatra is, if you use sheer number of clones in other languages
as an example, a success for writing API-only applications. I also
figured that if I wanted to slap something proper on the front, I could
easily integrate it with <a href="http://www.padrinorb.com">Padrino</a>.</p>

<p>But now I had to address the data storage issue.</p>

<h1>Redis</h1>

<p>Previously, as a way to learn Python at another company, I wrote an
application called <a href="https://github.com/lusis/vogeler">Vogeler</a>. That
application had a lot of moving parts - CouchDB for storage and RabbitMQ
for messaging.</p>

<p>I knew from dealing with CouchDB on CentOS5 that I wasn't going to use
THAT again. Much of it would have been overkill for Noah anyway. I
realized I really needed nothing more than a key/value store. That
really left me with either Riak or Redis. I love Riak but it wasn't the
right fit in this case. I needed something with a smaller dependency
footprint. Mind you Riak is VERY easy to install but managing Erlang
applications is still a bit edgy for some folks. I needed something
simpler.</p>

<p>I also realized early on that I needed some sort of basic queuing
functionality. That really sealed Redis for me. Not only did it have
zero external dependencies, but it also met the needs for queuing. I
could use <code>lists</code> as dedicated direct queues and I could use the
built-in <code>pubsub</code> as a broadcast mechanism. Redis also has a fast atomic
counter that could be used to approximate the ZK sequence primitive
should I want to do that.</p>

<p>Additionally, Redis has master/slave (not my first choice) support for
limited scaling as well as redundancy. One of my original design goals
was that Noah behave like a traditional web application. This is a model
ops folks understand very well at this point.</p>

<h1>EventMachine</h1>

<p>When you think asynchronous in the Ruby world, there's really only one
tool that comes to mind, EventMachine. Noah is designed for asynchronous
networks and is itself asynchronous in its design. The callback agent
itself uses EventMachine to process watches. As I said previously, this
is simply using an EM friendly Redis driver that can do <code>PSUBSCRIBE</code>
(using em-hiredis) and send watch messages (using em-http-request since
we only support HTTP by default).</p>

<h1>Ohm</h1>

<p>Finally I slapped <a href="http://ohm.keyvalue.org">Ohm</a> on top as the
abstraction layer for Redis access. Ohm, if you haven't used it, is
simply one of if not the best Ruby library for working with Redis. It's
easily extensible, very transparent and frankly, it just gets the hell
out of your way. A good example of this is converting some result to a
hash. By default, Ohm only returns the id of the record. Nothing more.
It also makes it VERY easy to drop past the abstraction and operate on
Redis directly. It even provides helpers to get the keys it uses to
query Redis. A good example of this is in the Linking and Tagging code.
The following is a method in the Tag model:</p>

<p>``` ruby</p>

<pre><code>def members=(member)
  self.key[:members].sadd(member.key)
  member.tag! self.name unless member.tags.member?(self)
end
</code></pre>

<p>```</p>

<p>Because Links and Tags are a one-to-many across multiple models, I drop
down to Redis and use <code>sadd</code> to add the object to a Redis set of objects
sharing the same tag.</p>

<p>It also has a very handy feature which is how the core of Watches are
done. You can define hooks at any phase of Redis interaction - before
and after saves, creates, updates and deletes. the entire Watch system
is nothing more than calling these post hooks to format the state of the
object as JSON, add metadata and send the message using <code>PUBLISH</code>
messages to Redis with the Noah namespace as the channel.</p>

<h1>Distribution vectors</h1>

<p>I've used this phrase with a few people. Essentially, I want as many
people as possible to be able to use the Noah server component. I've
kept the Ruby dependencies to a minimum and I've made sure that every
single one works on MRI 1.8.7 up to 1.9.2 as well as JRuby. I already
distribute the most current release as a war that can be deployed to a
container or run standalone. I want the lowest barrier to entry to get
the broadest install base possible. When a new PaaS offering comes out,
I pester the hell out of anyone I can find associated with it so I can
get deploy instructions written for Noah. So far you can run it on
Heroku (using the various hosted Redis providers), CloudFoundry and
dotcloud.</p>

<p>I'm a bit more lax on the callback daemon. Because it can be written in
any language that can talk to the Redis pubsub system and because it has
"stricter" performance needs, I'm willing to make the requirements for
the "official" daemon more stringent. It currently ONLY works on MRI
(mainly due to the em-hiredis requirement).</p>

<h2>Doing things differently</h2>

<p>Some people have asked me why I didn't use technology A or technology B.
I think I addressed that mostly above but I'll tackle a couple of key
ones.</p>

<p>ZeroMQ</p>

<p>The main reason for not using 0mq was that I wasn't really aware of it.
Were I to start over and still be using Ruby, I'd probably give it a
good strong look. The would still be the question of the storage
component though. There's still a possible place for it that I'll
address in part four.</p>

<p>NATS</p>

<p>This was something I simply had no idea about until I started poking
around the CloudFoundry code base. I can almost guarantee that NATS will
be a part of Noah in the future. Expect much more information about that
in part four.</p>

<p>MongoDB</p>

<p>You have got to be kidding me, right? I don't trust my data (or anyone
else's for that matter) to a product that doesn't understand what
durability means when we're talking about databases.</p>

<p>Insert favorite data store here</p>

<p>As I said, Redis was the best way to get multiple required functionality
into a single product. Why does a data storage engine have a pubsub
messaging subsystem built in? I don't know off the top of my head but
I'll take it.</p>

<h2>Wrap up - Part 3</h2>

<p>So again, because I evidently like recaps, here's the take away:</p>

<ul>
<li>The key components in Noah are Redis and Sinatra</li>
<li>Noah is written in Ruby because of time constraints in learning a
new language</li>
<li>Noah strives for the server component to have the broadest set of
distribution vectors as possible</li>
<li>Ruby dependencies are kept to a minimum to ensure the previous point</li>
<li>The lightest possible abstractions (Ohm) are used.</li>
<li>Stricter requirements exist for non-server components because of
flexibility in alternates</li>
<li>I really should learn me some erlang</li>
<li>I'm not a fan of MongoDB</li>
</ul>


<p>If you haven't guessed, I'm doing one part a night in this series.
Tomorrow is part four which will cover the future plans for Noah. I'm
also planning on a bonus part five to cover things that didn't really
fit into the first four.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Noah - Part 2]]></title>
    <link href="http://lusis.github.com/blog/2011/05/17/on-noah-part-2/"/>
    <updated>2011-05-17T18:38:00-04:00</updated>
    <id>http://lusis.github.com/blog/2011/05/17/on-noah-part-2</id>
    <content type="html"><![CDATA[<p><em>This is the second part in a series on Noah. Part 1 is available</em> <a href="http://goo.gl/l3Mgt">here</a></p>

<p>In part one of this series, I went over a little background about
ZooKeeper and how the basic Zookeeper concepts are implemented in Noah.
In this post, I want to go over a little bit about a few things that
Noah does differently.</p>

<!--more-->


<h2>Noah Primitives</h2>

<p>As mentioned in the previous post, Noah has 5 essential data types, four
of which are what I've interchangeably refered to as either Primitives
and Opinionated models. The four primitives are Host, Service,
Application and Configuration. The idea was to map some common use cases
for Zookeeper and Noah onto a set of objects that users would find
familiar.</p>

<p>You might detect a bit of Nagios inspiration in the first two.</p>

<ul>
<li><strong>Host:</strong>
  Analogous to a traditional host or server. The machine or instance running the operating system. Unique by name.</li>
<li><strong>Service:</strong>
  Typically mapped to something like HTTP or HTTPS. Think of this as the listening port on a Host. Services must be bound to Hosts. Unique by service name and host name.</li>
<li><strong>Application:</strong>
  Apache, your application (rails, php, java, whatever). There's a subtle difference here from Service. Unique by name.</li>
<li><strong>Configuration:</strong>
  A distinct configuration element. Has a one-to-many relationship with Applications. Supports limited mime typing.</li>
</ul>


<p>Hosts and Services have a unique attribute known as <code>status</code>. This is a
required attribute and is one of <code>up</code>,<code>down</code> or <code>pending</code>. These
primitives would work very well integrated into the OS init process.
Since Noah is curl-friendly, you could add something globally to init
scripts that updated Noah when your host is starting up or when some
critical init script starts. If you were to imagine Noah primitives as
part of the OSI model, these are analagous to Layers 2 and 3.</p>

<p>Applications and Configurations are intended to feel more like Layer 7
(again, using our OSI model analogy). The differentiation is that your
application might be a Sinatra or Java application that has a set of
Configurations associated with it. Interestingly enough, you might
choose to have something like Tomcat act as both a Service AND an
Application. The aspect of Tomcat as a Service is different than the
Java applications running in the container or even Tomcat's own
configurations (such as logging).</p>

<p>One thing I'm trying to pull off with Configurations is limited
mime-type support. When creating a Configuration in Noah, you can assign
a <code>format</code> attribute. Currently 3 formats or types are understood:</p>

<ul>
<li>string</li>
<li>json</li>
<li>yaml</li>
</ul>


<p>The idea is that, if you provide a type, we will serve that content back
to you in that format when you request it (assuming you request it that
way via HTTP headers). This should allow you to skip parsing the JSON
representation of the whole object and instead use it directly. Right
now this list is hardcoded. I have a task to convert this.</p>

<p>Hosts and Services make a great "canned" structure for building a
monitoring system on top of Noah. Applications and Configurations are a
lightweight configuration management system. Obviously there are more
uses than that but it's a good way to look at it.</p>

<h2>Ephemerals</h2>

<p>Ephemerals, as mentioned previously, are closer to what Zookeeper
provides. The way I like to describe Ephemerals to people is a '512 byte
key/value store with triggers' (via Watch callbacks). If none of the
Primitives fit your use case, the Ephemerals make a good place to start.
Simply send some data in the body of your post to the url and the data
is stored there. No attempt is made to understand or interpret the data.
The hierarchy of objects in the Ephemeral namespace is completely
arbitrary. Data living at <code>/ephemerals/foo</code> has no relationship with
data living at <code>/ephemerals/foo/bar</code>.</p>

<p>Ephemerals are also not browseable except via a Linking and Tagging.</p>

<h2>Links and Tags</h2>

<p>Links and Tags are, as far as I can tell, unique to Noah compared to
Zookeeper. Because we namespace against Primitives and Ephemerals, there
existed the need to visualize objects under a custom hierarchy.
Currently Links and Tags are the only way to visualize Ephemerals in a
JSON format.</p>

<p>Tags are pretty standard across the internet by now. You might choose to
tag a bunch of items as <code>production</code> or perhaps group a set of Hosts and
Services as <code>out-of-service</code>. Tagging an item is a simple process in the
API. Simply <code>PUT</code> the name of the tag(s) to the url of a distinct named
item appended by <code>tag</code>. For instance, the following JSON posted to
<code>/applications/my_kick_ass_app/tag</code> with tag the Application
<code>my_kick_ass_app</code> with the tags <code>sinatra</code>, <code>production</code> and <code>foobar</code>:</p>

<p>```javascript</p>

<pre><code>{"tags":["sinatra", "production", "foobar"]}
</code></pre>

<p>```</p>

<p>Links work similar to Tags (including the act of linking) except that
the top level namespace is now replaced with the name of the Link. The
top level namespace in Noah for the purposes of Watches is <code>//noah</code>. By
linking a group of objects together, you will be able to (not yet
implemented), perform operations such as Watches in bulk. For instance,
if you wanted to be informed of all changes to your objects in Noah, you
would create a Watch against <code>//noah/*</code>. This works fine for most people
but imagine you wanted a more multi-tenant friendly system. By using
links, you can group ONLY the objects you care about and create the
watch against that link. So <code>//noah/*</code> becomes <code>//my_organization/*</code> and
only those changes to items in that namespace will fire for that Watch.</p>

<p>The idea is also that other operations outside of setting Watches can be
applied to the underlying object in the link as well. The name Link was
inspired by the idea of symlinking.</p>

<h2>Watches and Callbacks</h2>

<p>In the first post, I mentioned that by nature of Noah being
"disconnected", Watches were persistent as opposed to one-shot.
Additionally, because of the pluggable nature of Noah Watches and
because Noah has no opinion regarding the destination of a fired Watch,
it becomes very easy to use Noah as a broadcast mechanism. You don't
need to have watches for each interested party. Instead, you can create
a callback plugin that could dump the messages on an ActiveMQ Fanout
queue or AMQP broadcast exchange. You could even use multicast to notify
multiple interested parties at once.</p>

<p>Again, the act of creating a watch and the destination for notifications
is entirely disconnected from the final client that might use the
information in that watch event.</p>

<p>Additionally, because of how changes are broadcast internally to Noah,
you don't even have to use the "official" Watch method. All actions in
Noah are published post-commit to a pubsub queue in Redis. Any language
that supports Redis pubsub can attach directly to the queue and
PSUBSCRIBE to the entire namespace or a subset. You can write your own
engine for listening, filtering and notifying clients.</p>

<p>This is exactly how the Watcher daemon works. It attaches to the Redis
pubsub queue, makes a few API calls for the current registered set of
watches and then uses the watches to filter messages. When a new watch
is created, that message is like any other change in Noah. The watcher
daemon sees that and immediately adds it to its internal filter. This
means that you can create a new watch, immediately change the watched
object and the callback will be made.</p>

<h2>Wrap up - Part Two</h2>

<p>So to wrap up:</p>

<ul>
<li>Noah has 5 basic "objects" in the system. Four of those are
opinionated and come with specific contracts. The other is a "dumb"
key/value store of sorts.</li>
<li>Noah provides Links and Tags as a way to perform logical grouping of
these objects. Links replace the top-level hierarchy.</li>
<li>Watches are persistent. The act of creating a watch and notifying on
watched objects is disconnected from the final recipient of the
message. System A can register a watch on behalf of System B.</li>
<li>Watches are nothing more than a set of filters applied to a Redis
pubsub queue listener. Any language that supports Redis and its
pubsub queue can be a processor for watches.</li>
<li>You don't even have to register any Watches in Noah if you choose to
attach and filter yourself.</li>
</ul>


<p>Part three in this series will discuss the technology stack under Noah
and the reasoning behind it. A bit of that was touched on in this post.
Part four is the discussion about long-term goals and roadmaps.</p>
]]></content>
  </entry>
  
</feed>
